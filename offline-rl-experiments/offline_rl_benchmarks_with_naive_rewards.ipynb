{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces, Space\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import d3rlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActionSpace(Space):\n",
    "    def __init__(self, shape=None, dtype=None):\n",
    "        super().__init__(shape, dtype)\n",
    "        actions = np.arange(0.5, 5.5, 0.5)\n",
    "        self.actions_map = {idx: action for idx, action in enumerate(actions)}\n",
    "        self.actions = list(self.actions_map.keys())\n",
    "    \n",
    "class MovieLensEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, data, use_prev_temp_as_feature=False, van_specific_embeddings=None, pbar=None):\n",
    "        # print(\"__init__ method\")\n",
    "        # with open('../gym/data/mlens/mlens-test-trajectories-v1.pkl', 'rb') as f:\n",
    "        # with open(test_traj_path, 'rb') as f:\n",
    "        self.dataset = data\n",
    "\n",
    "        super(MovieLensEnv, self).__init__()\n",
    "        actions = np.arange(0.5, 5.5, 0.5)\n",
    "        self.actions_map = {idx: action for idx, action in enumerate(actions)}\n",
    "        self.current_step = 0\n",
    "        self.max_steps = sum(len(traj['observations']) for traj in self.dataset)\n",
    "        self.action_space = spaces.Discrete(10)  # You need to define CustomActionSpace\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.dataset[0]['observations'].shape[1],), dtype=np.float32)\n",
    "        self.sampled_idx = None\n",
    "        self.action = None\n",
    "        self.reward = None\n",
    "        self.pbar = pbar\n",
    "        self.total_steps = 0\n",
    "        self.use_prev_temp = use_prev_temp_as_feature\n",
    "        # self.idx_of_prev_temp_feat = np.where(self.dataset[0]['features'] == 'd_prev_target_temp')[0][0]\n",
    "        self.personalized_features = van_specific_embeddings\n",
    "\n",
    "    def step(self, action):\n",
    "        self.action = action\n",
    "        target_rating = self.dataset[self.sampled_idx]['targets'][self.current_step]\n",
    "        # print(f\"action taken in step: {action}\")\n",
    "        # print(f\"type of action: {type(action)}\")\n",
    "        pred_rating = self.actions_map[action]\n",
    "        # print(f\"pred_rating: {pred_rating}\")\n",
    "        \n",
    "        # print(f\"action: {self.action} | pred_rating: {pred_rating} | original_rating: {target_rating}\")\n",
    "        acc = 0\n",
    "        if pred_rating == target_rating:\n",
    "            acc = 1\n",
    "\n",
    "        # Rewards scheme 5\n",
    "        # -------------------------------\n",
    "        error = abs(target_rating - pred_rating)\n",
    "        self.reward = (1- (error / 4.5)) ** 2\n",
    "        # # -------------------------------\n",
    "        \n",
    "        # Binary Rewards scheme \n",
    "        # -------------------------------\n",
    "        if target_rating >= 3.5 and pred_rating >= 3.5:\n",
    "            self.reward = 1\n",
    "        else:\n",
    "            self.reward = 0\n",
    "        \n",
    "        # # -------------------------------\n",
    "        # # Reward for special cases\n",
    "        # if target_rating != pred_rating:\n",
    "        #     special_reward = reward\n",
    "        # else:\n",
    "        #     special_reward = 0\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # if self.pbar is not None:\n",
    "        #     self.pbar.set_description(f\"(idx, step): ({self.sampled_idx}, {self.current_step}) | True rating: {target_rating} | Predicted rating: {pred_rating} | reward: {self.reward:.2f}\")\n",
    "        #     # time.sleep(0.25)\n",
    "        self.current_step += 1\n",
    "        obs, done = self._next_observation()\n",
    "        self.total_steps += 1\n",
    "        # return obs, self.reward, done, acc, target_rating, pred_rating, self.total_steps\n",
    "        return obs, self.reward, done, None, {}\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        self.sampled_idx = random.randint(0, len(self.dataset) - 1)\n",
    "        self.current_step = 0\n",
    "        traj = self.dataset[self.sampled_idx]\n",
    "        user_id = traj['user_id']\n",
    "\n",
    "\n",
    "        obs = traj['observations'][self.current_step]\n",
    "\n",
    "        if self.personalized_features is not None:\n",
    "            obs = np.hstack((obs, self.personalized_features[user_id]))\n",
    "        \n",
    "        return obs, None\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        if self.dataset[self.sampled_idx]['terminals'][self.current_step]:\n",
    "            done = True\n",
    "            obs, _ = self.reset()\n",
    "            return obs, done\n",
    "        \n",
    "        traj = self.dataset[self.sampled_idx]\n",
    "        user_id = traj['user_id']\n",
    "        obs = traj['observations'][self.current_step]\n",
    "        if self.personalized_features is not None:\n",
    "            obs = np.hstack((obs, self.personalized_features[van_id]))\n",
    "        done = False\n",
    "        return obs, done\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "        \n",
    "    def get_true_temperature(self):\n",
    "        target_temperature = self.dataset[self.sampled_idx]['actions'][self.current_step]\n",
    "        target_temperature = self.actions_map[target_temperature]\n",
    "        return target_temperature\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "with open(\"../data/dt-datasets/movielens/processed-data/all_trajectories_with_concatenated_movname_genres_tags_userid_reward_of_scale_5.pkl\", 'rb') as f:\n",
    "    all_trajectories = pickle.load(f)\n",
    "# all_trajs_copy = deepcopy(all_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train users: 427\n",
      "total test users: 183\n",
      "Train set: 427\n",
      "Test set: 183\n"
     ]
    }
   ],
   "source": [
    "# Calculate the size for the training set\n",
    "np.random.seed(42)\n",
    "trajectories = all_trajectories\n",
    "indices = {i for i in range(len(trajectories))}\n",
    "train_indices = list(np.random.choice(list(indices), size=round(0.7*len(indices)), replace=False))\n",
    "remaining_indices = indices.difference(train_indices)\n",
    "test_indices = remaining_indices\n",
    "\n",
    "print(f\"total train users: {len(train_indices)}\")\n",
    "print(f\"total test users: {len(test_indices)}\")\n",
    "\n",
    "train_trajectories = [trajectories[idx]for idx in train_indices]\n",
    "test_trajectories = [trajectories[idx]for idx in test_indices]\n",
    "\n",
    "print(\"Train set:\", len(train_trajectories))\n",
    "print(\"Test set:\", len(test_trajectories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward scheme 1: Naive reward: all observations marked with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_copy = deepcopy(train_data)\n",
    "train_trajectories_copy = deepcopy(train_trajectories)\n",
    "for traj in train_trajectories_copy:\n",
    "    name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "    traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "    # print(traj['observations'].shape)\n",
    "    traj['rewards'] =  np.ones_like(traj['rewards'])\n",
    "\n",
    "\n",
    "\n",
    "test_trajectories_copy = deepcopy(test_trajectories)\n",
    "for traj in test_trajectories_copy:\n",
    "    name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "    traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "    traj['rewards'] =  np.ones_like(traj['rewards'])\n",
    "\n",
    "    \n",
    "train_data_with_naive_rewards = train_trajectories_copy\n",
    "test_data_with_naive_rewards = test_trajectories_copy\n",
    "\n",
    "with open('data/train_data_with_naive_rewards.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data_with_naive_rewards, f)\n",
    "\n",
    "with open('data/test_data_with_naive_rewards.pkl', 'wb') as f:\n",
    "    pickle.dump(test_data_with_naive_rewards, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation for Discrete CQL\n",
    "import numpy as np\n",
    "observations_mlens = []\n",
    "observations_mlens = np.concatenate([ep['observations'] for ep in train_data_with_naive_rewards])\n",
    "actions_mlens = np.concatenate([ep['actions'] for ep in train_data_with_naive_rewards])\n",
    "rewards_mlens = np.concatenate([ep['rewards'] for ep in train_data_with_naive_rewards])\n",
    "terminals_mlens = np.concatenate([ep['terminals'] for ep in train_data_with_naive_rewards])\n",
    "\n",
    "timeouts = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.dataset import EpisodeGenerator\n",
    "\n",
    "episode_generator = EpisodeGenerator(\n",
    "    observations=observations_mlens,\n",
    "    actions=actions_mlens,\n",
    "    rewards=rewards_mlens,\n",
    "    terminals=terminals_mlens,\n",
    "    timeouts=timeouts,\n",
    ")\n",
    "\n",
    "episodes_generated_mlens = episode_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:15.08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mSignatures have been automatically determined.\u001b[0m \u001b[36maction_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('int64')], shape=[(1,)])\u001b[0m \u001b[36mobservation_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float32')], shape=[(800,)])\u001b[0m \u001b[36mreward_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float64')], shape=[(1,)])\u001b[0m\n",
      "\u001b[2m2024-02-11 13:15.08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAction-space has been automatically determined.\u001b[0m \u001b[36maction_space\u001b[0m=\u001b[35m<ActionSpace.DISCRETE: 2>\u001b[0m\n",
      "\u001b[2m2024-02-11 13:15.08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAction size has been automatically determined.\u001b[0m \u001b[36maction_size\u001b[0m=\u001b[35m10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from d3rlpy.dataset import ReplayBuffer, InfiniteBuffer\n",
    "\n",
    "dataset = ReplayBuffer(\n",
    "    InfiniteBuffer(),\n",
    "    episodes=episodes_generated_mlens,\n",
    "    transition_picker=None,\n",
    "    trajectory_slicer=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227, 800)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.episodes[0].observations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MovieLensEnv(test_data_with_naive_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discrete CQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start training\n",
    "# cql = d3rlpy.algos.DiscreteCQLConfig().create(device='cuda')\n",
    "# cql.fit(\n",
    "#     dataset,\n",
    "#     n_steps=10000,\n",
    "#     n_steps_per_epoch=1000,\n",
    "#     evaluators={\n",
    "#         'environment': d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# # evaluate\n",
    "# rewards = []\n",
    "# for _ in range(10):\n",
    "#     reward = d3rlpy.metrics.evaluate_qlearning_with_environment(cql, env)\n",
    "#     rewards.append(reward)\n",
    "# # print(np.round(rewards, 2))\n",
    "    \n",
    "# for r in np.round(rewards, 2):\n",
    "#     print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start training\n",
    "# dqn = d3rlpy.algos.DQNConfig().create(device='cuda')\n",
    "# dqn.fit(\n",
    "#     dataset,\n",
    "#     n_steps=10000,\n",
    "#     n_steps_per_epoch=1000,\n",
    "#     evaluators={\n",
    "#         'environment': d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# # evaluate\n",
    "# rewards = []\n",
    "# for _ in range(10):\n",
    "#     reward = d3rlpy.metrics.evaluate_qlearning_with_environment(dqn, env)\n",
    "#     rewards.append(reward)\n",
    "# # print(np.round(rewards, 2))\n",
    "    \n",
    "# for r in np.round(rewards, 2):\n",
    "#     print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:15.47\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mdataset info                  \u001b[0m \u001b[36mdataset_info\u001b[0m=\u001b[35mDatasetInfo(observation_signature=Signature(dtype=[dtype('float32')], shape=[(800,)]), action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]), reward_signature=Signature(dtype=[dtype('float64')], shape=[(1,)]), action_space=<ActionSpace.DISCRETE: 2>, action_size=10)\u001b[0m\n",
      "\u001b[2m2024-02-11 13:15.47\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDirectory is created at d3rlpy_logs/DoubleDQN_20240211131547\u001b[0m\n",
      "\u001b[2m2024-02-11 13:15.47\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mBuilding models...            \u001b[0m\n",
      "\u001b[2m2024-02-11 13:15.47\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mModels have been built.       \u001b[0m\n",
      "\u001b[2m2024-02-11 13:15.47\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mParameters                    \u001b[0m \u001b[36mparams\u001b[0m=\u001b[35m{'observation_shape': [800], 'action_size': 10, 'config': {'type': 'double_dqn', 'params': {'batch_size': 32, 'gamma': 0.99, 'observation_scaler': {'type': 'none', 'params': {}}, 'action_scaler': {'type': 'none', 'params': {}}, 'reward_scaler': {'type': 'none', 'params': {}}, 'learning_rate': 6.25e-05, 'optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'n_critics': 1, 'target_update_interval': 8000}}}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59dbb26883c347078ffe8501f4bea297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:15.53\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDoubleDQN_20240211131547: epoch=1 step=1000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m1\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0015747854709625244, 'time_algorithm_update': 0.0035011582374572755, 'loss': 0.49966634510457514, 'time_step': 0.0051364459991455075, 'environment': 18.91482362743111}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m1000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:15.53\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DoubleDQN_20240211131547/model_1000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421b6db83a1740ad851b03713dd4df81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:15.59\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDoubleDQN_20240211131547: epoch=2 step=2000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m2\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0016096196174621583, 'time_algorithm_update': 0.0033745198249816896, 'loss': 0.42170781384408473, 'time_step': 0.005047095537185669, 'environment': 33.12005319034266}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m2000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:15.59\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DoubleDQN_20240211131547/model_2000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf57d9deb8846149422806a1c0ea127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:16.05\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDoubleDQN_20240211131547: epoch=3 step=3000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m3\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0015847759246826172, 'time_algorithm_update': 0.0033999805450439452, 'loss': 0.4093866769969463, 'time_step': 0.0050436131954193116, 'environment': 25.744818758232036}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m3000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:16.05\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DoubleDQN_20240211131547/model_3000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c00832722747a49bf8117b30970fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start training\n",
    "ddqn = d3rlpy.algos.DoubleDQNConfig().create(device='cuda')\n",
    "ddqn.fit(\n",
    "    dataset,\n",
    "    n_steps=10000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    evaluators={\n",
    "        'environment': d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "    },\n",
    ")\n",
    "\n",
    "# evaluate\n",
    "rewards = []\n",
    "for _ in range(10):\n",
    "    reward = d3rlpy.metrics.evaluate_qlearning_with_environment(ddqn, env)\n",
    "    rewards.append(reward)\n",
    "# print(np.round(rewards, 2))\n",
    "    \n",
    "for r in np.round(rewards, 2):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "sac = d3rlpy.algos.DiscreteSACConfig().create(device='cuda')\n",
    "sac.fit(\n",
    "    dataset,\n",
    "    n_steps=10000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    evaluators={\n",
    "        'environment': d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "    },\n",
    ")\n",
    "\n",
    "# evaluate\n",
    "rewards = []\n",
    "for _ in range(10):\n",
    "    reward = d3rlpy.metrics.evaluate_qlearning_with_environment(sac, env)\n",
    "    rewards.append(reward)\n",
    "# print(np.round(rewards, 2))\n",
    "    \n",
    "for r in np.round(rewards, 2):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous CQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d3rlpy\n",
    "\n",
    "# prepare dataset\n",
    "dataset, env = d3rlpy.datasets.get_d4rl('hopper-medium-v0')\n",
    "\n",
    "# # prepare algorithm\n",
    "# cql = d3rlpy.algos.CQLConfig().create(device='cuda:0')\n",
    "\n",
    "# # train\n",
    "# cql.fit(\n",
    "#     dataset,\n",
    "#     n_steps=100000,\n",
    "#     evaluators={\"environment\": d3rlpy.metrics.EnvironmentEvaluator(env)},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.episodes[0].observations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d4rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d3rlpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
