{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces, Space\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import d3rlpy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Box, Space\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "class CustomActionSpace(Space):\n",
    "    def __init__(self, shape=None, dtype=None):\n",
    "        super().__init__(shape, dtype)\n",
    "        actions = np.arange(0.5, 5.5, 0.5)\n",
    "        self.actions_map = {idx: action for idx, action in enumerate(actions)}\n",
    "        self.actions = list(self.actions_map.keys())\n",
    "    \n",
    "class MovieLensEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, test_data, movie_embed_to_id, movies_ratings_and_tags, action_vocab):\n",
    "        # with open(test_traj_path, 'rb') as f:\n",
    "        super(MovieLensEnv, self).__init__()\n",
    "        self.dataset = test_data\n",
    "        self.action_vocab = action_vocab\n",
    "        self.current_step = 0\n",
    "        self.max_steps = sum(len(traj['observations']) for traj in self.dataset)\n",
    "        self.action_space = CustomActionSpace(shape=(1, 1))  # You need to define CustomActionSpace\n",
    "        self.observation_space = Box(low=0, high=1, shape=(self.dataset[0]['observations'].shape[1],), dtype=np.float32)\n",
    "        self.sampled_idx = None\n",
    "        self.action = None\n",
    "        self.reward = None\n",
    "        self.total_steps = 0\n",
    "        self.movies_ratings_and_tags = movies_ratings_and_tags\n",
    "        self.movie_embed_to_id = movie_embed_to_id\n",
    "\n",
    "    def step(self, action):\n",
    "        self.action = action\n",
    "        # print(self.action.shape, type(self.action))\n",
    "        # print(f\"action shape after view operation: {self.action.view(-1, self.action.shape).shape}\")\n",
    "        similarities = cosine_similarity(self.action.reshape(-1, self.action.shape[0]), self.action_vocab)\n",
    "        # print(f\"similarities.shape: {similarities.shape}\")\n",
    "\n",
    "        # Find indices of closest neighbors for each prediction in the batch\n",
    "        closest_indices = np.argmax(similarities, axis=1)\n",
    "\n",
    "        closest_vector = self.action_vocab[closest_indices.flatten()].reshape(1, self.action.shape[0])\n",
    "\n",
    "        action_pred = closest_vector.reshape(-1)\n",
    "\n",
    "        # print(f\"pred action shape: {action_pred.shape}\")\n",
    "        # Need to create a mapping between actions and rewards\n",
    "        # If the movie is actually rated by the user: then the reward is the user's rating\n",
    "        # Else, the reward is the average rating of all users for the movie\n",
    "\n",
    "        # First let's find the movie_id from the embed\n",
    "        movie_id = self.movie_embed_to_id[tuple(action_pred)]\n",
    "        user_id = self.dataset[self.sampled_idx]['user_id']\n",
    "\n",
    "        rating_by_user = self.movies_ratings_and_tags[(self.movies_ratings_and_tags['movieId'] == movie_id) & (self.movies_ratings_and_tags['userId'] == user_id)]['rating']\n",
    "\n",
    "        if rating_by_user.any():\n",
    "            self.reward = rating_by_user.values[0]\n",
    "        else:\n",
    "            self.reward = self.movies_ratings_and_tags[self.movies_ratings_and_tags['movieId'] == movie_id]['rating_global'].mean()\n",
    "\n",
    "        # Round to nearest 0.5\n",
    "        def round_to_nearest_half(number):\n",
    "            return round(number * 2) / 2\n",
    "\n",
    "        # Example usage\n",
    "        self.reward = round_to_nearest_half(self.reward)/5.0\n",
    "\n",
    "\n",
    "        done = False\n",
    "        \n",
    "        # if self.pbar is not None:\n",
    "        #     self.pbar.set_description(f\"(idx, step): ({self.sampled_idx}, {self.current_step}) | predicted movie_id: {movie_id} | reward: {self.reward:.2f}\")\n",
    "        #     # time.sleep(0.25)\n",
    "        self.current_step += 1\n",
    "        self.total_steps += 1\n",
    "        obs, done = self._next_observation()\n",
    "        return obs, self.reward, done, None, {}\n",
    "        \n",
    "        # return obs, self.reward, done, user_feature, self.total_steps, movie_id\n",
    "    \n",
    "    def reset(self):\n",
    "        self.sampled_idx = random.randint(0, len(self.dataset) - 1)\n",
    "        self.current_step = 0\n",
    "        traj = self.dataset[self.sampled_idx]\n",
    "        user_id = traj['user_id']\n",
    "\n",
    "        obs = traj['observations'][self.current_step]\n",
    "\n",
    "        return obs, None\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        if self.dataset[self.sampled_idx]['terminals'][self.current_step]:\n",
    "            done = True\n",
    "            obs, _ = self.reset()\n",
    "            return obs, done\n",
    "        \n",
    "        traj = self.dataset[self.sampled_idx]\n",
    "        user_id = traj['user_id']\n",
    "        obs = traj['observations'][self.current_step]\n",
    "\n",
    "        done = False\n",
    "        return obs, done\n",
    "\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "        \n",
    "    def get_true_temperature(self):\n",
    "        target_temperature = self.dataset[self.sampled_idx]['actions'][self.current_step]\n",
    "        target_temperature = self.action_space.actions_map[target_temperature]\n",
    "        return target_temperature\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_embed_shape = 32\n",
    "\n",
    "train_dataset_path = f\"../data/dt-datasets/movielens/train-test-sets/mlens-train-trajectories-movies-as-actions-reduced-from-768-to-{action_embed_shape}.pkl\"\n",
    "\n",
    "test_set_path = f\"../data/dt-datasets/movielens/train-test-sets/mlens-test-trajectories-movies-as-actions-reduced-from-768-to-{action_embed_shape}.pkl\"\n",
    "\n",
    "movie_embeds_to_id_map_path = f\"../data/dt-datasets/movielens/processed-data/movie_embed_with_shape_{action_embed_shape}_to_id_mapping.pkl\"\n",
    "\n",
    "action_vocab_path = f\"../data/dt-datasets/movielens/processed-data/action_vocab_of_shape_{action_embed_shape}.pkl\"\n",
    "\n",
    "with open(action_vocab_path, 'rb') as f:\n",
    "    action_vocab = pickle.load(f)\n",
    "\n",
    "with open(train_dataset_path, 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "with open(test_set_path, 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "with open(movie_embeds_to_id_map_path, 'rb') as f:\n",
    "    movie_embed_to_id = pickle.load(f)\n",
    "\n",
    "movies_ratings_and_tags = pd.read_csv(\"../data/movies_ratings_and_tags_mlens_small.csv\")\n",
    "movies_ratings_and_tags.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "overall_ratings = movies_ratings_and_tags.groupby('movieId')['rating'].mean().reset_index()\n",
    "# Merge the overall ratings back into the original DataFrame\n",
    "movies_ratings_and_tags = movies_ratings_and_tags.merge(overall_ratings, on='movieId', suffixes=('', '_global'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]['observations'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation for Discrete CQL\n",
    "import numpy as np\n",
    "observations_mlens = []\n",
    "observations_mlens = np.concatenate([ep['observations'] for ep in train_data])\n",
    "actions_mlens = np.concatenate([ep['actions'] for ep in train_data])\n",
    "rewards_mlens = np.concatenate([ep['rewards'] for ep in train_data])\n",
    "terminals_mlens = np.concatenate([ep['terminals'] for ep in train_data])\n",
    "\n",
    "timeouts = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.dataset import EpisodeGenerator\n",
    "\n",
    "episode_generator = EpisodeGenerator(\n",
    "    observations=observations_mlens,\n",
    "    actions=actions_mlens,\n",
    "    rewards=rewards_mlens,\n",
    "    terminals=terminals_mlens,\n",
    "    timeouts=timeouts,\n",
    ")\n",
    "\n",
    "episodes_generated_mlens = episode_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.dataset import ReplayBuffer, InfiniteBuffer\n",
    "\n",
    "dataset = ReplayBuffer(\n",
    "    InfiniteBuffer(),\n",
    "    episodes=episodes_generated_mlens,\n",
    "    transition_picker=None,\n",
    "    trajectory_slicer=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.episodes[0].observations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MovieLensEnv(test_data=test_data, movie_embed_to_id=movie_embed_to_id, movies_ratings_and_tags=movies_ratings_and_tags, action_vocab=action_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous CQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start training\n",
    "# cql = d3rlpy.algos.CQLConfig().create(device='cuda')\n",
    "# cql.fit(\n",
    "#     dataset,\n",
    "#     n_steps=10000,\n",
    "#     n_steps_per_epoch=1000,\n",
    "#     evaluators={\n",
    "#         'environment': d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# # evaluate\n",
    "# rewards = []\n",
    "# for _ in range(5):\n",
    "#     reward = d3rlpy.metrics.evaluate_qlearning_with_environment(cql, env)\n",
    "#     rewards.append(reward)\n",
    "# # print(np.round(rewards, 2))\n",
    "    \n",
    "# for r in np.round(rewards, 2):\n",
    "#     print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "ddpg = d3rlpy.algos.DDPGConfig().create(device='cuda')\n",
    "ddpg.fit(\n",
    "    dataset,\n",
    "    n_steps=10000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    # evaluators={\n",
    "    #     'environment': d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "    # },\n",
    ")\n",
    "\n",
    "# evaluate\n",
    "rewards = []\n",
    "for _ in range(5):\n",
    "    reward = d3rlpy.metrics.evaluate_qlearning_with_environment(ddpg, env)\n",
    "    rewards.append(reward)\n",
    "\n",
    "    \n",
    "for r in np.round(rewards, 1):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "bear = d3rlpy.algos.BEARConfig().create(device='cuda')\n",
    "bear.fit(\n",
    "    dataset,\n",
    "    n_steps=10000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    # evaluators={\n",
    "    #     'environment': d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "    # },\n",
    ")\n",
    "\n",
    "# evaluate\n",
    "rewards = []\n",
    "for _ in range(5):\n",
    "    reward = d3rlpy.metrics.evaluate_qlearning_with_environment(bear, env)\n",
    "    rewards.append(reward)\n",
    "\n",
    "    \n",
    "for r in np.round(rewards, 1):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# start training\n",
    "bc = d3rlpy.algos.BCConfig().create(device='cuda')\n",
    "bc.fit(\n",
    "    dataset,\n",
    "    n_steps=10000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    # evaluators={\n",
    "    #     'environment': d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "    # },\n",
    ")\n",
    "\n",
    "# evaluate\n",
    "rewards = []\n",
    "for _ in tqdm(range(5)):\n",
    "    reward = d3rlpy.metrics.evaluate_qlearning_with_environment(bc, env)\n",
    "    rewards.append(reward)\n",
    "\n",
    "    \n",
    "for r in np.round(rewards, 1):\n",
    "    print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d3rlpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
