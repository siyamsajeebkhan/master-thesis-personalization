{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces, Space\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import d3rlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Box, Space\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "class CustomActionSpace(Space):\n",
    "    def __init__(self, shape=None, dtype=None):\n",
    "        super().__init__(shape, dtype)\n",
    "        actions = np.arange(0.5, 5.5, 0.5)\n",
    "        self.actions_map = {idx: action for idx, action in enumerate(actions)}\n",
    "        self.actions = list(self.actions_map.keys())\n",
    "    \n",
    "class MovieLensEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, test_traj_path, movie_embed_to_id, movies_ratings_and_tags):\n",
    "        with open(test_traj_path, 'rb') as f:\n",
    "            self.dataset = pickle.load(f)\n",
    "\n",
    "        super(MovieLensEnv, self).__init__()\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.max_steps = sum(len(traj['observations']) for traj in self.dataset)\n",
    "        self.action_space = CustomActionSpace(shape=(1, 1))  # You need to define CustomActionSpace\n",
    "        self.observation_space = Box(low=0, high=1, shape=(self.dataset[0]['observations'].shape[1],), dtype=np.float32)\n",
    "        self.sampled_idx = None\n",
    "        self.action = None\n",
    "        self.reward = None\n",
    "        self.total_steps = 0\n",
    "        self.movies_ratings_and_tags = movies_ratings_and_tags\n",
    "        self.movie_embed_to_id = movie_embed_to_id\n",
    "\n",
    "    def step(self, action):\n",
    "        self.action = action.cpu().numpy()\n",
    "\n",
    "        # Need to create a mapping between actions and rewards\n",
    "        # If the movie is actually rated by the user: then the reward is the user's rating\n",
    "        # Else, the reward is the average rating of all users for the movie\n",
    "\n",
    "        # First let's find the movie_id from the embed\n",
    "        movie_id = self.movie_embed_to_id[tuple(self.action)]\n",
    "        user_id = self.dataset[self.sampled_idx]['user_id']\n",
    "\n",
    "        rating_by_user = self.movies_ratings_and_tags[(self.movies_ratings_and_tags['movieId'] == movie_id) & (self.movies_ratings_and_tags['userId'] == user_id)]['rating']\n",
    "\n",
    "        if rating_by_user.any():\n",
    "            self.reward = rating_by_user.values[0]\n",
    "        else:\n",
    "            self.reward = self.movies_ratings_and_tags[self.movies_ratings_and_tags['movieId'] == movie_id]['rating_global'].mean()\n",
    "\n",
    "        # Round to nearest 0.5\n",
    "        def round_to_nearest_half(number):\n",
    "            return round(number * 2) / 2\n",
    "\n",
    "        # Example usage\n",
    "        self.reward = round_to_nearest_half(self.reward)\n",
    "\n",
    "\n",
    "        done = False\n",
    "        \n",
    "        # if self.pbar is not None:\n",
    "        #     self.pbar.set_description(f\"(idx, step): ({self.sampled_idx}, {self.current_step}) | predicted movie_id: {movie_id} | reward: {self.reward:.2f}\")\n",
    "        #     # time.sleep(0.25)\n",
    "        self.current_step += 1\n",
    "        self.total_steps += 1\n",
    "        if self.personalized_features is not None:\n",
    "            obs, done, user_feature = self._next_observation()\n",
    "            return obs, self.reward, done, user_feature, self.total_steps, movie_id\n",
    "        else:\n",
    "            obs, done = self._next_observation()\n",
    "            return obs, self.reward, done, self.total_steps, movie_id\n",
    "        \n",
    "        # return obs, self.reward, done, user_feature, self.total_steps, movie_id\n",
    "    \n",
    "    def reset(self):\n",
    "        self.sampled_idx = random.randint(0, len(self.dataset) - 1)\n",
    "        self.current_step = 0\n",
    "        traj = self.dataset[self.sampled_idx]\n",
    "        user_id = traj['user_id']\n",
    "\n",
    "        obs = traj['observations'][self.current_step]\n",
    "\n",
    "        if self.personalized_features is not None:\n",
    "            user_feature = self.personalized_features[self.personalized_features['userId'] == user_id].values.reshape(-1)\n",
    "\n",
    "            # print(f\"Before injecting personal features, shape of obs: {obs.shape}\")\n",
    "            # print(f\"shape of personal feature: {user_feature.shape}\")\n",
    "        # obs = np.hstack((user_feature, obs))\n",
    "            # print(f\"obs with personal features has shape: {obs.shape}\")\n",
    "        if self.personalized_features is not None:\n",
    "            return obs, user_feature\n",
    "        else:\n",
    "            return obs\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        if self.dataset[self.sampled_idx]['terminals'][self.current_step]:\n",
    "            done = True\n",
    "            if self.personalized_features is not None:\n",
    "                obs, user_feature = self.reset()\n",
    "                return obs, done, user_feature\n",
    "            else:\n",
    "                obs = self.reset()\n",
    "                return obs, done\n",
    "            \n",
    "        \n",
    "        traj = self.dataset[self.sampled_idx]\n",
    "        user_id = traj['user_id']\n",
    "        obs = traj['observations'][self.current_step]\n",
    "        if self.personalized_features is not None:\n",
    "            user_feature = self.personalized_features[self.personalized_features['userId'] == user_id].values.reshape(-1)\n",
    "            # print(f\"Before injecting personal features, shape of obs: {obs.shape}\")\n",
    "            # print(f\"shape of personal feature: {user_feature.shape}\")\n",
    "\n",
    "            # obs = np.hstack((obs, user_feature))\n",
    "            # print(f\"obs with personal features has shape: {obs.shape}\")\n",
    "        done = False\n",
    "        if self.personalized_features is not None:\n",
    "            return obs, done, user_feature\n",
    "        else:\n",
    "            return obs, done\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "        \n",
    "    def get_true_temperature(self):\n",
    "        target_temperature = self.dataset[self.sampled_idx]['actions'][self.current_step]\n",
    "        target_temperature = self.action_space.actions_map[target_temperature]\n",
    "        return target_temperature\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "with open(\"../data/dt-datasets/movielens/processed-data/all_trajectories_with_concatenated_movname_genres_tags_userid_reward_of_scale_5.pkl\", 'rb') as f:\n",
    "    all_trajectories = pickle.load(f)\n",
    "# all_trajs_copy = deepcopy(all_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232, 2336)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_trajectories[0]['observations'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the size for the training set\n",
    "np.random.seed(42)\n",
    "trajectories = all_trajectories\n",
    "indices = {i for i in range(len(trajectories))}\n",
    "train_indices = list(np.random.choice(list(indices), size=round(0.7*len(indices)), replace=False))\n",
    "remaining_indices = indices.difference(train_indices)\n",
    "test_indices = remaining_indices\n",
    "\n",
    "print(f\"total train users: {len(train_indices)}\")\n",
    "print(f\"total test users: {len(test_indices)}\")\n",
    "\n",
    "train_trajectories = [trajectories[idx]for idx in train_indices]\n",
    "test_trajectories = [trajectories[idx]for idx in test_indices]\n",
    "\n",
    "print(\"Train set:\", len(train_trajectories))\n",
    "print(\"Test set:\", len(test_trajectories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward scheme 3: Binary reward: 1 or 0; 1 if liked 0 if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trajectories_copy = deepcopy(train_trajectories)\n",
    "threshold = 3.5\n",
    "for traj in train_trajectories_copy:\n",
    "    name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "    traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "    # print(traj['observations'].shape)\n",
    "    # errors = abs(highest_rating - traj['targets'])\n",
    "    traj['rewards'] = (traj['targets'] >= threshold).astype(int)\n",
    "\n",
    "\n",
    "test_trajectories_copy = deepcopy(test_trajectories)\n",
    "for traj in test_trajectories_copy:\n",
    "    name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "    traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "    # print(traj['observations'].shape)\n",
    "    # errors = abs(highest_rating - traj['targets'])\n",
    "    traj['rewards'] = (traj['targets'] >= threshold).astype(int)\n",
    "\n",
    "train_data_with_binary_rewards = train_trajectories_copy\n",
    "test_data_with_binary_rewards = test_trajectories_copy\n",
    "\n",
    "with open('data/train_data_with_binary_rewards.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data_with_binary_rewards, f)\n",
    "\n",
    "with open('data/test_data_with_binary_rewards.pkl', 'wb') as f:\n",
    "    pickle.dump(test_data_with_binary_rewards, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation for Discrete CQL\n",
    "import numpy as np\n",
    "observations_mlens = []\n",
    "observations_mlens = np.concatenate([ep['observations'] for ep in train_data_with_binary_rewards])\n",
    "actions_mlens = np.concatenate([ep['actions'] for ep in train_data_with_binary_rewards])\n",
    "rewards_mlens = np.concatenate([ep['rewards'] for ep in train_data_with_binary_rewards])\n",
    "terminals_mlens = np.concatenate([ep['terminals'] for ep in train_data_with_binary_rewards])\n",
    "\n",
    "timeouts = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.dataset import EpisodeGenerator\n",
    "\n",
    "episode_generator = EpisodeGenerator(\n",
    "    observations=observations_mlens,\n",
    "    actions=actions_mlens,\n",
    "    rewards=rewards_mlens,\n",
    "    terminals=terminals_mlens,\n",
    "    timeouts=timeouts,\n",
    ")\n",
    "\n",
    "episodes_generated_mlens = episode_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.dataset import ReplayBuffer, InfiniteBuffer\n",
    "\n",
    "dataset = ReplayBuffer(\n",
    "    InfiniteBuffer(),\n",
    "    episodes=episodes_generated_mlens,\n",
    "    transition_picker=None,\n",
    "    trajectory_slicer=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.episodes[0].observations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MovieLensEnv(test_data_with_binary_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous CQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cql = d3rlpy.algos.DiscreteCQLConfig().create(device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cql.fit(\"bla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "cql = d3rlpy.algos.DiscreteCQLConfig().create(device='cuda')\n",
    "cql.fit(\n",
    "    dataset,\n",
    "    n_steps=10000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    evaluators={\n",
    "        'environment': d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "    },\n",
    ")\n",
    "\n",
    "# evaluate\n",
    "rewards = []\n",
    "for _ in range(10):\n",
    "    reward = d3rlpy.metrics.evaluate_qlearning_with_environment(cql, env)\n",
    "    rewards.append(reward)\n",
    "# print(np.round(rewards, 2))\n",
    "    \n",
    "for r in np.round(rewards, 2):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "dqn = d3rlpy.algos.DQNConfig().create(device='cuda')\n",
    "dqn.fit(\n",
    "    dataset,\n",
    "    n_steps=10000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    evaluators={\n",
    "        'environment': d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "    },\n",
    ")\n",
    "\n",
    "# evaluate\n",
    "rewards = []\n",
    "for _ in range(10):\n",
    "    reward = d3rlpy.metrics.evaluate_qlearning_with_environment(dqn, env)\n",
    "    rewards.append(reward)\n",
    "# print(np.round(rewards, 2))\n",
    "    \n",
    "for r in np.round(rewards, 2):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "ddqn = d3rlpy.algos.DoubleDQNConfig().create(device='cuda')\n",
    "ddqn.fit(\n",
    "    dataset,\n",
    "    n_steps=10000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    evaluators={\n",
    "        'environment': d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "    },\n",
    ")\n",
    "\n",
    "# evaluate\n",
    "rewards = []\n",
    "for _ in range(10):\n",
    "    reward = d3rlpy.metrics.evaluate_qlearning_with_environment(ddqn, env)\n",
    "    rewards.append(reward)\n",
    "# print(np.round(rewards, 2))\n",
    "    \n",
    "for r in np.round(rewards, 2):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous CQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d3rlpy\n",
    "\n",
    "# prepare dataset\n",
    "dataset, env = d3rlpy.datasets.get_d4rl('hopper-medium-v0')\n",
    "\n",
    "# # prepare algorithm\n",
    "# cql = d3rlpy.algos.CQLConfig().create(device='cuda:0')\n",
    "\n",
    "# # train\n",
    "# cql.fit(\n",
    "#     dataset,\n",
    "#     n_steps=100000,\n",
    "#     evaluators={\"environment\": d3rlpy.metrics.EnvironmentEvaluator(env)},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.episodes[0].observations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d4rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d3rlpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
