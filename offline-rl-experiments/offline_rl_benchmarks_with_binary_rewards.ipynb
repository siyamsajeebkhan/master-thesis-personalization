{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces, Space\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import d3rlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActionSpace(Space):\n",
    "    def __init__(self, shape=None, dtype=None):\n",
    "        super().__init__(shape, dtype)\n",
    "        actions = np.arange(0.5, 5.5, 0.5)\n",
    "        self.actions_map = {idx: action for idx, action in enumerate(actions)}\n",
    "        self.actions = list(self.actions_map.keys())\n",
    "    \n",
    "class MovieLensEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, data, use_prev_temp_as_feature=False, van_specific_embeddings=None, pbar=None):\n",
    "        # print(\"__init__ method\")\n",
    "        # with open('../gym/data/mlens/mlens-test-trajectories-v1.pkl', 'rb') as f:\n",
    "        # with open(test_traj_path, 'rb') as f:\n",
    "        self.dataset = data\n",
    "\n",
    "        super(MovieLensEnv, self).__init__()\n",
    "        actions = np.arange(0.5, 5.5, 0.5)\n",
    "        self.actions_map = {idx: action for idx, action in enumerate(actions)}\n",
    "        self.current_step = 0\n",
    "        self.max_steps = sum(len(traj['observations']) for traj in self.dataset)\n",
    "        self.action_space = spaces.Discrete(10)  # You need to define CustomActionSpace\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.dataset[0]['observations'].shape[1],), dtype=np.float32)\n",
    "        self.sampled_idx = None\n",
    "        self.action = None\n",
    "        self.reward = None\n",
    "        self.pbar = pbar\n",
    "        self.total_steps = 0\n",
    "        self.use_prev_temp = use_prev_temp_as_feature\n",
    "        # self.idx_of_prev_temp_feat = np.where(self.dataset[0]['features'] == 'd_prev_target_temp')[0][0]\n",
    "        self.personalized_features = van_specific_embeddings\n",
    "\n",
    "    def step(self, action):\n",
    "        self.action = action\n",
    "        target_rating = self.dataset[self.sampled_idx]['targets'][self.current_step]\n",
    "        # print(f\"action taken in step: {action}\")\n",
    "        # print(f\"type of action: {type(action)}\")\n",
    "        pred_rating = self.actions_map[action]\n",
    "        # print(f\"pred_rating: {pred_rating}\")\n",
    "        \n",
    "        # print(f\"action: {self.action} | pred_rating: {pred_rating} | original_rating: {target_rating}\")\n",
    "        acc = 0\n",
    "        if pred_rating == target_rating:\n",
    "            acc = 1\n",
    "\n",
    "        # Rewards scheme 5\n",
    "        # -------------------------------\n",
    "        # error = abs(target_rating - pred_rating)\n",
    "        # self.reward = (1- (error / 4.5)) ** 2\n",
    "        # # -------------------------------\n",
    "        \n",
    "        # Binary Rewards scheme \n",
    "        # -------------------------------\n",
    "        if target_rating >= 3.5 and pred_rating >= 3.5:\n",
    "            self.reward = 1\n",
    "        else:\n",
    "            self.reward = 0\n",
    "        \n",
    "        # # -------------------------------\n",
    "        # # Reward for special cases\n",
    "        # if target_rating != pred_rating:\n",
    "        #     special_reward = reward\n",
    "        # else:\n",
    "        #     special_reward = 0\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # if self.pbar is not None:\n",
    "        #     self.pbar.set_description(f\"(idx, step): ({self.sampled_idx}, {self.current_step}) | True rating: {target_rating} | Predicted rating: {pred_rating} | reward: {self.reward:.2f}\")\n",
    "        #     # time.sleep(0.25)\n",
    "        self.current_step += 1\n",
    "        obs, done = self._next_observation()\n",
    "        self.total_steps += 1\n",
    "        # return obs, self.reward, done, acc, target_rating, pred_rating, self.total_steps\n",
    "        return obs, self.reward, done, None, {}\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        self.sampled_idx = random.randint(0, len(self.dataset) - 1)\n",
    "        self.current_step = 0\n",
    "        traj = self.dataset[self.sampled_idx]\n",
    "        user_id = traj['user_id']\n",
    "\n",
    "\n",
    "        obs = traj['observations'][self.current_step]\n",
    "\n",
    "        if self.personalized_features is not None:\n",
    "            obs = np.hstack((obs, self.personalized_features[user_id]))\n",
    "        \n",
    "        return obs, None\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        if self.dataset[self.sampled_idx]['terminals'][self.current_step]:\n",
    "            done = True\n",
    "            obs, _ = self.reset()\n",
    "            return obs, done\n",
    "        \n",
    "        traj = self.dataset[self.sampled_idx]\n",
    "        user_id = traj['user_id']\n",
    "        obs = traj['observations'][self.current_step]\n",
    "        if self.personalized_features is not None:\n",
    "            obs = np.hstack((obs, self.personalized_features[van_id]))\n",
    "        done = False\n",
    "        return obs, done\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "        \n",
    "    def get_true_temperature(self):\n",
    "        target_temperature = self.dataset[self.sampled_idx]['actions'][self.current_step]\n",
    "        target_temperature = self.actions_map[target_temperature]\n",
    "        return target_temperature\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "with open(\"../data/dt-datasets/movielens/processed-data/all_trajectories_with_concatenated_movname_genres_tags_userid_reward_of_scale_5.pkl\", 'rb') as f:\n",
    "    all_trajectories = pickle.load(f)\n",
    "# all_trajs_copy = deepcopy(all_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train users: 427\n",
      "total test users: 183\n",
      "Train set: 427\n",
      "Test set: 183\n"
     ]
    }
   ],
   "source": [
    "# Calculate the size for the training set\n",
    "np.random.seed(42)\n",
    "trajectories = all_trajectories\n",
    "indices = {i for i in range(len(trajectories))}\n",
    "train_indices = list(np.random.choice(list(indices), size=round(0.7*len(indices)), replace=False))\n",
    "remaining_indices = indices.difference(train_indices)\n",
    "test_indices = remaining_indices\n",
    "\n",
    "print(f\"total train users: {len(train_indices)}\")\n",
    "print(f\"total test users: {len(test_indices)}\")\n",
    "\n",
    "train_trajectories = [trajectories[idx]for idx in train_indices]\n",
    "test_trajectories = [trajectories[idx]for idx in test_indices]\n",
    "\n",
    "print(\"Train set:\", len(train_trajectories))\n",
    "print(\"Test set:\", len(test_trajectories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward scheme 3: Binary reward: 1 or 0; 1 if liked 0 if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trajectories_copy = deepcopy(train_trajectories)\n",
    "threshold = 3.5\n",
    "for traj in train_trajectories_copy:\n",
    "    name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "    traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "    # print(traj['observations'].shape)\n",
    "    # errors = abs(highest_rating - traj['targets'])\n",
    "    traj['rewards'] = (traj['targets'] >= threshold).astype(int)\n",
    "\n",
    "\n",
    "test_trajectories_copy = deepcopy(test_trajectories)\n",
    "for traj in test_trajectories_copy:\n",
    "    name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "    traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "    # print(traj['observations'].shape)\n",
    "    # errors = abs(highest_rating - traj['targets'])\n",
    "    traj['rewards'] = (traj['targets'] >= threshold).astype(int)\n",
    "\n",
    "train_data_with_binary_rewards = train_trajectories_copy\n",
    "test_data_with_binary_rewards = test_trajectories_copy\n",
    "\n",
    "with open('data/train_data_with_binary_rewards.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data_with_binary_rewards, f)\n",
    "\n",
    "with open('data/test_data_with_binary_rewards.pkl', 'wb') as f:\n",
    "    pickle.dump(test_data_with_binary_rewards, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation for Discrete CQL\n",
    "import numpy as np\n",
    "observations_mlens = []\n",
    "observations_mlens = np.concatenate([ep['observations'] for ep in train_data_with_binary_rewards])\n",
    "actions_mlens = np.concatenate([ep['actions'] for ep in train_data_with_binary_rewards])\n",
    "rewards_mlens = np.concatenate([ep['rewards'] for ep in train_data_with_binary_rewards])\n",
    "terminals_mlens = np.concatenate([ep['terminals'] for ep in train_data_with_binary_rewards])\n",
    "\n",
    "timeouts = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.dataset import EpisodeGenerator\n",
    "\n",
    "episode_generator = EpisodeGenerator(\n",
    "    observations=observations_mlens,\n",
    "    actions=actions_mlens,\n",
    "    rewards=rewards_mlens,\n",
    "    terminals=terminals_mlens,\n",
    "    timeouts=timeouts,\n",
    ")\n",
    "\n",
    "episodes_generated_mlens = episode_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:27.15\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mSignatures have been automatically determined.\u001b[0m \u001b[36maction_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('int64')], shape=[(1,)])\u001b[0m \u001b[36mobservation_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('float32')], shape=[(800,)])\u001b[0m \u001b[36mreward_signature\u001b[0m=\u001b[35mSignature(dtype=[dtype('int64')], shape=[(1,)])\u001b[0m\n",
      "\u001b[2m2024-02-11 13:27.15\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAction-space has been automatically determined.\u001b[0m \u001b[36maction_space\u001b[0m=\u001b[35m<ActionSpace.DISCRETE: 2>\u001b[0m\n",
      "\u001b[2m2024-02-11 13:27.15\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAction size has been automatically determined.\u001b[0m \u001b[36maction_size\u001b[0m=\u001b[35m10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from d3rlpy.dataset import ReplayBuffer, InfiniteBuffer\n",
    "\n",
    "dataset = ReplayBuffer(\n",
    "    InfiniteBuffer(),\n",
    "    episodes=episodes_generated_mlens,\n",
    "    transition_picker=None,\n",
    "    trajectory_slicer=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227, 800)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.episodes[0].observations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MovieLensEnv(test_data_with_binary_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discrete CQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start training\n",
    "# cql = d3rlpy.algos.DiscreteCQLConfig().create(device='cuda')\n",
    "# cql.fit(\n",
    "#     dataset,\n",
    "#     n_steps=10000,\n",
    "#     n_steps_per_epoch=1000,\n",
    "#     evaluators={\n",
    "#         'environment': d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# # evaluate\n",
    "# rewards = []\n",
    "# for _ in range(10):\n",
    "#     reward = d3rlpy.metrics.evaluate_qlearning_with_environment(cql, env)\n",
    "#     rewards.append(reward)\n",
    "# # print(np.round(rewards, 2))\n",
    "    \n",
    "# for r in np.round(rewards, 2):\n",
    "#     print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:28.55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mdataset info                  \u001b[0m \u001b[36mdataset_info\u001b[0m=\u001b[35mDatasetInfo(observation_signature=Signature(dtype=[dtype('float32')], shape=[(800,)]), action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]), reward_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]), action_space=<ActionSpace.DISCRETE: 2>, action_size=10)\u001b[0m\n",
      "\u001b[2m2024-02-11 13:28.55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDirectory is created at d3rlpy_logs/DQN_20240211132855\u001b[0m\n",
      "\u001b[2m2024-02-11 13:28.55\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mBuilding models...            \u001b[0m\n",
      "\u001b[2m2024-02-11 13:28.55\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mModels have been built.       \u001b[0m\n",
      "\u001b[2m2024-02-11 13:28.55\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mParameters                    \u001b[0m \u001b[36mparams\u001b[0m=\u001b[35m{'observation_shape': [800], 'action_size': 10, 'config': {'type': 'dqn', 'params': {'batch_size': 32, 'gamma': 0.99, 'observation_scaler': {'type': 'none', 'params': {}}, 'action_scaler': {'type': 'none', 'params': {}}, 'reward_scaler': {'type': 'none', 'params': {}}, 'learning_rate': 6.25e-05, 'optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'n_critics': 1, 'target_update_interval': 8000}}}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba970f950b145a583c4b1d232f9efb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:29.02\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDQN_20240211132855: epoch=1 step=1000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m1\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.001644219398498535, 'time_algorithm_update': 0.003269291639328003, 'loss': 0.4139505652189255, 'time_step': 0.004974134922027588, 'environment': 54.770023420049824}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m1000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:29.02\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DQN_20240211132855/model_1000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ad60deb55548b4ba76ce7d03cc8729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:29.07\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDQN_20240211132855: epoch=2 step=2000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m2\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0016619021892547608, 'time_algorithm_update': 0.003310084581375122, 'loss': 0.35927419032156466, 'time_step': 0.005024294853210449, 'environment': 66.85420451869669}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m2000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:29.07\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DQN_20240211132855/model_2000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9910767edb6841e09e5215d6d698bcb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:29.12\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDQN_20240211132855: epoch=3 step=3000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m3\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0016223554611206055, 'time_algorithm_update': 0.003258695125579834, 'loss': 0.34989781664311886, 'time_step': 0.004934022903442383, 'environment': 72.180401167175}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m3000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:29.12\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DQN_20240211132855/model_3000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11886dfe60b0461e8b87db943239211f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:29.18\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDQN_20240211132855: epoch=4 step=4000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m4\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0016711032390594482, 'time_algorithm_update': 0.0033316402435302735, 'loss': 0.34295177841186525, 'time_step': 0.005059114694595337, 'environment': 76.15577479369787}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m4000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:29.18\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DQN_20240211132855/model_4000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ee670478f543768944e92ade92ca9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:29.23\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDQN_20240211132855: epoch=5 step=5000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m5\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.001614142656326294, 'time_algorithm_update': 0.003193758726119995, 'loss': 0.337283948764205, 'time_step': 0.004860852956771851, 'environment': 56.21985578277819}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m5000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:29.23\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DQN_20240211132855/model_5000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f0732c210341b49d411e23c938746b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:29.29\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDQN_20240211132855: epoch=6 step=6000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m6\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0016687748432159424, 'time_algorithm_update': 0.0032659831047058105, 'loss': 0.3398749498873949, 'time_step': 0.004987427473068237, 'environment': 48.57794643683288}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m6000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:29.29\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DQN_20240211132855/model_6000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc7d6ce8e744f409db5ff49fc5df81c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:29.35\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDQN_20240211132855: epoch=7 step=7000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m7\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0015975892543792724, 'time_algorithm_update': 0.0032206614017486574, 'loss': 0.33341740249097346, 'time_step': 0.004870854139328003, 'environment': 59.759058723358315}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m7000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:29.35\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DQN_20240211132855/model_7000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7513e73fb2416d9bd5f0eee348fc17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:29.40\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDQN_20240211132855: epoch=8 step=8000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m8\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0016170566082000732, 'time_algorithm_update': 0.003234290361404419, 'loss': 0.3350099585056305, 'time_step': 0.004902865171432495, 'environment': 65.86207723587265}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m8000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:29.40\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DQN_20240211132855/model_8000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5c34ef7dfc4feca3834bb0eb0e4f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:29.46\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDQN_20240211132855: epoch=9 step=9000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m9\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0016836612224578858, 'time_algorithm_update': 0.003324442148208618, 'loss': 0.06315795130934566, 'time_step': 0.005070282220840454, 'environment': 63.933878117131826}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m9000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:29.46\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DQN_20240211132855/model_9000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a040386548ce4cbeb43e88d7ca4f5110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:29.51\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDQN_20240211132855: epoch=10 step=10000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m10\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0017283413410186768, 'time_algorithm_update': 0.003416623830795288, 'loss': 0.05479653418343514, 'time_step': 0.005207204103469849, 'environment': 67.17517251775607}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m10000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:29.51\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DQN_20240211132855/model_10000.d3\u001b[0m\n",
      "66.35\n",
      "68.9\n",
      "67.68\n",
      "69.06\n",
      "50.98\n",
      "71.11\n",
      "46.64\n",
      "62.26\n",
      "69.63\n",
      "70.77\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "dqn = d3rlpy.algos.DQNConfig().create(device='cuda')\n",
    "dqn.fit(\n",
    "    dataset,\n",
    "    n_steps=10000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    evaluators={\n",
    "        'environment': d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "    },\n",
    ")\n",
    "\n",
    "# evaluate\n",
    "rewards = []\n",
    "for _ in range(10):\n",
    "    reward = d3rlpy.metrics.evaluate_qlearning_with_environment(dqn, env)\n",
    "    rewards.append(reward)\n",
    "# print(np.round(rewards, 2))\n",
    "    \n",
    "for r in np.round(rewards, 2):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:27.17\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mdataset info                  \u001b[0m \u001b[36mdataset_info\u001b[0m=\u001b[35mDatasetInfo(observation_signature=Signature(dtype=[dtype('float32')], shape=[(800,)]), action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]), reward_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]), action_space=<ActionSpace.DISCRETE: 2>, action_size=10)\u001b[0m\n",
      "\u001b[2m2024-02-11 13:27.17\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDirectory is created at d3rlpy_logs/DoubleDQN_20240211132717\u001b[0m\n",
      "\u001b[2m2024-02-11 13:27.17\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mBuilding models...            \u001b[0m\n",
      "\u001b[2m2024-02-11 13:27.17\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mModels have been built.       \u001b[0m\n",
      "\u001b[2m2024-02-11 13:27.17\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mParameters                    \u001b[0m \u001b[36mparams\u001b[0m=\u001b[35m{'observation_shape': [800], 'action_size': 10, 'config': {'type': 'double_dqn', 'params': {'batch_size': 32, 'gamma': 0.99, 'observation_scaler': {'type': 'none', 'params': {}}, 'action_scaler': {'type': 'none', 'params': {}}, 'reward_scaler': {'type': 'none', 'params': {}}, 'learning_rate': 6.25e-05, 'optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'n_critics': 1, 'target_update_interval': 8000}}}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e335fb9fc764ed7b90ec89449639047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:27.23\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDoubleDQN_20240211132717: epoch=1 step=1000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m1\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0016401112079620361, 'time_algorithm_update': 0.0034022080898284913, 'loss': 0.45806796592473986, 'time_step': 0.005103318452835083, 'environment': 61.12494848765058}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m1000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:27.23\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DoubleDQN_20240211132717/model_1000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e92ef063e54d49970a481a15cd47bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:27.29\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDoubleDQN_20240211132717: epoch=2 step=2000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m2\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0016365442276000976, 'time_algorithm_update': 0.003261301517486572, 'loss': 0.34124353030323984, 'time_step': 0.0049538564682006835, 'environment': 64.23304194764127}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m2000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:27.29\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DoubleDQN_20240211132717/model_2000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3af22fc98741fd9a71453fbc07978b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:27.34\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDoubleDQN_20240211132717: epoch=3 step=3000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m3\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0016311161518096924, 'time_algorithm_update': 0.0032949364185333252, 'loss': 0.3580922721847892, 'time_step': 0.004983624219894409, 'environment': 66.25463734213118}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m3000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:27.34\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DoubleDQN_20240211132717/model_3000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d1db9321cb4c6590799e5919b89b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:27.40\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDoubleDQN_20240211132717: epoch=4 step=4000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m4\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0016907517910003662, 'time_algorithm_update': 0.0033993911743164062, 'loss': 0.3473753787204623, 'time_step': 0.005151326179504394, 'environment': 50.23706123935607}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m4000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:27.40\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DoubleDQN_20240211132717/model_4000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c1598590d94ef6b4dd60f693936ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:27.46\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDoubleDQN_20240211132717: epoch=5 step=5000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m5\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0017340548038482667, 'time_algorithm_update': 0.0034393789768218993, 'loss': 0.31005658972263334, 'time_step': 0.0052343418598175045, 'environment': 73.60489603758148}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m5000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:27.46\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DoubleDQN_20240211132717/model_5000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d0a02a248a45b1b965267c17b340ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:27.51\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDoubleDQN_20240211132717: epoch=6 step=6000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m6\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0016828913688659668, 'time_algorithm_update': 0.003341737508773804, 'loss': 0.28209242078661917, 'time_step': 0.005082767248153686, 'environment': 68.28980869199984}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m6000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:27.51\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DoubleDQN_20240211132717/model_6000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38f791efab14ea298a20ae8f8402cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:27.57\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDoubleDQN_20240211132717: epoch=7 step=7000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m7\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0016415410041809083, 'time_algorithm_update': 0.0032884111404418944, 'loss': 0.2926566868647933, 'time_step': 0.0049905295372009275, 'environment': 73.46979155534862}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m7000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:27.57\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DoubleDQN_20240211132717/model_7000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f40672687b483098e1fd911d65926f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:28.02\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDoubleDQN_20240211132717: epoch=8 step=8000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m8\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.001708214521408081, 'time_algorithm_update': 0.003382113456726074, 'loss': 0.28276054468005896, 'time_step': 0.005149126529693604, 'environment': 66.55570588755165}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m8000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:28.02\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DoubleDQN_20240211132717/model_8000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd602aa54e4401c9bf17baec5bdb4a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:28.08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDoubleDQN_20240211132717: epoch=9 step=9000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m9\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0017132432460784913, 'time_algorithm_update': 0.003343533992767334, 'loss': 0.019949583027279005, 'time_step': 0.005121784687042236, 'environment': 68.69245364081135}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m9000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:28.08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DoubleDQN_20240211132717/model_9000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722bd699f96c44b8b475a096b0f5245a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-02-11 13:28.15\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDoubleDQN_20240211132717: epoch=10 step=10000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m10\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0017888224124908447, 'time_algorithm_update': 0.0034689466953277586, 'loss': 0.00654171628144104, 'time_step': 0.005318193912506103, 'environment': 66.24656360124017}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m10000\u001b[0m\n",
      "\u001b[2m2024-02-11 13:28.15\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/DoubleDQN_20240211132717/model_10000.d3\u001b[0m\n",
      "63.05\n",
      "69.88\n",
      "67.38\n",
      "55.7\n",
      "56.58\n",
      "73.0\n",
      "72.5\n",
      "57.91\n",
      "60.85\n",
      "69.05\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "ddqn = d3rlpy.algos.DoubleDQNConfig().create(device='cuda')\n",
    "ddqn.fit(\n",
    "    dataset,\n",
    "    n_steps=10000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    evaluators={\n",
    "        'environment': d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "    },\n",
    ")\n",
    "\n",
    "# evaluate\n",
    "rewards = []\n",
    "for _ in range(10):\n",
    "    reward = d3rlpy.metrics.evaluate_qlearning_with_environment(ddqn, env)\n",
    "    rewards.append(reward)\n",
    "# print(np.round(rewards, 2))\n",
    "    \n",
    "for r in np.round(rewards, 2):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous CQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d3rlpy\n",
    "\n",
    "# prepare dataset\n",
    "dataset, env = d3rlpy.datasets.get_d4rl('hopper-medium-v0')\n",
    "\n",
    "# # prepare algorithm\n",
    "# cql = d3rlpy.algos.CQLConfig().create(device='cuda:0')\n",
    "\n",
    "# # train\n",
    "# cql.fit(\n",
    "#     dataset,\n",
    "#     n_steps=100000,\n",
    "#     evaluators={\"environment\": d3rlpy.metrics.EnvironmentEvaluator(env)},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.episodes[0].observations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d4rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d3rlpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
