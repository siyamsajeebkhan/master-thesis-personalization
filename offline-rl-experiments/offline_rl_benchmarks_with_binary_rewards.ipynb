{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces, Space\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import d3rlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActionSpace(Space):\n",
    "    def __init__(self, shape=None, dtype=None):\n",
    "        super().__init__(shape, dtype)\n",
    "        actions = np.arange(0.5, 5.5, 0.5)\n",
    "        self.actions_map = {idx: action for idx, action in enumerate(actions)}\n",
    "        self.actions = list(self.actions_map.keys())\n",
    "    \n",
    "class MovieLensEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, data, use_prev_temp_as_feature=False, van_specific_embeddings=None, pbar=None):\n",
    "        # print(\"__init__ method\")\n",
    "        # with open('../gym/data/mlens/mlens-test-trajectories-v1.pkl', 'rb') as f:\n",
    "        # with open(test_traj_path, 'rb') as f:\n",
    "        self.dataset = data\n",
    "\n",
    "        super(MovieLensEnv, self).__init__()\n",
    "        actions = np.arange(0.5, 5.5, 0.5)\n",
    "        self.actions_map = {idx: action for idx, action in enumerate(actions)}\n",
    "        self.current_step = 0\n",
    "        self.max_steps = sum(len(traj['observations']) for traj in self.dataset)\n",
    "        self.action_space = spaces.Discrete(10)  # You need to define CustomActionSpace\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.dataset[0]['observations'].shape[1],), dtype=np.float32)\n",
    "        self.sampled_idx = None\n",
    "        self.action = None\n",
    "        self.reward = None\n",
    "        self.pbar = pbar\n",
    "        self.total_steps = 0\n",
    "        self.use_prev_temp = use_prev_temp_as_feature\n",
    "        # self.idx_of_prev_temp_feat = np.where(self.dataset[0]['features'] == 'd_prev_target_temp')[0][0]\n",
    "        self.personalized_features = van_specific_embeddings\n",
    "\n",
    "    def step(self, action):\n",
    "        self.action = action\n",
    "        target_rating = self.dataset[self.sampled_idx]['targets'][self.current_step]\n",
    "        # print(f\"action taken in step: {action}\")\n",
    "        # print(f\"type of action: {type(action)}\")\n",
    "        pred_rating = self.actions_map[action]\n",
    "        # print(f\"pred_rating: {pred_rating}\")\n",
    "        \n",
    "        # print(f\"action: {self.action} | pred_rating: {pred_rating} | original_rating: {target_rating}\")\n",
    "        acc = 0\n",
    "        if pred_rating == target_rating:\n",
    "            acc = 1\n",
    "\n",
    "        # Rewards scheme 5\n",
    "        # -------------------------------\n",
    "        # error = abs(target_rating - pred_rating)\n",
    "        # self.reward = (1- (error / 4.5)) ** 2\n",
    "        # # -------------------------------\n",
    "        \n",
    "        # Binary Rewards scheme \n",
    "        # -------------------------------\n",
    "        if target_rating >= 3.5 and pred_rating >= 3.5:\n",
    "            self.reward = 1\n",
    "        else:\n",
    "            self.reward = 0\n",
    "        \n",
    "        # # -------------------------------\n",
    "        # # Reward for special cases\n",
    "        # if target_rating != pred_rating:\n",
    "        #     special_reward = reward\n",
    "        # else:\n",
    "        #     special_reward = 0\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # if self.pbar is not None:\n",
    "        #     self.pbar.set_description(f\"(idx, step): ({self.sampled_idx}, {self.current_step}) | True rating: {target_rating} | Predicted rating: {pred_rating} | reward: {self.reward:.2f}\")\n",
    "        #     # time.sleep(0.25)\n",
    "        self.current_step += 1\n",
    "        obs, done = self._next_observation()\n",
    "        self.total_steps += 1\n",
    "        # return obs, self.reward, done, acc, target_rating, pred_rating, self.total_steps\n",
    "        return obs, self.reward, done, None, {}\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        self.sampled_idx = random.randint(0, len(self.dataset) - 1)\n",
    "        self.current_step = 0\n",
    "        traj = self.dataset[self.sampled_idx]\n",
    "        user_id = traj['user_id']\n",
    "\n",
    "\n",
    "        obs = traj['observations'][self.current_step]\n",
    "\n",
    "        if self.personalized_features is not None:\n",
    "            obs = np.hstack((obs, self.personalized_features[user_id]))\n",
    "        \n",
    "        return obs, None\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        if self.dataset[self.sampled_idx]['terminals'][self.current_step]:\n",
    "            done = True\n",
    "            obs, _ = self.reset()\n",
    "            return obs, done\n",
    "        \n",
    "        traj = self.dataset[self.sampled_idx]\n",
    "        user_id = traj['user_id']\n",
    "        obs = traj['observations'][self.current_step]\n",
    "        if self.personalized_features is not None:\n",
    "            obs = np.hstack((obs, self.personalized_features[van_id]))\n",
    "        done = False\n",
    "        return obs, done\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "        \n",
    "    def get_true_temperature(self):\n",
    "        target_temperature = self.dataset[self.sampled_idx]['actions'][self.current_step]\n",
    "        target_temperature = self.actions_map[target_temperature]\n",
    "        return target_temperature\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "with open(\"../data/dt-datasets/movielens/processed-data/all_trajectories_with_concatenated_movname_genres_tags_userid_reward_of_scale_5.pkl\", 'rb') as f:\n",
    "    all_trajectories = pickle.load(f)\n",
    "# all_trajs_copy = deepcopy(all_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the size for the training set\n",
    "np.random.seed(42)\n",
    "trajectories = all_trajectories\n",
    "indices = {i for i in range(len(trajectories))}\n",
    "train_indices = list(np.random.choice(list(indices), size=round(0.7*len(indices)), replace=False))\n",
    "remaining_indices = indices.difference(train_indices)\n",
    "test_indices = remaining_indices\n",
    "\n",
    "print(f\"total train users: {len(train_indices)}\")\n",
    "print(f\"total test users: {len(test_indices)}\")\n",
    "\n",
    "train_trajectories = [trajectories[idx]for idx in train_indices]\n",
    "test_trajectories = [trajectories[idx]for idx in test_indices]\n",
    "\n",
    "print(\"Train set:\", len(train_trajectories))\n",
    "print(\"Test set:\", len(test_trajectories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward scheme 3: Binary reward: 1 or 0; 1 if liked 0 if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trajectories_copy = deepcopy(train_trajectories)\n",
    "threshold = 3.5\n",
    "for traj in train_trajectories_copy:\n",
    "    name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "    traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "    # print(traj['observations'].shape)\n",
    "    # errors = abs(highest_rating - traj['targets'])\n",
    "    traj['rewards'] = (traj['targets'] >= threshold).astype(int)\n",
    "\n",
    "\n",
    "test_trajectories_copy = deepcopy(test_trajectories)\n",
    "for traj in test_trajectories_copy:\n",
    "    name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "    traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "    # print(traj['observations'].shape)\n",
    "    # errors = abs(highest_rating - traj['targets'])\n",
    "    traj['rewards'] = (traj['targets'] >= threshold).astype(int)\n",
    "\n",
    "train_data_with_binary_rewards = train_trajectories_copy\n",
    "test_data_with_binary_rewards = test_trajectories_copy\n",
    "\n",
    "with open('data/train_data_with_binary_rewards.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data_with_binary_rewards, f)\n",
    "\n",
    "with open('data/test_data_with_binary_rewards.pkl', 'wb') as f:\n",
    "    pickle.dump(test_data_with_binary_rewards, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation for Discrete CQL\n",
    "import numpy as np\n",
    "observations_mlens = []\n",
    "observations_mlens = np.concatenate([ep['observations'] for ep in train_data_with_binary_rewards])\n",
    "actions_mlens = np.concatenate([ep['actions'] for ep in train_data_with_binary_rewards])\n",
    "rewards_mlens = np.concatenate([ep['rewards'] for ep in train_data_with_binary_rewards])\n",
    "terminals_mlens = np.concatenate([ep['terminals'] for ep in train_data_with_binary_rewards])\n",
    "\n",
    "timeouts = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.dataset import EpisodeGenerator\n",
    "\n",
    "episode_generator = EpisodeGenerator(\n",
    "    observations=observations_mlens,\n",
    "    actions=actions_mlens,\n",
    "    rewards=rewards_mlens,\n",
    "    terminals=terminals_mlens,\n",
    "    timeouts=timeouts,\n",
    ")\n",
    "\n",
    "episodes_generated_mlens = episode_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.dataset import ReplayBuffer, InfiniteBuffer\n",
    "\n",
    "dataset = ReplayBuffer(\n",
    "    InfiniteBuffer(),\n",
    "    episodes=episodes_generated_mlens,\n",
    "    transition_picker=None,\n",
    "    trajectory_slicer=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.episodes[0].observations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MovieLensEnv(test_data_with_binary_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discrete CQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start training\n",
    "# cql = d3rlpy.algos.DiscreteCQLConfig().create(device='cuda')\n",
    "# cql.fit(\n",
    "#     dataset,\n",
    "#     n_steps=10000,\n",
    "#     n_steps_per_epoch=1000,\n",
    "#     evaluators={\n",
    "#         'environment': d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# # evaluate\n",
    "# rewards = []\n",
    "# for _ in range(10):\n",
    "#     reward = d3rlpy.metrics.evaluate_qlearning_with_environment(cql, env)\n",
    "#     rewards.append(reward)\n",
    "# # print(np.round(rewards, 2))\n",
    "    \n",
    "# for r in np.round(rewards, 2):\n",
    "#     print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start training\n",
    "# dqn = d3rlpy.algos.DQNConfig().create(device='cuda')\n",
    "# dqn.fit(\n",
    "#     dataset,\n",
    "#     n_steps=10000,\n",
    "#     n_steps_per_epoch=1000,\n",
    "#     evaluators={\n",
    "#         'environment': d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# # evaluate\n",
    "# rewards = []\n",
    "# for _ in range(10):\n",
    "#     reward = d3rlpy.metrics.evaluate_qlearning_with_environment(dqn, env)\n",
    "#     rewards.append(reward)\n",
    "# # print(np.round(rewards, 2))\n",
    "    \n",
    "# for r in np.round(rewards, 2):\n",
    "#     print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start training\n",
    "# ddqn = d3rlpy.algos.DoubleDQNConfig().create(device='cuda')\n",
    "# ddqn.fit(\n",
    "#     dataset,\n",
    "#     n_steps=10000,\n",
    "#     n_steps_per_epoch=1000,\n",
    "#     evaluators={\n",
    "#         'environment': d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# # evaluate\n",
    "# rewards = []\n",
    "# for _ in range(10):\n",
    "#     reward = d3rlpy.metrics.evaluate_qlearning_with_environment(ddqn, env)\n",
    "#     rewards.append(reward)\n",
    "# # print(np.round(rewards, 2))\n",
    "    \n",
    "# for r in np.round(rewards, 2):\n",
    "#     print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "bc = d3rlpy.algos.DiscreteBCConfig().create(device='cuda')\n",
    "bc.fit(\n",
    "    dataset,\n",
    "    n_steps=10000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    evaluators={\n",
    "        'environment': d3rlpy.metrics.EnvironmentEvaluator(env),\n",
    "    },\n",
    ")\n",
    "\n",
    "# evaluate\n",
    "rewards = []\n",
    "for _ in range(10):\n",
    "    reward = d3rlpy.metrics.evaluate_qlearning_with_environment(bc, env)\n",
    "    rewards.append(reward)\n",
    "# print(np.round(rewards, 2))\n",
    "    \n",
    "for r in np.round(rewards, 2):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous CQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d3rlpy\n",
    "\n",
    "# prepare dataset\n",
    "dataset, env = d3rlpy.datasets.get_d4rl('hopper-medium-v0')\n",
    "\n",
    "# # prepare algorithm\n",
    "# cql = d3rlpy.algos.CQLConfig().create(device='cuda:0')\n",
    "\n",
    "# # train\n",
    "# cql.fit(\n",
    "#     dataset,\n",
    "#     n_steps=100000,\n",
    "#     evaluators={\"environment\": d3rlpy.metrics.EnvironmentEvaluator(env)},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.episodes[0].observations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d4rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d3rlpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
