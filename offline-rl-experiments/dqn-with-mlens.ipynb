{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import base64\n",
    "import pickle\n",
    "import zlib\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "from torch import nn, tensor\n",
    "from collections import deque\n",
    "from gym.spaces import Box, Discrete\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import CnnPolicy\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env.base_vec_env import VecEnv\n",
    "# from stable_baselines3.common.policies import BasePolicy, register_policy\n",
    "import time\n",
    "from datetime import date\n",
    "from matplotlib import pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "import torch\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movielens Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces, Space\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "class CustomActionSpace(Space):\n",
    "    def __init__(self, shape=None, dtype=None):\n",
    "        super().__init__(shape, dtype)\n",
    "        actions = np.arange(0.5, 5.5, 0.5)\n",
    "        self.actions_map = {idx: action for idx, action in enumerate(actions)}\n",
    "        self.actions = list(self.actions_map.keys())\n",
    "    \n",
    "class MovieLensEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, data, use_prev_temp_as_feature=False, van_specific_embeddings=None, pbar=None):\n",
    "        # print(\"__init__ method\")\n",
    "        # with open('../gym/data/mlens/mlens-test-trajectories-v1.pkl', 'rb') as f:\n",
    "        # with open(test_traj_path, 'rb') as f:\n",
    "        self.dataset = data\n",
    "\n",
    "        super(MovieLensEnv, self).__init__()\n",
    "        actions = np.arange(0.5, 5.5, 0.5)\n",
    "        self.actions_map = {idx: action for idx, action in enumerate(actions)}\n",
    "        self.current_step = 0\n",
    "        self.max_steps = sum(len(traj['observations']) for traj in self.dataset)\n",
    "        self.action_space = spaces.Discrete(10)  # You need to define CustomActionSpace\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.dataset[0]['observations'].shape[1],), dtype=np.float32)\n",
    "        self.sampled_idx = None\n",
    "        self.action = None\n",
    "        self.reward = None\n",
    "        self.pbar = pbar\n",
    "        self.total_steps = 0\n",
    "        self.use_prev_temp = use_prev_temp_as_feature\n",
    "        # self.idx_of_prev_temp_feat = np.where(self.dataset[0]['features'] == 'd_prev_target_temp')[0][0]\n",
    "        self.personalized_features = van_specific_embeddings\n",
    "\n",
    "    def step(self, action):\n",
    "        self.action = action\n",
    "        target_rating = self.dataset[self.sampled_idx]['targets'][self.current_step]\n",
    "        # print(f\"action taken in step: {action}\")\n",
    "        # print(f\"type of action: {type(action)}\")\n",
    "        pred_rating = self.actions_map[action]\n",
    "        # print(f\"pred_rating: {pred_rating}\")\n",
    "        \n",
    "        # print(f\"action: {self.action} | pred_rating: {pred_rating} | original_rating: {target_rating}\")\n",
    "        acc = 0\n",
    "        if pred_rating == target_rating:\n",
    "            acc = 1\n",
    "\n",
    "        # Rewards scheme 5\n",
    "        # -------------------------------\n",
    "        # error = abs(target_rating - pred_rating)\n",
    "        # self.reward = (1- (error / 4.5)) ** 2\n",
    "        # # -------------------------------\n",
    "        \n",
    "        # Binary Rewards scheme \n",
    "        # -------------------------------\n",
    "        if target_rating >= 3.5 and pred_rating >= 3.5:\n",
    "            self.reward = 1\n",
    "        else:\n",
    "            self.reward = 0\n",
    "        \n",
    "        # # -------------------------------\n",
    "        # # Reward for special cases\n",
    "        # if target_rating != pred_rating:\n",
    "        #     special_reward = reward\n",
    "        # else:\n",
    "        #     special_reward = 0\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # if self.pbar is not None:\n",
    "        #     self.pbar.set_description(f\"(idx, step): ({self.sampled_idx}, {self.current_step}) | True rating: {target_rating} | Predicted rating: {pred_rating} | reward: {self.reward:.2f}\")\n",
    "        #     # time.sleep(0.25)\n",
    "        self.current_step += 1\n",
    "        obs, done = self._next_observation()\n",
    "        self.total_steps += 1\n",
    "        # return obs, self.reward, done, acc, target_rating, pred_rating, self.total_steps\n",
    "        return obs, self.reward, done, None, {}\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        self.sampled_idx = random.randint(0, len(self.dataset) - 1)\n",
    "        self.current_step = 0\n",
    "        traj = self.dataset[self.sampled_idx]\n",
    "        user_id = traj['user_id']\n",
    "\n",
    "\n",
    "        obs = traj['observations'][self.current_step]\n",
    "\n",
    "        if self.personalized_features is not None:\n",
    "            obs = np.hstack((obs, self.personalized_features[user_id]))\n",
    "        \n",
    "        return obs, None\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        if self.dataset[self.sampled_idx]['terminals'][self.current_step]:\n",
    "            done = True\n",
    "            obs, _ = self.reset()\n",
    "            return obs, done\n",
    "        \n",
    "        traj = self.dataset[self.sampled_idx]\n",
    "        user_id = traj['user_id']\n",
    "        obs = traj['observations'][self.current_step]\n",
    "        if self.personalized_features is not None:\n",
    "            obs = np.hstack((obs, self.personalized_features[van_id]))\n",
    "        done = False\n",
    "        return obs, done\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "        \n",
    "    def get_true_temperature(self):\n",
    "        target_temperature = self.dataset[self.sampled_idx]['actions'][self.current_step]\n",
    "        target_temperature = self.actions_map[target_temperature]\n",
    "        return target_temperature\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Versions of the data with different reward schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "with open(\"../data/dt-datasets/movielens/processed-data/all_trajectories_with_concatenated_movname_genres_tags_userid_reward_of_scale_5.pkl\", 'rb') as f:\n",
    "    all_trajectories = pickle.load(f)\n",
    "# all_trajs_copy = deepcopy(all_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the size for the training set\n",
    "np.random.seed(42)\n",
    "trajectories = all_trajectories\n",
    "indices = {i for i in range(len(trajectories))}\n",
    "train_indices = list(np.random.choice(list(indices), size=round(0.7*len(indices)), replace=False))\n",
    "remaining_indices = indices.difference(train_indices)\n",
    "test_indices = remaining_indices\n",
    "\n",
    "print(f\"total train users: {len(train_indices)}\")\n",
    "print(f\"total test users: {len(test_indices)}\")\n",
    "\n",
    "train_trajectories = [trajectories[idx]for idx in train_indices]\n",
    "test_trajectories = [trajectories[idx]for idx in test_indices]\n",
    "\n",
    "print(\"Train set:\", len(train_trajectories))\n",
    "print(\"Test set:\", len(test_trajectories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward scheme 1: Naive reward: all observations marked with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_copy = deepcopy(train_data)\n",
    "train_trajectories_copy = deepcopy(train_trajectories)\n",
    "for traj in train_trajectories_copy:\n",
    "    name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "    traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "    # print(traj['observations'].shape)\n",
    "    traj['rewards'] =  np.ones_like(traj['rewards'])\n",
    "\n",
    "\n",
    "\n",
    "test_trajectories_copy = deepcopy(test_trajectories)\n",
    "for traj in test_trajectories_copy:\n",
    "    name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "    traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "    traj['rewards'] =  np.ones_like(traj['rewards'])\n",
    "\n",
    "    \n",
    "train_data_with_naive_rewards = train_trajectories_copy\n",
    "test_data_with_naive_rewards = test_trajectories_copy\n",
    "\n",
    "with open('data/train_data_with_naive_rewards.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data_with_naive_rewards, f)\n",
    "\n",
    "with open('data/test_data_with_naive_rewards.pkl', 'wb') as f:\n",
    "    pickle.dump(test_data_with_naive_rewards, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward scheme 2: Proximity based reward: scaled reward in the range of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_trajectories_copy = deepcopy(train_trajectories)\n",
    "# highest_rating = 5.0\n",
    "# for traj in train_trajectories_copy:\n",
    "#     name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "#     traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "#     # print(traj['observations'].shape)\n",
    "#     errors = abs(highest_rating - traj['targets'])\n",
    "#     traj['rewards'] = (1- (errors / 4.5)) ** 2\n",
    "\n",
    "# test_trajectories_copy = deepcopy(test_trajectories)\n",
    "# for traj in test_trajectories_copy:\n",
    "#     name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "#     traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "#     # print(traj['observations'].shape)\n",
    "#     errors = abs(highest_rating - traj['targets'])\n",
    "#     traj['rewards'] = (1- (errors / 4.5)) ** 2\n",
    "\n",
    "# train_data_with_range_rewards = train_trajectories_copy\n",
    "# test_data_with_range_rewards = test_trajectories_copy\n",
    "\n",
    "# with open('data/train_data_with_range_rewards.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_data_with_range_rewards, f)\n",
    "\n",
    "# with open('data/test_data_with_range_rewards.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_data_with_range_rewards, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward scheme 3: Binary reward: 1 or 0; 1 if liked 0 if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_trajectories_copy = deepcopy(train_trajectories)\n",
    "# threshold = 3.5\n",
    "# for traj in train_trajectories_copy:\n",
    "#     name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "#     traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "#     # print(traj['observations'].shape)\n",
    "#     # errors = abs(highest_rating - traj['targets'])\n",
    "#     traj['rewards'] = (traj['targets'] >= threshold).astype(int)\n",
    "\n",
    "\n",
    "# test_trajectories_copy = deepcopy(test_trajectories)\n",
    "# for traj in test_trajectories_copy:\n",
    "#     name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "#     traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "#     # print(traj['observations'].shape)\n",
    "#     # errors = abs(highest_rating - traj['targets'])\n",
    "#     traj['rewards'] = (traj['targets'] >= threshold).astype(int)\n",
    "\n",
    "# train_data_with_binary_rewards = train_trajectories_copy\n",
    "# test_data_with_binary_rewards = test_trajectories_copy\n",
    "\n",
    "# with open('data/train_data_with_binary_rewards.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_data_with_binary_rewards, f)\n",
    "\n",
    "# with open('data/test_data_with_binary_rewards.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_data_with_binary_rewards, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with RL methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import numpy as np\n",
    "\n",
    "# Create a function to instantiate your custom environment\n",
    "def create_custom_env(data):\n",
    "    return MovieLensEnv(data)  # Instantiate your custom environment\n",
    "\n",
    "# Create a vectorized environment\n",
    "# env = make_vec_env(create_custom_env, n_envs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_data_with_binary_rewards.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "train_env = create_custom_env(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "model = DQN(\"MlpPolicy\", train_env, verbose=1, seed=123)\n",
    "model.learn(total_timesteps=10000, log_interval=4)\n",
    "# model.save(\"dqn_cartpole\")\n",
    "\n",
    "# del model # remove to demonstrate saving and loading\n",
    "\n",
    "# model = DQN.load(\"dqn_cartpole\")\n",
    "\n",
    "# obs, info = env.reset()\n",
    "# while True:\n",
    "#     action, _states = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, terminated, truncated, info = env.step(action)\n",
    "#     if terminated or truncated:\n",
    "#         obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test_data_with_binary_rewards.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "max_ep_len = 100\n",
    "episode_rewards = []\n",
    "\n",
    "for i in range(10):\n",
    "    test_env = create_custom_env(test_data)\n",
    "    obs = test_env.reset()[0]\n",
    "    rewards = 0\n",
    "    for t in range(max_ep_len):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        # print(f\"action: {action}, type: {type(action)}, shape: {action.shape}\")\n",
    "        obs, reward, done, _, info = test_env.step(action.item())\n",
    "        # print(f\"reward: {reward}\")\n",
    "        rewards += reward\n",
    "        if done:\n",
    "            obs, _ = test_env.reset()\n",
    "    episode_rewards.append(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_ep_rewards = [r * 100/ 100 for r in episode_rewards]\n",
    "scaled_ep_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another evaluation policy\n",
    "\n",
    "# from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# # # Evaluate the trained model\n",
    "# with open('data/test_data_with_naive_rewards.pkl', 'rb') as f:\n",
    "#     test_data = pickle.load(f)\n",
    "# test_env = Monitor(create_custom_env(test_data))\n",
    "\n",
    "# mean_reward, another = evaluate_policy(model, test_env, n_eval_episodes=5, return_episode_rewards=True)\n",
    "# mean_reward, another\n",
    "# # # Use the trained model to interact with the environment\n",
    "# # # test_env = make_vec_env(create_custom_env, num_envs=1)\n",
    "# # test_env = create_custom_env(test_data)\n",
    "# # obs = test_env.reset()[0]\n",
    "# # # print(f\"obs: {obs}; shape: {obs.shape}\")\n",
    "# # rewards = []\n",
    "# # for _ in range(1000):\n",
    "# #     action, _ = model.predict(obs, deterministic=True)\n",
    "# #     # print(f\"action: {action}, type: {type(action)}, shape: {action.shape}\")\n",
    "# #     obs, reward, done, _, info = test_env.step(action.item())\n",
    "# #     print(f\"reward: {reward}\")\n",
    "# #     rewards.append(reward)\n",
    "# #     if done:\n",
    "# #         obs, _ = test_env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "offline-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
