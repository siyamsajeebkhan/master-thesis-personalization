{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import base64\n",
    "import pickle\n",
    "import zlib\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "from torch import nn, tensor\n",
    "from collections import deque\n",
    "from gym.spaces import Box, Discrete\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import CnnPolicy\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env.base_vec_env import VecEnv\n",
    "# from stable_baselines3.common.policies import BasePolicy, register_policy\n",
    "import time\n",
    "from datetime import date\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movielens Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces, Space\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class CustomActionSpace(Space):\n",
    "    def __init__(self, shape=None, dtype=None):\n",
    "        super().__init__(shape, dtype)\n",
    "        actions = np.arange(0.5, 5.5, 0.5)\n",
    "        self.actions_map = {idx: action for idx, action in enumerate(actions)}\n",
    "        self.actions = list(self.actions_map.keys())\n",
    "    \n",
    "class MovieLensEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, data, action_vocab, movie_embed_to_id, movies_ratings_and_tags, use_prev_temp_as_feature=False, van_specific_embeddings=None, pbar=None):\n",
    "        self.dataset = data\n",
    "\n",
    "        super(MovieLensEnv, self).__init__()\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.max_steps = sum(len(traj['observations']) for traj in self.dataset)\n",
    "        self.act_dim = self.dataset[0]['actions'].shape[1]\n",
    "\n",
    "        self.low_action_space = [-1.0] * 8  # Lower bounds for each dimension\n",
    "        self.high_action_space = [1.0] * 8  # Upper bounds for each dimension\n",
    "\n",
    "        # self.action_space = spaces.box.Box(low=-1, high=1, shape=(self.act_dim,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=np.array(self.low_action_space), high=np.array(self.high_action_space), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.dataset[0]['observations'].shape[1],), dtype=np.float32)\n",
    "        # self.observation_space = Space(shape=(self.dataset[0]['observations'].shape[1],), dtype=np.float32)\n",
    "        self.sampled_idx = None\n",
    "        self.action = None\n",
    "        self.reward = None\n",
    "        self.pbar = pbar\n",
    "        self.total_steps = 0\n",
    "        self.use_prev_temp = use_prev_temp_as_feature\n",
    "        self.personalized_features = van_specific_embeddings\n",
    "        self.movies_ratings_and_tags = movies_ratings_and_tags\n",
    "        self.movie_embed_to_id = movie_embed_to_id\n",
    "        self.action_vocab = action_vocab\n",
    "\n",
    "    def step(self, action):\n",
    "        # print(f\"Back to stepping\")\n",
    "        raw_action = action\n",
    "\n",
    "        # Need to create a mapping between actions and rewards\n",
    "        # If the movie is actually rated by the user: then the reward is the user's rating\n",
    "        # Else, the reward is the average rating of all users for the movie\n",
    "        # if np.isnan(raw_action).sum() > 0:\n",
    "        #     print(f\"raw_action: {raw_action}\")\n",
    "        # print(f\"raw_action: {raw_action}\")\n",
    "        similarities = cosine_similarity(raw_action.reshape(1, -1), self.action_vocab)\n",
    "\n",
    "        # Find indices of closest neighbors for each prediction in the batch\n",
    "        closest_indices = np.argmax(similarities, axis=1)\n",
    "\n",
    "        # print(f\"closest_indices.shape: {closest_indices.shape}\")\n",
    "        closest_vectors = self.action_vocab[closest_indices]\n",
    "\n",
    "        final_action = closest_vectors.reshape(-1)\n",
    "\n",
    "        # print(f\"final action: {final_action}, shape: {final_action.shape}\")\n",
    "        # First let's find the movie_id from the embed\n",
    "        movie_id = self.movie_embed_to_id[tuple(final_action)]\n",
    "        user_id = self.dataset[self.sampled_idx]['user_id']\n",
    "\n",
    "        rating_by_user = self.movies_ratings_and_tags[(self.movies_ratings_and_tags['movieId'] == movie_id) & (self.movies_ratings_and_tags['userId'] == user_id)]['rating']\n",
    "\n",
    "        if rating_by_user.any():\n",
    "            self.reward = rating_by_user.values[0]\n",
    "        else:\n",
    "            self.reward = self.movies_ratings_and_tags[self.movies_ratings_and_tags['movieId'] == movie_id]['rating_global'].mean()\n",
    "\n",
    "        # Round to nearest 0.5\n",
    "        def round_to_nearest_half(number):\n",
    "            return round(number * 2) / 2\n",
    "\n",
    "        # Example usage\n",
    "        self.reward = round_to_nearest_half(self.reward)\n",
    "        done = False\n",
    "        \n",
    "\n",
    "        if self.pbar is not None:\n",
    "            self.pbar.set_description(f\"(idx, step): ({self.sampled_idx}, {self.current_step}) | predicted movie_id: {movie_id} | reward: {self.reward:.2f}\")\n",
    "            # time.sleep(0.25)\n",
    "        self.current_step += 1\n",
    "        obs, done = self._next_observation()\n",
    "\n",
    "        print(f\"Step: {self.total_steps} | Predicted movie_id: {movie_id} | reward: {self.reward}\")\n",
    "        self.total_steps += 1\n",
    "\n",
    "        return obs, self.reward, done, None, {}\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        # print(f\"Reset method called\")\n",
    "        self.sampled_idx = random.randint(0, len(self.dataset) - 1)\n",
    "        self.current_step = 0\n",
    "        traj = self.dataset[self.sampled_idx]\n",
    "        user_id = traj['user_id']\n",
    "\n",
    "        obs = traj['observations'][self.current_step]\n",
    "\n",
    "        if self.personalized_features is not None:\n",
    "            obs = np.hstack((obs, self.personalized_features[user_id]))\n",
    "        # print(f\"returning from reset()\")print()\n",
    "        # return obs, None\n",
    "        return obs, False\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        if self.dataset[self.sampled_idx]['terminals'][self.current_step]:\n",
    "            done = True\n",
    "            obs, _ = self.reset()\n",
    "            return obs, done\n",
    "        \n",
    "        traj = self.dataset[self.sampled_idx]\n",
    "        user_id = traj['user_id']\n",
    "        obs = traj['observations'][self.current_step]\n",
    "        if self.personalized_features is not None:\n",
    "            obs = np.hstack((obs, self.personalized_features[user_id]))\n",
    "        done = False\n",
    "        return obs, done\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "        \n",
    "    def get_true_temperature(self):\n",
    "        target_temperature = self.dataset[self.sampled_idx]['actions'][self.current_step]\n",
    "        target_temperature = self.action_space.actions_map[target_temperature]\n",
    "        return target_temperature\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Versions of the data with different reward schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copy import deepcopy\n",
    "# with open(\"../data/dt-datasets/movielens/processed-data/all_trajectories_with_concatenated_movname_genres_tags_userid_reward_of_scale_5.pkl\", 'rb') as f:\n",
    "#     all_trajectories = pickle.load(f)\n",
    "# # all_trajs_copy = deepcopy(all_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the size for the training set\n",
    "# np.random.seed(42)\n",
    "# trajectories = all_trajectories\n",
    "# indices = {i for i in range(len(trajectories))}\n",
    "# train_indices = list(np.random.choice(list(indices), size=round(0.7*len(indices)), replace=False))\n",
    "# remaining_indices = indices.difference(train_indices)\n",
    "# test_indices = remaining_indices\n",
    "\n",
    "# print(f\"total train users: {len(train_indices)}\")\n",
    "# print(f\"total test users: {len(test_indices)}\")\n",
    "\n",
    "# train_trajectories = [trajectories[idx]for idx in train_indices]\n",
    "# test_trajectories = [trajectories[idx]for idx in test_indices]\n",
    "\n",
    "# print(\"Train set:\", len(train_trajectories))\n",
    "# print(\"Test set:\", len(test_trajectories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward scheme 1: Naive reward: all observations marked with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_data_copy = deepcopy(train_data)\n",
    "# train_trajectories_copy = deepcopy(train_trajectories)\n",
    "# for traj in train_trajectories_copy:\n",
    "#     name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "#     traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "#     # print(traj['observations'].shape)\n",
    "#     traj['rewards'] =  np.ones_like(traj['rewards'])\n",
    "\n",
    "\n",
    "\n",
    "# test_trajectories_copy = deepcopy(test_trajectories)\n",
    "# for traj in test_trajectories_copy:\n",
    "#     name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "#     traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "#     traj['rewards'] =  np.ones_like(traj['rewards'])\n",
    "\n",
    "    \n",
    "# train_data_with_naive_rewards = train_trajectories_copy\n",
    "# test_data_with_naive_rewards = test_trajectories_copy\n",
    "\n",
    "# with open('data/train_data_with_naive_rewards.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_data_with_naive_rewards, f)\n",
    "\n",
    "# with open('data/test_data_with_naive_rewards.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_data_with_naive_rewards, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward scheme 2: Proximity based reward: scaled reward in the range of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_trajectories_copy = deepcopy(train_trajectories)\n",
    "# highest_rating = 5.0\n",
    "# for traj in train_trajectories_copy:\n",
    "#     name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "#     traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "#     # print(traj['observations'].shape)\n",
    "#     errors = abs(highest_rating - traj['targets'])\n",
    "#     traj['rewards'] = (1- (errors / 4.5)) ** 2\n",
    "\n",
    "# test_trajectories_copy = deepcopy(test_trajectories)\n",
    "# for traj in test_trajectories_copy:\n",
    "#     name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "#     traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "#     # print(traj['observations'].shape)\n",
    "#     errors = abs(highest_rating - traj['targets'])\n",
    "#     traj['rewards'] = (1- (errors / 4.5)) ** 2\n",
    "\n",
    "# train_data_with_range_rewards = train_trajectories_copy\n",
    "# test_data_with_range_rewards = test_trajectories_copy\n",
    "\n",
    "# with open('data/train_data_with_range_rewards.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_data_with_range_rewards, f)\n",
    "\n",
    "# with open('data/test_data_with_range_rewards.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_data_with_range_rewards, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward scheme 3: Binary reward: 1 or 0; 1 if liked 0 if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_trajectories_copy = deepcopy(train_trajectories)\n",
    "# threshold = 3.5\n",
    "# for traj in train_trajectories_copy:\n",
    "#     name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "#     traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "#     # print(traj['observations'].shape)\n",
    "#     errors = abs(highest_rating - traj['targets'])\n",
    "#     traj['rewards'] = (traj['targets'] >= threshold).astype(int)\n",
    "\n",
    "\n",
    "# test_trajectories_copy = deepcopy(test_trajectories)\n",
    "# for traj in test_trajectories_copy:\n",
    "#     name_and_genre_embeds = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "#     traj['observations'] = np.concatenate((name_and_genre_embeds, traj['observations'][:, 3*768:]), axis=1)\n",
    "#     # print(traj['observations'].shape)\n",
    "#     errors = abs(highest_rating - traj['targets'])\n",
    "#     traj['rewards'] = (traj['targets'] >= threshold).astype(int)\n",
    "\n",
    "# train_data_with_binary_rewards = train_trajectories_copy\n",
    "# test_data_with_binary_rewards = test_trajectories_copy\n",
    "\n",
    "# with open('data/train_data_with_binary_rewards.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_data_with_binary_rewards, f)\n",
    "\n",
    "# with open('data/test_data_with_binary_rewards.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_data_with_binary_rewards, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with RL methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import numpy as np\n",
    "\n",
    "# Create a function to instantiate your custom environment\n",
    "def create_custom_env(data, action_vocab, movie_embed_to_id, movies_ratings_and_tags):\n",
    "    return MovieLensEnv(data, action_vocab, movie_embed_to_id, movies_ratings_and_tags)  # Instantiate your custom environment\n",
    "\n",
    "# Create a vectorized environment\n",
    "# env = make_vec_env(create_custom_env, n_envs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_embed_shape = 8\n",
    "\n",
    "train_data_path = f'../data/dt-datasets/movielens/train-test-sets/mlens-train-trajectories-movies-as-actions-reduced-from-768-to-{action_embed_shape}_with_tanh.pkl'\n",
    "test_data_path = f'../data/dt-datasets/movielens/train-test-sets/mlens-test-trajectories-movies-as-actions-reduced-from-768-to-{action_embed_shape}_with_tanh.pkl'\n",
    "\n",
    "movies_ratings_and_tags = pd.read_csv(\"../data/movies_ratings_and_tags_mlens_small.csv\")\n",
    "movies_ratings_and_tags.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "overall_ratings = movies_ratings_and_tags.groupby('movieId')['rating'].mean().reset_index()\n",
    "# Merge the overall ratings back into the original DataFrame\n",
    "movies_ratings_and_tags = movies_ratings_and_tags.merge(overall_ratings, on='movieId', suffixes=('', '_global'))\n",
    "\n",
    "movie_embeds_to_id_map_path = f\"../data/dt-datasets/movielens/processed-data/movie_embed_with_shape_{action_embed_shape}_to_id_mapping_with_tanh.pkl\"\n",
    "\n",
    "action_vocab_path = f\"../data/dt-datasets/movielens/processed-data/action_vocab_of_shape_{action_embed_shape}_with_tanh.pkl\"\n",
    "\n",
    "with open(train_data_path, 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "with open(test_data_path, 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "# # Try normalizing the train_data between -1 and 1\n",
    "# for traj in train_data:\n",
    "#     traj['actions'] = 2 * (traj['actions'] - np.min(traj['actions'])) / (np.max(traj['actions']) - np.min(traj['actions'])) - 1\n",
    "#     traj['observations'] = 2 * (traj['observations'] - np.min(traj['observations'])) / (np.max(traj['actions']) - np.min(traj['actions'])) - 1\n",
    "\n",
    "\n",
    "with open(movie_embeds_to_id_map_path, 'rb') as f:\n",
    "    movie_embed_to_id = pickle.load(f)\n",
    "\n",
    "with open(action_vocab_path, 'rb') as f:\n",
    "    action_vocab = pickle.load(f)\n",
    "\n",
    "train_env = Monitor(create_custom_env(data=train_data, action_vocab=action_vocab, movie_embed_to_id=movie_embed_to_id, movies_ratings_and_tags=movies_ratings_and_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_obs = []\n",
    "# all_actions = []\n",
    "# for traj in train_data:\n",
    "#     all_obs.append(traj['observations'])\n",
    "#     all_actions.append(traj['actions'])\n",
    "\n",
    "# obss = np.concatenate(all_obs)\n",
    "# actions = np.concatenate(all_actions)\n",
    "# obss.shape, actions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env.action_space.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = train_env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "n_actions\n",
    "\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", train_env, action_noise=action_noise, verbose=1, seed=123, learning_rate=1e-4)\n",
    "model.policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=10000, log_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate DDPG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ep_len = 100\n",
    "episode_rewards = []\n",
    "\n",
    "for i in range(10):\n",
    "    test_env = create_custom_env(test_data, action_vocab=action_vocab, movie_embed_to_id=movie_embed_to_id, movies_ratings_and_tags=movies_ratings_and_tags)\n",
    "    obs = test_env.reset()[0]\n",
    "    rewards = 0\n",
    "    for t in range(max_ep_len):\n",
    "        action, _ = model.predict(obs)\n",
    "        # print(f\"action: {action}, type: {type(action)}, shape: {action.shape}\")\n",
    "        obs, reward, done, _, info = test_env.step(action)\n",
    "        # print(f\"reward: {reward}\")\n",
    "        rewards += reward\n",
    "        if done:\n",
    "            obs, _ = test_env.reset()\n",
    "    episode_rewards.append(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_ep_rewards = [r * 100/ 100 for r in episode_rewards]\n",
    "scaled_ep_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another evaluation policy\n",
    "\n",
    "# from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# # # Evaluate the trained model\n",
    "# with open('data/test_data_with_naive_rewards.pkl', 'rb') as f:\n",
    "#     test_data = pickle.load(f)\n",
    "# test_env = Monitor(create_custom_env(test_data))\n",
    "\n",
    "# mean_reward, another = evaluate_policy(model, test_env, n_eval_episodes=5, return_episode_rewards=True)\n",
    "# mean_reward, another\n",
    "# # # Use the trained model to interact with the environment\n",
    "# # # test_env = make_vec_env(create_custom_env, num_envs=1)\n",
    "# # test_env = create_custom_env(test_data)\n",
    "# # obs = test_env.reset()[0]\n",
    "# # # print(f\"obs: {obs}; shape: {obs.shape}\")\n",
    "# # rewards = []\n",
    "# # for _ in range(1000):\n",
    "# #     action, _ = model.predict(obs, deterministic=True)\n",
    "# #     # print(f\"action: {action}, type: {type(action)}, shape: {action.shape}\")\n",
    "# #     obs, reward, done, _, info = test_env.step(action.item())\n",
    "# #     print(f\"reward: {reward}\")\n",
    "# #     rewards.append(reward)\n",
    "# #     if done:\n",
    "# #         obs, _ = test_env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "offline-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
