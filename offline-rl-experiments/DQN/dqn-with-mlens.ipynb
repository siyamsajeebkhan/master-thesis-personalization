{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import base64\n",
    "import pickle\n",
    "import zlib\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "from torch import nn, tensor\n",
    "from collections import deque\n",
    "from gym.spaces import Box, Discrete\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import CnnPolicy\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env.base_vec_env import VecEnv\n",
    "# from stable_baselines3.common.policies import BasePolicy, register_policy\n",
    "import time\n",
    "from datetime import date\n",
    "from matplotlib import pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "import torch\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5,\n",
       " 1: 1.0,\n",
       " 2: 1.5,\n",
       " 3: 2.0,\n",
       " 4: 2.5,\n",
       " 5: 3.0,\n",
       " 6: 3.5,\n",
       " 7: 4.0,\n",
       " 8: 4.5,\n",
       " 9: 5.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = np.arange(0.5, 5.5, 0.5)\n",
    "actions_map = {idx: action for idx, action in enumerate(actions)}\n",
    "actions_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_map[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces, Space\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "class CustomActionSpace(Space):\n",
    "    def __init__(self, shape=None, dtype=None):\n",
    "        super().__init__(shape, dtype)\n",
    "        actions = np.arange(0.5, 5.5, 0.5)\n",
    "        self.actions_map = {idx: action for idx, action in enumerate(actions)}\n",
    "        self.actions = list(self.actions_map.keys())\n",
    "    \n",
    "class MovieLensEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, data, use_prev_temp_as_feature=False, van_specific_embeddings=None, pbar=None):\n",
    "        # print(\"__init__ method\")\n",
    "        # with open('../gym/data/mlens/mlens-test-trajectories-v1.pkl', 'rb') as f:\n",
    "        # with open(test_traj_path, 'rb') as f:\n",
    "        self.dataset = data\n",
    "\n",
    "        super(MovieLensEnv, self).__init__()\n",
    "        actions = np.arange(0.5, 5.5, 0.5)\n",
    "        self.actions_map = {idx: action for idx, action in enumerate(actions)}\n",
    "        self.current_step = 0\n",
    "        self.max_steps = sum(len(traj['observations']) for traj in self.dataset)\n",
    "        self.action_space = spaces.Discrete(10)  # You need to define CustomActionSpace\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.dataset[0]['observations'].shape[1],), dtype=np.float32)\n",
    "        self.sampled_idx = None\n",
    "        self.action = None\n",
    "        self.reward = None\n",
    "        self.pbar = pbar\n",
    "        self.total_steps = 0\n",
    "        self.use_prev_temp = use_prev_temp_as_feature\n",
    "        # self.idx_of_prev_temp_feat = np.where(self.dataset[0]['features'] == 'd_prev_target_temp')[0][0]\n",
    "        self.personalized_features = van_specific_embeddings\n",
    "\n",
    "    def step(self, action):\n",
    "        self.action = action\n",
    "        target_rating = self.dataset[self.sampled_idx]['targets'][self.current_step]\n",
    "        # print(f\"action taken in step: {action}\")\n",
    "        # print(f\"type of action: {type(action)}\")\n",
    "        pred_rating = self.actions_map[action]\n",
    "        # print(f\"pred_rating: {pred_rating}\")\n",
    "        \n",
    "        acc = 0\n",
    "        if pred_rating == target_rating:\n",
    "            acc = 1\n",
    "\n",
    "        # Rewards scheme 5\n",
    "        # -------------------------------\n",
    "        error = abs(target_rating - pred_rating)\n",
    "        self.reward = (1- (error / 4.5)) ** 2\n",
    "        # # -------------------------------\n",
    "        \n",
    "        # # Binary Rewards scheme \n",
    "        # # -------------------------------\n",
    "        # if target_rating >= 3.5 and pred_rating >= 3.5:\n",
    "        #     self.reward = 1\n",
    "        # else:\n",
    "        #     self.reward = 0\n",
    "        \n",
    "        # # -------------------------------\n",
    "        # # Reward for special cases\n",
    "        # if target_rating != pred_rating:\n",
    "        #     special_reward = reward\n",
    "        # else:\n",
    "        #     special_reward = 0\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # if self.pbar is not None:\n",
    "        #     self.pbar.set_description(f\"(idx, step): ({self.sampled_idx}, {self.current_step}) | True rating: {target_rating} | Predicted rating: {pred_rating} | reward: {self.reward:.2f}\")\n",
    "        #     # time.sleep(0.25)\n",
    "        self.current_step += 1\n",
    "        obs, done = self._next_observation()\n",
    "        self.total_steps += 1\n",
    "        # return obs, self.reward, done, acc, target_rating, pred_rating, self.total_steps\n",
    "        return obs, self.reward, done, None, {}\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        self.sampled_idx = random.randint(0, len(self.dataset) - 1)\n",
    "        self.current_step = 0\n",
    "        traj = self.dataset[self.sampled_idx]\n",
    "        user_id = traj['user_id']\n",
    "\n",
    "\n",
    "        obs = traj['observations'][self.current_step]\n",
    "\n",
    "        if self.personalized_features is not None:\n",
    "            obs = np.hstack((obs, self.personalized_features[user_id]))\n",
    "        \n",
    "        return obs, None\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        if self.dataset[self.sampled_idx]['terminals'][self.current_step]:\n",
    "            done = True\n",
    "            obs, _ = self.reset()\n",
    "            return obs, done\n",
    "        \n",
    "        traj = self.dataset[self.sampled_idx]\n",
    "        user_id = traj['user_id']\n",
    "        obs = traj['observations'][self.current_step]\n",
    "        if self.personalized_features is not None:\n",
    "            obs = np.hstack((obs, self.personalized_features[van_id]))\n",
    "        done = False\n",
    "        return obs, done\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "        \n",
    "    def get_true_temperature(self):\n",
    "        target_temperature = self.dataset[self.sampled_idx]['actions'][self.current_step]\n",
    "        target_temperature = self.actions_map[target_temperature]\n",
    "        return target_temperature\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"../Data/mlens-train-trajectories-v1_with_0_to_1_reward_with_tags_and_summed.pkl\"\n",
    "\n",
    "with open (train_data_path, 'rb') as f:\n",
    "    train_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import numpy as np\n",
    "\n",
    "# Create a function to instantiate your custom environment\n",
    "def create_custom_env(data):\n",
    "    return MovieLensEnv(data)  # Instantiate your custom environment\n",
    "\n",
    "# Create a vectorized environment\n",
    "# env = make_vec_env(create_custom_env, n_envs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_env = create_custom_env(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssk/anaconda3/envs/offline-rl/lib/python3.8/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the DQN model\n",
    "model = PPO(\"MlpPolicy\", sample_env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fdcdc41d550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=10000, log_interval=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"../Data/mlens-test-trajectories-v1_with_0_to_1_reward_with_tags_and_summed.pkl\"\n",
    "with open (test_data_path, 'rb') as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Evaluate the trained model\n",
    "# # mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=1)\n",
    "\n",
    "# # Use the trained model to interact with the environment\n",
    "# # test_env = make_vec_env(create_custom_env, num_envs=1)\n",
    "# test_env = create_custom_env(test_data)\n",
    "# obs = test_env.reset()[0]\n",
    "# # print(f\"obs: {obs}; shape: {obs.shape}\")\n",
    "# rewards = []\n",
    "# for _ in range(1000):\n",
    "#     action, _ = model.predict(obs, deterministic=True)\n",
    "#     # print(f\"action: {action}, type: {type(action)}, shape: {action.shape}\")\n",
    "#     obs, reward, done, _, info = test_env.step(action.item())\n",
    "#     print(f\"reward: {reward}\")\n",
    "#     rewards.append(reward)\n",
    "#     if done:\n",
    "#         obs, _ = test_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ep_len = 1000\n",
    "episode_rewards = []\n",
    "\n",
    "for i in range(10):\n",
    "    test_env = create_custom_env(test_data)\n",
    "    obs = test_env.reset()[0]\n",
    "    rewards = 0\n",
    "    for t in range(max_ep_len):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        # print(f\"action: {action}, type: {type(action)}, shape: {action.shape}\")\n",
    "        obs, reward, done, _, info = test_env.step(action.item())\n",
    "        print(f\"reward: {reward}\")\n",
    "        rewards += reward\n",
    "        if done:\n",
    "            obs, _ = test_env.reset()\n",
    "    episode_rewards.append(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN on a gym env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "model = DQN(\"MlpPolicy\", env, verbose=0)\n",
    "model.learn(total_timesteps=10000, log_interval=4)\n",
    "# model.save(\"dqn_cartpole\")\n",
    "\n",
    "# del model # remove to demonstrate saving and loading\n",
    "\n",
    "# model = DQN.load(\"dqn_cartpole\")\n",
    "\n",
    "# obs, info = env.reset()\n",
    "# while True:\n",
    "#     action, _states = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, terminated, truncated, info = env.step(action)\n",
    "#     if terminated or truncated:\n",
    "#         obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "offline-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
