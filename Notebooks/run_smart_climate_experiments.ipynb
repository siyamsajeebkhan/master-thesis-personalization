{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/q621464/Desktop/Thesis/code/decision-transformer-thesis\")\n",
    "sys.path.append(\"/home/q621464/Desktop/Thesis/code/decision-transformer-thesis/atari\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 15:35:02.093547: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-23 15:35:02.211425: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/q621464/miniconda3/envs/decision-transformer-atari/lib/python3.7/site-packages/cv2/../../lib64::/home/q621464/.mujoco/mujoco200/bin\n",
      "2023-08-23 15:35:02.211438: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-08-23 15:35:02.733665: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/q621464/miniconda3/envs/decision-transformer-atari/lib/python3.7/site-packages/cv2/../../lib64::/home/q621464/.mujoco/mujoco200/bin\n",
      "2023-08-23 15:35:02.733727: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/q621464/miniconda3/envs/decision-transformer-atari/lib/python3.7/site-packages/cv2/../../lib64::/home/q621464/.mujoco/mujoco200/bin\n",
      "2023-08-23 15:35:02.733767: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/q621464/miniconda3/envs/decision-transformer-atari/lib/python3.7/site-packages/gin/tf/__init__.py:48: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import logging\n",
    "# make deterministic\n",
    "from atari.mingpt.utils import set_seed\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from atari.mingpt.model_atari import GPT, GPTConfig\n",
    "from atari.mingpt.trainer_atari import Trainer, TrainerConfig\n",
    "from atari.mingpt.utils import sample\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "import blosc\n",
    "import argparse\n",
    "from atari.create_dataset import create_dataset\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Apply dimensionality reduction here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self, seed=123, context_length=30, epochs=5, model_type='reward_conditioned', num_steps=500000, num_buffers=50, env='SmartClimate', batch_size=128, log_to_wandb=False, trajectories_per_buffer=10, train_data_dir='../atari/data-for-dt/smart-climate-train-trajectories-v9.pkl', val_data_dir='../atari/data-for-dt/smart-climate-val-trajectories-v9.pkl', test_data_dir='../atari/data-for-dt/smart-climate-test-trajectories-v9.pkl') -> None:\n",
    "        self.seed = seed\n",
    "        self.context_length = context_length\n",
    "        self.epochs = epochs\n",
    "        self.model_type = model_type\n",
    "        self.num_steps =num_steps\n",
    "        self.num_buffers = num_buffers\n",
    "        self.env = env\n",
    "        self.batch_size = batch_size\n",
    "        self.log_to_wandb = log_to_wandb\n",
    "        self.trajectories_per_buffer = trajectories_per_buffer\n",
    "        self.train_data_dir = train_data_dir\n",
    "        self.val_data_dir = val_data_dir\n",
    "        self.test_data_dir = test_data_dir\n",
    "        self.dim_reductor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config(env='SmartClimate', epochs=30, seed=123, context_length=20, train_data_dir='../atari/data-for-dt/smart-climate-train-trajectories-v9.pkl')\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensionality(obss):\n",
    "    X_train = obss\n",
    "    # Data normalizationd\n",
    "    # scaler = StandardScaler()\n",
    "    \n",
    "    # scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    # X_train_normalized = 2 * scaler.fit_transform(X_train) - 1\n",
    "    \n",
    "    X_train_normalized = obss\n",
    "\n",
    "    # Initialize PCA with the desired number of components (e.g., None for all components)\n",
    "    pca = PCA(n_components=None)\n",
    "\n",
    "    # Fit and transform the training data using PCA\n",
    "    X_train_pca = pca.fit_transform(X_train_normalized)\n",
    "\n",
    "    # Get the explained variance ratio\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "    # Get the cumulative explained variance ratio\n",
    "    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "    # Set a threshold for the cumulative explained variance ratio (e.g., 95% or 99%)\n",
    "    threshold_variance = 0.85\n",
    "\n",
    "    # Determine the number of components required to achieve the threshold\n",
    "    n_components = np.argmax(cumulative_variance_ratio >= threshold_variance) + 1\n",
    "    print(f\"Number of selected components: {n_components}\")\n",
    "\n",
    "    # Reduce the dimensionality to the selected number of components\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train_normalized)\n",
    "    print(f\"Shape of the reeduced dimensionality dataset: {X_train_pca.shape}\")\n",
    "\n",
    "    obss_reduced = X_train_pca\n",
    "    return obss_reduced, pca\n",
    "    # X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_dir, total_trajectories=8000):\n",
    "    with open(data_dir, 'rb') as f:\n",
    "        trajectories = pickle.load(f)[0:total_trajectories]\n",
    "    obss = []\n",
    "    actions = []\n",
    "    returns = [0]\n",
    "    done_idxs = []\n",
    "    stepwise_returns = []    \n",
    "    for traj in trajectories:\n",
    "        obss += traj['observations'].tolist()\n",
    "        actions += traj['actions'].tolist()\n",
    "        stepwise_returns += traj['rewards'].tolist()\n",
    "        done_idxs += [len(obss)]\n",
    "        returns += [0]\n",
    "\n",
    "    actions = np.array(actions)\n",
    "    returns = np.array(returns)\n",
    "    stepwise_returns = np.array(stepwise_returns)\n",
    "    done_idxs = np.array(done_idxs)\n",
    "\n",
    "    # -- create reward-to-go dataset\n",
    "    start_index = 0\n",
    "    rtg = np.zeros_like(stepwise_returns)\n",
    "    for i in done_idxs:\n",
    "        i = int(i)\n",
    "        curr_traj_returns = stepwise_returns[start_index:i]\n",
    "        for j in range(i-1, start_index-1, -1): # start from i-1\n",
    "            rtg_j = curr_traj_returns[j-start_index:i-start_index]\n",
    "            rtg[j] = sum(rtg_j)\n",
    "        start_index = i\n",
    "    print('max rtg is %d' % max(rtg))\n",
    "\n",
    "    # -- create timestep dataset\n",
    "    start_index = 0\n",
    "    timesteps = np.zeros(len(actions)+1, dtype=int)\n",
    "    print(f\"total done idx: {len(done_idxs)}\")\n",
    "    for i in done_idxs:\n",
    "        # print(f\"done_idx: {i}\")\n",
    "        i = int(i)\n",
    "        # print(f\"start_idx: {start_index}, i: {i}\")\n",
    "        timesteps[start_index:i+1] = np.arange(i+1 - start_index)\n",
    "        start_index = i+1\n",
    "    print('max timestep is %d' % max(timesteps))\n",
    "    \n",
    "    # if apply_dim_reduction:\n",
    "    #     obss, dim_reductor = reduce_dimensionality(obss)\n",
    "    return obss, actions, returns, done_idxs, rtg, timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateActionReturnDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size, actions, done_idxs, rtgs, timesteps, mean=0, std=1):\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = max(actions) + 1 # TODO: needs to be changed. Does it change dynamically based on the sampled data?\n",
    "        self.data = data\n",
    "        self.actions = actions\n",
    "        self.done_idxs = done_idxs\n",
    "        self.rtgs = rtgs\n",
    "        self.timesteps = timesteps\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(f\"Sampled idx: {idx}\")\n",
    "        block_size = self.block_size // 3\n",
    "        states = torch.tensor(np.array(self.data[idx:idx+block_size]), dtype=torch.float32).reshape(block_size, -1) # \n",
    "        # (block_size, 4*84*84)\n",
    "        \n",
    "        # print(f\"There are nan values in the dataloader's batch: {torch.isnan(states).any()}\")\n",
    "        # mean = torch.mean(states)\n",
    "        # std = torch.std(states)\n",
    "        # states = (states - self.mean) / self.std\n",
    "        # print(f\"mean: {mean}, std: {std} of the batch\\n\")\n",
    "        # states = states / 255.\n",
    "        # print(f\"There are nan values in the dataloader's batch after normalization: {torch.isnan(states).any()}\")\n",
    "        actions = torch.tensor(self.actions[idx:idx+block_size], dtype=torch.long).unsqueeze(1) # (block_size, 1)\n",
    "        rtgs = torch.tensor(self.rtgs[idx:idx+block_size], dtype=torch.float32).unsqueeze(1)\n",
    "        timesteps = torch.tensor(self.timesteps[idx:idx+1], dtype=torch.int64).unsqueeze(1)\n",
    "        # print(f\"timesteps shape: {timesteps.shape}\")\n",
    "\n",
    "        return states, actions, rtgs, timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max rtg is 552\n",
      "total done idx: 200\n",
      "max timestep is 552\n",
      "max rtg is 551\n",
      "total done idx: 165\n",
      "max timestep is 552\n",
      "max rtg is 556\n",
      "total done idx: 166\n",
      "max timestep is 559\n",
      "There are nan values in the obss: False\n",
      "***********************\n",
      "SmartClimate Environment\n",
      "***********************\n",
      "total obss: 50574\n",
      "total actions: (50574,)\n",
      "total returns: (201,)\n",
      "timesteps: 50575\n"
     ]
    }
   ],
   "source": [
    "# Create the train dataset first\n",
    "obss, actions, returns, done_idxs, rtgs, timesteps = create_dataset(args.train_data_dir, total_trajectories=200)\n",
    "\n",
    "# Create the validation dataset\n",
    "obss_val, actions_val, returns_val, done_idxs_val, rtgs_val, timesteps_val = create_dataset(args.val_data_dir, total_trajectories=200)\n",
    "\n",
    "# Create the validation dataset\n",
    "obss_test, actions_test, returns_test, done_idxs_test, rtgs_test, timesteps_test = create_dataset(args.test_data_dir, total_trajectories=2000)\n",
    "\n",
    "# Sanity check\n",
    "# Are there any nan values in the obss\n",
    "print(f\"There are nan values in the obss: {np.isnan(np.array(obss)).any()}\")\n",
    "\n",
    "print(\"*\" * len(args.env + \"Environment\"))\n",
    "print(f\"{args.env} Environment\")\n",
    "print(\"*\" * len(args.env + \"Environment\"))\n",
    "print(f\"total obss: {len(obss)}\\ntotal actions: {actions.shape}\\ntotal returns: {returns.shape}\\ntimesteps: {len(timesteps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.22437387541029, 5.311578543822653)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# used for input normalization\n",
    "states = np.concatenate(obss, axis=0)\n",
    "state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "state_mean, state_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With PCA\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# # Fit and transform the data to perform z-score normalization\n",
    "# X_train_normalized = 2 * scaler.fit_transform(obss) - 1\n",
    "# obss, dim_reductor = reduce_dimensionality(X_train_normalized)\n",
    "\n",
    "# # scaler_val = MinMaxScaler(feature_range=(0, 1))\n",
    "# obss_val_norm = 2 * scaler.transform(obss_val) - 1\n",
    "# obss_val = dim_reductor.transform(obss_val_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Without PCA\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# # Fit and transform the data to perform z-score normalization\n",
    "# obss = 2 * scaler.fit_transform(obss) - 1\n",
    "\n",
    "# # scaler_val = MinMaxScaler(feature_range=(0, 1))\n",
    "# obss_val = 2 * scaler.transform(obss_val) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args.context_length = 20\n",
    "train_dataset = StateActionReturnDataset(obss, args.context_length*3, actions, done_idxs, rtgs, timesteps)\n",
    "\n",
    "val_dataset = StateActionReturnDataset(obss_val, args.context_length*3, actions_val, done_idxs_val, rtgs_val, timesteps_val)\n",
    "\n",
    "test_dataset = StateActionReturnDataset(obss_test, args.context_length*3, actions_test, done_idxs_test, rtgs_test, timesteps_test)\n",
    "# print(f\"vocab size: {train_dataset.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best hyperparameters from hyperparameter tuning: \n",
    "1. Aimed at minimizing val_loss: {'learning_rate': 0.00019714606731026987, 'num_hidden_units': 40, 'context_length': 20, 'n_layer': 4}\n",
    "2. Aimed at increasing val_accuracy: {'learning_rate': 0.0003999625589556598, 'num_hidden_units': 208, 'context_length': 10, 'n_layer': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_timestep: 559\n"
     ]
    }
   ],
   "source": [
    "max_timestep = max(np.concatenate([timesteps, timesteps_val, timesteps_test]))\n",
    "print(f\"max_timestep: {max_timestep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (tok_emb): Embedding(25, 128)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=128, out_features=25, bias=False)\n",
       "  (state_encoder): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (ret_emb): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (action_embeddings): Sequential(\n",
       "    (0): Embedding(25, 128)\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if max(timesteps) > max(timesteps_val):\n",
    "#     max_timestep = max(timesteps)\n",
    "# else:\n",
    "#     max_timestep = max(timesteps_val)\n",
    "\n",
    "\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, n_layer=3, n_head=1, n_embd=128, model_type=args.model_type, max_timestep=max_timestep, input_dim=len(obss[0]))\n",
    "model = GPT(mconf)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a trainer instance and kick off training\n",
    "args.epochs = 10\n",
    "args.batch_size = 64\n",
    "tconf = TrainerConfig(max_epochs=args.epochs, batch_size=args.batch_size, learning_rate=1e-4, lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*args.context_length*3, num_workers=4, seed=args.seed, model_type=args.model_type, env=args.env, max_timestep=max(timesteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(obss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train events: 50514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 789: train loss 0.21948. lr 9.341160e-05: 100%|██████████| 790/790 [02:00<00:00,  6.57it/s]\n",
      "(idx, step): (68, 31) | True temperature: 17.0 | Predicted temperature: 24.0 | reward: 0.17:   0%|          | 0/100 [00:02<?, ?it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 168.1128472222221 from 331 steps, in 100: 50.78937982544474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(idx, step): (97, 21) | True temperature: 18.5 | Predicted temperature: 23.5 | reward: 0.34:   0%|          | 0/100 [00:05<?, ?it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 176.77777777777783 from 662 steps, in 100: 26.703591809331996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(idx, step): (85, 32) | True temperature: 25.0 | Predicted temperature: 16.0 | reward: 0.06:   0%|          | 0/100 [00:08<?, ?it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 129.06076388888852 from 1059 steps, in 100: 12.187040971566432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(idx, step): (40, 37) | True temperature: 22.0 | Predicted temperature: 17.0 | reward: 0.34:   0%|          | 0/100 [00:08<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 2.456597222222221 from 1109 steps, in 100: 0.2215146277928062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(idx, step): (143, 29) | True temperature: 19.0 | Predicted temperature: 24.0 | reward: 0.34:   0%|          | 0/100 [00:10<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 161.6284722222219 from 1419 steps, in 100: 11.39030811995926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(idx, step): (41, 30) | True temperature: 23.5 | Predicted temperature: 21.0 | reward: 0.63:   0%|          | 0/100 [00:13<?, ?it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 123.88541666666656 from 1740 steps, in 100: 7.1198515325670435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(idx, step): (22, 28) | True temperature: 21.5 | Predicted temperature: 18.5 | reward: 0.56:   0%|          | 0/100 [00:13<?, ?it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 55.763888888888935 from 1852 steps, in 100: 3.0110091192704607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(idx, step): (17, 31) | True temperature: 26.0 | Predicted temperature: 23.5 | reward: 0.63:   0%|          | 0/100 [00:15<?, ?it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 152.87500000000043 from 2129 steps, in 100: 7.180601221230645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(idx, step): (114, 31) | True temperature: 21.0 | Predicted temperature: 18.0 | reward: 0.56:   0%|          | 0/100 [00:18<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 234.19270833333394 from 2496 steps, in 100: 9.382720686431648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(idx, step): (114, 222) | True temperature: 21.0 | Predicted temperature: 25.5 | reward: 0.39:   0%|          | 0/100 [00:19<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 115.45659722222216 from 2718 steps, in 100: 4.247851259095738\n",
      "total num_steps in the env: 15515, total rewards: 1320.2100694444446\n",
      "target return: 100, eval return: 8, total steps: 2718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 2 iter 789: train loss 0.18767. lr 7.515330e-05: 100%|██████████| 790/790 [02:08<00:00,  6.15it/s]\n",
      "(idx, step): (5, 27) | True temperature: 18.5 | Predicted temperature: 20.5 | reward: 0.69:   0%|          | 0/100 [00:03<?, ?it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 114.2395833333329 from 466 steps, in 100: 24.51493204577959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(idx, step): (146, 38) | True temperature: 20.0 | Predicted temperature: 20.0 | reward: 1.00:   0%|          | 0/100 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 56.249999999999915 from 547 steps, in 100: 10.2833638025594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(idx, step): (120, 41) | True temperature: 18.5 | Predicted temperature: 23.0 | reward: 0.39:   0%|          | 0/100 [00:06<?, ?it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 233.9965277777784 from 870 steps, in 100: 26.896152618135446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(idx, step): (87, 42) | True temperature: 21.0 | Predicted temperature: 16.0 | reward: 0.34:   0%|          | 0/100 [00:07<?, ?it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 122.75868055555527 from 1047 steps, in 100: 11.72480234532524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(idx, step): (52, 31) | True temperature: 28.0 | Predicted temperature: 25.0 | reward: 0.56:   0%|          | 0/100 [00:08<?, ?it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 49.00000000000009 from 1191 steps, in 100: 4.114189756507145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(idx, step): (134, 33) | True temperature: 21.0 | Predicted temperature: 25.0 | reward: 0.44:   0%|          | 0/100 [00:08<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 52.3680555555556 from 1273 steps, in 100: 4.113751418346866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(idx, step): (3, 28) | True temperature: 16.0 | Predicted temperature: 23.5 | reward: 0.14:   0%|          | 0/100 [00:09<?, ?it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 31.225694444444443 from 1349 steps, in 100: 2.3147290173791286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(idx, step): (131, 32) | True temperature: 16.0 | Predicted temperature: 18.0 | reward: 0.69:   0%|          | 0/100 [00:11<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 88.19791666666643 from 1586 steps, in 100: 5.561028793610745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(idx, step): (163, 33) | True temperature: 21.0 | Predicted temperature: 22.5 | reward: 0.77:   0%|          | 0/100 [00:13<?, ?it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 182.62847222222044 from 1997 steps, in 100: 9.145141323095666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(idx, step): (163, 104) | True temperature: 23.0 | Predicted temperature: 22.5 | reward: 0.92:   0%|          | 0/100 [00:14<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward from this episode: 86.50173611111107 from 2101 steps, in 100: 4.117169733989104\n",
      "total num_steps in the env: 12427, total rewards: 1017.1666666666646\n",
      "target return: 100, eval return: 8, total steps: 2101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3 iter 533: train loss 0.22573. lr 5.857218e-05:  68%|██████▊   | 534/790 [01:26<00:41,  6.19it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_126084/1200204467.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# returns, predicted_actions, target_actions, epoch_losses, val_losses = trainer.train()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Average reward achieved: {np.mean(results['epoch_returns']):.2f} with RTG: {rtg}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thesis/code/decision-transformer-thesis/atari/mingpt/trainer_atari.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mepoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;31m# val_loss, val_acc = self.evaluate_model()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thesis/code/decision-transformer-thesis/atari/mingpt/trainer_atari.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(split, epoch_num)\u001b[0m\n\u001b[1;32m    125\u001b[0m                     \u001b[0;31m# forward the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# collapse all losses if they are scattered on multiple gpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/decision-transformer-atari/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thesis/code/decision-transformer-thesis/atari/mingpt/model_atari.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, states, actions, targets, rtgs, timesteps)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# print(f\"position_embeddings.shape: {position_embeddings.shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/decision-transformer-atari/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/decision-transformer-atari/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/decision-transformer-atari/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thesis/code/decision-transformer-thesis/atari/mingpt/model_atari.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/decision-transformer-atari/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/decision-transformer-atari/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/decision-transformer-atari/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/decision-transformer-atari/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/decision-transformer-atari/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Total train events: {len(train_dataset)}\")\n",
    "rtg = 100\n",
    "max_ep_len = 100\n",
    "trainer = Trainer(model, train_dataset, test_dataset, val_dataset, tconf, args.env, state_mean=state_mean, state_std=state_std, dim_reductor=None, scaler=None, rtg=rtg, max_ep_len=max_ep_len, num_eval_episodes=10)\n",
    "\n",
    "# returns, predicted_actions, target_actions, epoch_losses, val_losses = trainer.train()\n",
    "results = trainer.train()\n",
    "\n",
    "print(f\"Average reward achieved: {np.mean(results['epoch_returns']):.2f} with RTG: {rtg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, test_targets, test_preds = trainer.test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targets_merged = torch.cat(test_targets).flatten()\n",
    "test_preds_merged = torch.cat(test_preds).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((test_targets_merged == test_preds_merged).sum()) * 100. / len(test_targets_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(test_targets_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(test_preds_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses = results['epoch_losses']\n",
    "val_losses = results['val_losses']\n",
    "predicted_actions = results['predicted _actions']\n",
    "target_actions = results['target_actions']\n",
    "train_epoch_accuracies = results['train_accuracies']\n",
    "val_accuracies = results['val_accuracies']\n",
    "returns = results['epoch_returns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_losses, label='train_loss')\n",
    "plt.plot(val_losses, label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(f\"train and val loss for {len(train_dataset)} train and {len(val_dataset)} datapoints with batch size {args.batch_size}\")\n",
    "# plt.show()\n",
    "# plt.savefig(f'Figures/train_and_val_loss_for_{len(train_dataset)}_datapoints.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_epoch_accuracies, label='train_acc')\n",
    "plt.plot(val_accuracies, label='val_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(f\"train and val acc for {len(train_dataset)} train and {len(val_dataset)} datapoints\")\n",
    "# plt.show()\n",
    "# plt.savefig(f'Figures/train_and_val_loss_for_{len(train_dataset)}_datapoints.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atari.mingpt.envs.smart_climate_env import CustomActionSpace\n",
    "action_map = CustomActionSpace().actions_map\n",
    "\n",
    "train_actions_str = [str(action_map[action]) for action in actions]\n",
    "predicted_actions_str = [str(action) for action in predicted_actions]\n",
    "target_actions_str = [str(action) for action in target_actions]\n",
    "val_actions_str = [str(action_map[action]) for action in actions_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_actions_unique, counts = np.unique(train_actions_str, return_counts=True)\n",
    "train_action_count_dict = dict(zip(train_actions_unique, counts))\n",
    "\n",
    "target_actions_unique, counts = np.unique(target_actions_str, return_counts=True)\n",
    "target_action_count_dict = dict(zip(target_actions_unique, counts))\n",
    "\n",
    "predicted_actions_unique, counts = np.unique(predicted_actions_str, return_counts=True)\n",
    "predicted_action_count_dict = dict(zip(predicted_actions_unique, counts))\n",
    "\n",
    "val_actions_unique, counts = np.unique(val_actions_str, return_counts=True)\n",
    "val_action_count_dict = dict(zip(val_actions_unique, counts))\n",
    "\n",
    "for action in np.arange(16.0, 28.5, 0.5):\n",
    "    if str(action) not in predicted_action_count_dict.keys():\n",
    "        predicted_action_count_dict[str(action)] = 0\n",
    "    if str(action) not in train_action_count_dict.keys():\n",
    "        train_action_count_dict[str(action)] = 0\n",
    "    if str(action) not in target_action_count_dict.keys():\n",
    "        target_action_count_dict[str(action)] = 0\n",
    "    if str(action) not in val_action_count_dict.keys():\n",
    "        val_action_count_dict[str(action)] = 0\n",
    "\n",
    "sorted_keys = sorted(predicted_action_count_dict)\n",
    "predicted_action_count_dict = {key: predicted_action_count_dict[key] for key in sorted_keys}\n",
    "train_action_count_dict = {key: train_action_count_dict[key] for key in sorted_keys}\n",
    "target_action_count_dict = {key: target_action_count_dict[key] for key in sorted_keys}\n",
    "val_action_count_dict = {key: val_action_count_dict[key] for key in sorted_keys}\n",
    "\n",
    "sorted(train_action_count_dict.keys()) == sorted(predicted_action_count_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(target_actions_str, predicted_actions_str)\n",
    "print(f\"Evaluation Accuracy: {accuracy * 100:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "macro_f1 = f1_score(target_actions_str, predicted_actions_str, average='macro')\n",
    "\n",
    "# Calculate the micro F1 score\n",
    "micro_f1 = f1_score(target_actions_str, predicted_actions_str, average='micro')\n",
    "\n",
    "print(\"Macro F1 Score:\", macro_f1)\n",
    "print(\"Micro F1 Score:\", micro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.arange(16, 28.5, 0.5)\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "for target, prediction in zip(target_actions, predicted_actions):\n",
    "    if target == prediction:\n",
    "        correct_pred[target] += 1\n",
    "    total_pred[target] += 1\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    if total_pred[classname] > 0:\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    else:\n",
    "        accuracy = 0\n",
    "    print(f'Accuracy for class: {classname} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# Calculate and display the classification report for each class\n",
    "class_names = sorted(np.unique(train_actions_str))\n",
    "report = classification_report(target_actions_str, predicted_actions_str, target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for three datasets\n",
    "data1 = train_action_count_dict.values()\n",
    "data4 = val_action_count_dict.values()\n",
    "data2 = target_action_count_dict.values()\n",
    "data3 = predicted_action_count_dict.values()\n",
    "\n",
    "# Define the x-axis labels for the bars\n",
    "x_labels = train_action_count_dict.keys()\n",
    "\n",
    "# Create a figure and three subplots side by side\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 5))\n",
    "\n",
    "# Plot the bar plots on each subplot\n",
    "bar_width = 1\n",
    "x = np.arange(len(x_labels))\n",
    "\n",
    "# Bar plot for Dataset 1\n",
    "axes[0].bar(x, data1, bar_width, color='blue', label='train actions', edgecolor='black', alpha=0.5)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(x_labels, rotation=90)\n",
    "axes[0].set_title('Target temperature from train dataset')\n",
    "\n",
    "# Bar plot for Dataset 4\n",
    "axes[1].bar(x, data4, bar_width, color='purple', label='predicted actions', edgecolor='black', alpha=0.5)\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(x_labels, rotation=90)\n",
    "axes[1].set_title('Target temperature from the the validation dataset')\n",
    "\n",
    "# Bar plot for Dataset 2\n",
    "axes[2].bar(x, data2, bar_width, color='green', label='target actions', edgecolor='black', alpha=0.5)\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(x_labels, rotation=90)\n",
    "axes[2].set_title('Target temperature from the test set')\n",
    "\n",
    "# Bar plot for Dataset 3\n",
    "axes[3].bar(x, data3, bar_width, color='red', label='predicted actions', edgecolor='black', alpha=0.5)\n",
    "axes[3].set_xticks(x)\n",
    "axes[3].set_xticklabels(x_labels, rotation=90)\n",
    "axes[3].set_title('Target temperature from the predictions on the test set')\n",
    "\n",
    "\n",
    "\n",
    "# Add labels and title to the overall figure\n",
    "plt.suptitle('Bar Plots of Train, Eval and Predicted target temperatures')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Figures/temp.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_probabilities(labels):\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    total_counts = len(labels)\n",
    "    probabilities = counts / total_counts\n",
    "    return unique_labels, probabilities\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
    "\n",
    "\n",
    "# Compute probabilities and unique labels for each list\n",
    "labels1, probabilities1 = compute_probabilities(train_actions_str)\n",
    "labels2, probabilities2 = compute_probabilities(val_actions_str)\n",
    "\n",
    "# Union of unique labels from both lists\n",
    "all_labels = np.union1d(labels1, labels2)\n",
    "\n",
    "# Fill missing labels with zero probabilities\n",
    "probabilities1_all = np.zeros_like(all_labels, dtype=float)\n",
    "probabilities2_all = np.zeros_like(all_labels, dtype=float)\n",
    "probabilities1_all[np.searchsorted(all_labels, labels1)] = probabilities1\n",
    "probabilities2_all[np.searchsorted(all_labels, labels2)] = probabilities2\n",
    "\n",
    "# Compute KL-divergence from list1 to list2\n",
    "kl_divergence_1to2 = kl_divergence(probabilities1_all, probabilities2_all)\n",
    "\n",
    "# Compute KL-divergence from list2 to list1\n",
    "kl_divergence_2to1 = kl_divergence(probabilities2_all, probabilities1_all)\n",
    "\n",
    "print(\"KL-Divergence from list1 to list2:\", kl_divergence_1to2)\n",
    "print(\"KL-Divergence from list2 to list1:\", kl_divergence_2to1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_actions = [float(action) for action in train_actions_str]\n",
    "val_actions = [float(action) for action in val_actions_str]\n",
    "\n",
    "print(np.mean(train_actions), np.mean(val_actions))\n",
    "print(np.median(train_actions), np.median(val_actions))\n",
    "print(np.std(train_actions), np.std(val_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build confusion matrix\n",
    "unique_labels = np.unique(np.concatenate((target_actions_str, predicted_actions_str)))\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(target_actions_str, predicted_actions_str, labels=unique_labels)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix for better visualization\n",
    "df_cm = pd.DataFrame(cf_matrix, index=unique_labels, columns=unique_labels)\n",
    "\n",
    "plt.figure(figsize = (16,10))\n",
    "# plt.plot(df_cm)\n",
    "sn.heatmap(df_cm, annot=True, cmap='Blues', cbar=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f\"Confusion matrix for {len(train_dataset)} train and {len(predicted_actions)} datapoints\")\n",
    "# plt.show()\n",
    "plt.savefig(f'Figures/Confusion matrix for {len(train_dataset)} train and {len(predicted_actions)} datapoints.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(returns)\n",
    "plt.title(\"Return using sampling from the test env\")\n",
    "plt.show()\n",
    "# plt.savefig(f\"Figures/return_with_sampling_500_train_traj.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sorted(train_actions_str))\n",
    "plt.xlabel('Target Temperatures')\n",
    "plt.title(\"Target Temperature distribution in the training set\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sorted(target_actions_str))\n",
    "plt.xlabel('Target temperature')\n",
    "plt.title(\"Target temperatures distribution in the evaluation set\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sorted(predicted_actions_str))\n",
    "plt.xlabel('Target temperatures from the evaluation set')\n",
    "plt.title(\"Predicted target temperatures distribution from the evaluation set\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Numbers of pairs of bars you want\n",
    "# # N = 4\n",
    "\n",
    "# # Data on X-axis\n",
    "\n",
    "# # Specify the values of blue bars (height)\n",
    "# # train_actions = list(train_action_count_dict.values())\n",
    "# target_actions = list(target_action_count_dict.values())\n",
    "\n",
    "\n",
    "# # Specify the values of orange bars (height)\n",
    "# pred_actions = list(predicted_action_count_dict.values())\n",
    "\n",
    "\n",
    "# print(len(target_actions), len(pred_actions))\n",
    "# # Position of bars on x-axis\n",
    "# ind = np.arange(16.0, 28.5, 0.5)\n",
    "# # ind = np.arange(5)\n",
    "# # print(ind)\n",
    "\n",
    "# # Figure size\n",
    "# plt.figure(figsize=(10,5))\n",
    "\n",
    "# # Width of a bar \n",
    "# width = 0.2       \n",
    "\n",
    "# # Plotting\n",
    "# plt.bar(ind, target_actions , width, label='target action')\n",
    "# plt.bar(ind + width, pred_actions, width, label='pred action')\n",
    "\n",
    "# plt.xlabel('Actions')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Action distribution comparison between target and predictions')\n",
    "\n",
    "# # # xticks()\n",
    "# # # First argument - A list of positions at which ticks should be placed\n",
    "# # # Second argument -  A list of labels to place at the given locations\n",
    "# # plt.xticks(ind+0.1, train_action_count_dict.keys())\n",
    "# plt.xticks(ind+0.1, ind)\n",
    "\n",
    "# # Finding the best position for legends and putting it\n",
    "# plt.legend(loc='best')\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from your_pytorch_model import YourModel  # Import your PyTorch model\n",
    "\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters to be tuned by Optuna\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    num_hidden_units = trial.suggest_int('num_hidden_units', 32, 256)\n",
    "    \n",
    "    # Load the val_dataset\n",
    "    val_data_dir = \"../atari/data-for-dt/smart-climate-val-trajectories-v2.pkl\"\n",
    "    # Create the dataset first\n",
    "    obss, actions, returns, done_idxs, rtgs, timesteps = create_dataset(val_data_dir, total_trajectories=8000, apply_dim_reduction=True)\n",
    "    \n",
    "    # Split your data into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(obss, actions, test_size=0.1, random_state=123)\n",
    "    \n",
    "    val_dataset = StateActionReturnDataset(X_train, args.context_length*3, y_train, done_idxs, rtgs, timesteps)\n",
    "    print(f\"vocab size: {val_dataset.vocab_size}\")\n",
    "    \n",
    "    # Define the model here\n",
    "    mconf = GPTConfig(val_dataset.vocab_size, val_dataset.block_size, n_layer=6, n_head=8, n_embd=128, model_type=args.model_type, max_timestep=max(timesteps), input_dim=len(obss[0]))\n",
    "    model = GPT(mconf)\n",
    "\n",
    "    # Train the model for a fixed number of epochs\n",
    "    epochs = 10\n",
    "    tconf = TrainerConfig(max_epochs=epochs, batch_size=args.batch_size, learning_rate=6e-4, lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*args.context_length*3, num_workers=4, seed=args.seed, model_type=args.model_type, env=args.env, max_timestep=max(timesteps))\n",
    "    \n",
    "    rtg = 100\n",
    "    max_ep_len = 100\n",
    "    trainer = Trainer(model, train_dataset, None, tconf, args.env, rtg=rtg, max_ep_len=max_ep_len, num_eval_episodes=1)\n",
    "    \n",
    "    trainer.train()\n",
    "    val_loss, val_accuracy = trainer.evaluate_model()\n",
    "\n",
    "    # Return the validation loss as the objective value to be minimized\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "learning_rate = best_params['learning_rate']\n",
    "num_hidden_units = best_params['num_hidden_units']\n",
    "learning_rate, num_hidden_units\n",
    "# # Train your final model using the best hyperparameters\n",
    "# final_model = model(num_hidden_units)\n",
    "# optimizer = optim.Adam(final_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model on the entire training data\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the val_dataset\n",
    "val_data_dir = \"../atari/data-for-dt/smart-climate-val-trajectories-v2.pkl\"\n",
    "# Create the dataset first\n",
    "obss, actions, returns, done_idxs, rtgs, timesteps = create_dataset(val_data_dir, total_trajectories=8000, apply_dim_reduction=True)\n",
    "# Sanity check\n",
    "# Are there any nan values in the obss\n",
    "print(f\"There are nan values in the obss: {np.isnan(np.array(obss)).any()}\")\n",
    "print(\"*\" * len(args.env + \"Environment\"))\n",
    "print(f\"{args.env} Environment\")\n",
    "print(\"*\" * len(args.env + \"Environment\"))\n",
    "print(f\"total obss: {len(obss)}\\ntotal actions: {actions.shape}\\ntotal returns: {returns.shape}\\ntimesteps: {len(timesteps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = StateActionReturnDataset(obss, args.context_length*3, actions, done_idxs, rtgs, timesteps)\n",
    "print(f\"vocab size: {train_dataset.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a trainer instance and kick off training\n",
    "epochs = 10\n",
    "tconf = TrainerConfig(max_epochs=epochs, batch_size=args.batch_size, learning_rate=6e-4, lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*args.context_length*3, num_workers=4, seed=args.seed, model_type=args.model_type, env=args.env, max_timestep=max(timesteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtg = 100\n",
    "max_ep_len = 100\n",
    "trainer = Trainer(model, train_dataset, None, tconf, args.env, rtg=rtg, max_ep_len=max_ep_len, num_eval_episodes=10)\n",
    "# avg_return, predicted_actions, target_actions, epoch_losses = trainer.train()\n",
    "# print(f\"Average reward achieved: {avg_return:.2f} with RTG: {rtg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision-transformer-atari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
