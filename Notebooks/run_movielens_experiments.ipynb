{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/q621464/Desktop/Thesis/code/decision-transformer-thesis\")\n",
    "sys.path.append(\"/home/q621464/Desktop/Thesis/code/decision-transformer-thesis/atari\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "# make deterministic\n",
    "from atari.mingpt.utils import set_seed\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from atari.mingpt.model_atari import GPT, GPTConfig\n",
    "from atari.mingpt.trainer_atari import Trainer, TrainerConfig\n",
    "from atari.mingpt.utils import sample\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "import blosc\n",
    "import argparse\n",
    "from atari.create_dataset import create_dataset\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self, seed=123, context_length=30, epochs=5, model_type='reward_conditioned', num_steps=500000, num_buffers=50, env='SmartClimate', batch_size=128, log_to_wandb=False, trajectories_per_buffer=10, data_dir='../atari/data-for-dt/smart-climate-train-trajectories.pkl') -> None:\n",
    "        self.seed = seed\n",
    "        self.context_length = context_length\n",
    "        self.epochs = epochs\n",
    "        self.model_type = model_type\n",
    "        self.num_steps =num_steps\n",
    "        self.num_buffers = num_buffers\n",
    "        self.env = env\n",
    "        self.batch_size = batch_size\n",
    "        self.log_to_wandb = log_to_wandb\n",
    "        self.trajectories_per_buffer = trajectories_per_buffer\n",
    "        self.data_dir = data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config(data_dir='../atari/data-for-dt/mlens-train-trajectories-v1.pkl', env='Movielens', epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_dir):\n",
    "    with open(data_dir, 'rb') as f:\n",
    "        trajectories = pickle.load(f)\n",
    "    obss = []\n",
    "    actions = []\n",
    "    returns = [0]\n",
    "    done_idxs = []\n",
    "    stepwise_returns = []    \n",
    "    for traj in trajectories:\n",
    "        obss += traj['observations'].tolist()\n",
    "        actions += traj['actions'].tolist()\n",
    "        stepwise_returns += traj['rewards'].tolist()\n",
    "        done_idxs += [len(obss)]\n",
    "        returns += [0]\n",
    "\n",
    "    actions = np.array(actions)\n",
    "    returns = np.array(returns)\n",
    "    stepwise_returns = np.array(stepwise_returns)\n",
    "    done_idxs = np.array(done_idxs)\n",
    "\n",
    "    # -- create reward-to-go dataset\n",
    "    start_index = 0\n",
    "    rtg = np.zeros_like(stepwise_returns)\n",
    "    for i in done_idxs:\n",
    "        i = int(i)\n",
    "        curr_traj_returns = stepwise_returns[start_index:i]\n",
    "        for j in range(i-1, start_index-1, -1): # start from i-1\n",
    "            rtg_j = curr_traj_returns[j-start_index:i-start_index]\n",
    "            rtg[j] = sum(rtg_j)\n",
    "        start_index = i\n",
    "    print('max rtg is %d' % max(rtg))\n",
    "\n",
    "    # -- create timestep dataset\n",
    "    start_index = 0\n",
    "    timesteps = np.zeros(len(actions)+1, dtype=int)\n",
    "    print(f\"total done idx: {len(done_idxs)}\")\n",
    "    for i in done_idxs:\n",
    "        # print(f\"done_idx: {i}\")\n",
    "        i = int(i)\n",
    "        timesteps[start_index:i+1] = np.arange(i+1 - start_index)\n",
    "        start_index = i+1\n",
    "    print('max timestep is %d' % max(timesteps))\n",
    "\n",
    "    return obss, actions, returns, done_idxs, rtg, timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateActionReturnDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size, actions, done_idxs, rtgs, timesteps):        \n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = max(actions) + 1 # TODO: needs to be changed. Does it change dynamically based on the sampled data?\n",
    "        self.data = data\n",
    "        self.actions = actions\n",
    "        self.done_idxs = done_idxs\n",
    "        self.rtgs = rtgs\n",
    "        self.timesteps = timesteps\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(f\"Fetching for idx: {idx}\")\n",
    "        block_size = self.block_size // 3\n",
    "        done_idx = idx + block_size # TODO: needs change in the prepared datset for Mlens\n",
    "        # print(f\"done_idx initially: {done_idx}\")\n",
    "        for i in self.done_idxs:\n",
    "            # print(f\"i={i} and idx={idx}\")\n",
    "            if i >= idx + block_size: # first done_idx greater than idx\n",
    "                done_idx = min(int(i), done_idx)\n",
    "                break\n",
    "        idx = done_idx - block_size\n",
    "        # print(f\"done_idx after: {done_idx} and start_idx: {idx}\")\n",
    "        \n",
    "        states = torch.tensor(np.array(self.data[idx:done_idx]), dtype=torch.float32).reshape(block_size, -1) # \n",
    "        (block_size, 4*84*84)\n",
    "        \n",
    "        # print(f\"There are nan values in the dataloader's batch: {torch.isnan(states).any()}\")\n",
    "        mean = torch.mean(states)\n",
    "        std = torch.std(states)\n",
    "        states = (states - mean) / std\n",
    "        # print(f\"mean: {mean}, std: {std} of the batch\\n\")\n",
    "        # states = states / 255.\n",
    "        # print(f\"There are nan values in the dataloader's batch after normalization: {torch.isnan(states).any()}\")\n",
    "        actions = torch.tensor(self.actions[idx:done_idx], dtype=torch.long).unsqueeze(1) # (block_size, 1)\n",
    "        rtgs = torch.tensor(self.rtgs[idx:done_idx], dtype=torch.float32).unsqueeze(1)\n",
    "        timesteps = torch.tensor(self.timesteps[idx:idx+1], dtype=torch.int64).unsqueeze(1)\n",
    "\n",
    "        return states, actions, rtgs, timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obss, actions, returns, done_idxs, rtgs, timesteps = create_dataset(args.data_dir)\n",
    "print(f\"input_dim={len(obss[0])}\")\n",
    "\n",
    "# Sanity check\n",
    "# Are there any nan values in the obss\n",
    "print(f\"There are nan values in the obss: {np.isnan(np.array(obss)).any()}\")\n",
    "\n",
    "print(\"*\" * len(args.env + \"Environment\"))\n",
    "print(f\"{args.env} Environment\")\n",
    "print(\"*\" * len(args.env + \"Environment\"))\n",
    "print(f\"total obss: {len(obss)}\\ntotal actions: {actions.shape}\\ntotal returns: {returns.shape}\\ntimesteps: {len(timesteps)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StateActionReturnDataset(obss, args.context_length*3, actions, done_idxs, rtgs, timesteps)\n",
    "print(f\"vocab size: {train_dataset.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, n_layer=6, n_head=8, n_embd=128, model_type=args.model_type, max_timestep=max(timesteps), input_dim=len(obss[0]))\n",
    "model = GPT(mconf)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a trainer instance and kick off training\n",
    "epochs = args.epochs\n",
    "tconf = TrainerConfig(max_epochs=epochs, batch_size=args.batch_size, learning_rate=6e-4, lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*args.context_length*3, num_workers=4, seed=args.seed, model_type=args.model_type, env=args.env, max_timestep=max(timesteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtg = 1000\n",
    "max_ep_len = 1000\n",
    "trainer = Trainer(model, train_dataset, None, tconf, args.env, rtg=rtg, max_ep_len=max_ep_len, num_eval_episodes=1)\n",
    "avg_return, predicted_actions, target_actions = trainer.train()\n",
    "print(f\"Average reward achieved: {avg_return:.2f} with RTG: {rtg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atari.mingpt.envs.movielens_env import CustomActionSpace\n",
    "action_map = CustomActionSpace().actions_map\n",
    "\n",
    "train_actions_str = [str(action_map[action]) for action in actions]\n",
    "predicted_actions_str = [str(action) for action in predicted_actions]\n",
    "target_actions_str = [str(action) for action in target_actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of actions in the training set\n",
    "labels = np.arange(0.5, 5.5, 0.5)\n",
    "sns.histplot(sorted(train_actions_str), color='blue', alpha=0.5, label='train actions')\n",
    "sns.histplot(sorted(predicted_actions_str), color='red', alpha=0.5, label='pred actions')\n",
    "sns.histplot(sorted(target_actions_str), color='green', alpha=0.5, label='target actions')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Action distribution of the training set\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_actions_unique, counts = np.unique(train_actions_str, return_counts=True)\n",
    "train_action_count_dict = dict(zip(train_actions_unique, counts))\n",
    "\n",
    "\n",
    "predicted_actions_unique, counts = np.unique(predicted_actions_str, return_counts=True)\n",
    "predicted_action_count_dict = dict(zip(predicted_actions_unique, counts))\n",
    "\n",
    "for action in np.arange(0.5, 5.5, 0.5):\n",
    "    if str(action) not in predicted_action_count_dict.keys():\n",
    "        predicted_action_count_dict[str(action)] = 0\n",
    "    if str(action) not in train_action_count_dict.keys():\n",
    "        train_action_count_dict[str(action)] = 0\n",
    "\n",
    "sorted_keys = sorted(predicted_action_count_dict)\n",
    "predicted_action_count_dict = {key: predicted_action_count_dict[key] for key in sorted_keys}\n",
    "train_action_count_dict, predicted_action_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sorted(train_actions_str))\n",
    "plt.xlabel('Actions in the train set')\n",
    "plt.title(\"Action distribution in the training set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sorted(target_actions_str))\n",
    "plt.xlabel('Target actions in the evaluation set')\n",
    "plt.title(\"Target action distribution in the evaluation set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sorted(predicted_actions_str))\n",
    "plt.xlabel('Predicted actions from the evaluation set')\n",
    "plt.title(\"Predicted action distribution from the evaluation set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbers of pairs of bars you want\n",
    "# N = 4\n",
    "\n",
    "# Data on X-axis\n",
    "\n",
    "# Specify the values of blue bars (height)\n",
    "train_actions = list(train_action_count_dict.values())\n",
    "\n",
    "\n",
    "# Specify the values of orange bars (height)\n",
    "pred_actions = list(predicted_action_count_dict.values())\n",
    "\n",
    "\n",
    "print(train_actions, pred_actions)\n",
    "# Position of bars on x-axis\n",
    "ind = np.arange(0.5, 5.5, 0.5)\n",
    "# ind = np.arange(5)\n",
    "# print(ind)\n",
    "\n",
    "# Figure size\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Width of a bar \n",
    "width = 0.2       \n",
    "\n",
    "# Plotting\n",
    "plt.bar(ind, train_actions , width, label='train action')\n",
    "plt.bar(ind + width, pred_actions, width, label='pred action')\n",
    "\n",
    "plt.xlabel('Here goes x-axis label')\n",
    "plt.ylabel('Here goes y-axis label')\n",
    "plt.title('Here goes title of the plot')\n",
    "\n",
    "# # xticks()\n",
    "# # First argument - A list of positions at which ticks should be placed\n",
    "# # Second argument -  A list of labels to place at the given locations\n",
    "plt.xticks(ind+0.1, train_action_count_dict.keys())\n",
    "\n",
    "# Finding the best position for legends and putting it\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(target_actions_str, predicted_actions_str)\n",
    "# print(np.sum(cf_matrix))\n",
    "# df_cm = pd.DataFrame(cf_matrix*100/np.sum(cf_matrix), index = sorted(np.unique(target_actions_str)),\n",
    "#                      columns = sorted(np.unique(target_actions_str)))\n",
    "df_cm = pd.DataFrame(cf_matrix*100/np.sum(cf_matrix), index = sorted(np.unique(target_actions_str)),\n",
    "                     columns = sorted(np.unique(target_actions_str)))\n",
    "# print(df_cm.head())\n",
    "plt.figure(figsize = (12,8))\n",
    "# plt.plot(df_cm)\n",
    "sn.heatmap(df_cm, annot=True, cmap='Blues', cbar=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig(\"confusion_matrix_Movielens_dataset.jpg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision-transformer-atari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
