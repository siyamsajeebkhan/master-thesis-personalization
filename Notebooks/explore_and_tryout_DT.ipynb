{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset name: Hopper medium\n",
    "\n",
    "DATA_PATH = \"/home/q621464/Desktop/Thesis/code/decision-transformer/gym/data/hopper-medium-v2.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2186"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(DATA_PATH, 'rb') as data_file:\n",
    "    trajectories = pickle.load(data_file)\n",
    "len(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, traj_lens, returns = [], [], []\n",
    "for path in trajectories:\n",
    "    states.append(path['observations'])\n",
    "    traj_lens.append(len(path['observations']))\n",
    "    returns.append(path['rewards'].sum())\n",
    "traj_lens, returns = np.array(traj_lens), np.array(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.concatenate(states, axis=0)\n",
    "state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "\n",
    "num_timesteps = sum(traj_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_traj = 1.\n",
    "\n",
    "# only train on top pct_traj trajectories (for %BC experiment)\n",
    "num_timesteps = max(int(pct_traj*num_timesteps), 1)\n",
    "sorted_inds = np.argsort(returns)  # lowest to highest\n",
    "num_trajectories = 1\n",
    "timesteps = traj_lens[sorted_inds[-1]]\n",
    "ind = len(trajectories) - 2\n",
    "while ind >= 0 and timesteps + traj_lens[sorted_inds[ind]] <= num_timesteps:\n",
    "    timesteps += traj_lens[sorted_inds[ind]]\n",
    "    num_trajectories += 1\n",
    "    ind -= 1\n",
    "sorted_inds = sorted_inds[-num_trajectories:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00014501, 0.00016102, 0.00015301, ..., 0.00087208, 0.00090108,\n",
       "       0.00100009])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# used to reweight sampling so we sample according to timesteps instead of trajectories\n",
    "p_sample = traj_lens[sorted_inds] / sum(traj_lens[sorted_inds])\n",
    "p_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 2183, 2184, 2185])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(num_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_batch(batch_size=256, max_len=20):\n",
    "    batch_inds = np.random.choice(\n",
    "        np.arange(num_trajectories),\n",
    "        size=batch_size,\n",
    "        replace=True,\n",
    "        p=p_sample,  # reweights so we sample according to timesteps\n",
    "    )\n",
    "\n",
    "    s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "    for i in range(batch_size):\n",
    "        traj = trajectories[int(sorted_inds[batch_inds[i]])]\n",
    "        si = random.randint(0, traj['rewards'].shape[0] - 1)\n",
    "\n",
    "        # get sequences from dataset\n",
    "        s.append(traj['observations'][si:si + max_len].reshape(1, -1, state_dim))\n",
    "        a.append(traj['actions'][si:si + max_len].reshape(1, -1, act_dim))\n",
    "        r.append(traj['rewards'][si:si + max_len].reshape(1, -1, 1))\n",
    "        if 'terminals' in traj:\n",
    "            d.append(traj['terminals'][si:si + max_len].reshape(1, -1))\n",
    "        else:\n",
    "            d.append(traj['dones'][si:si + max_len].reshape(1, -1))\n",
    "        timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "        timesteps[-1][timesteps[-1] >= max_ep_len] = max_ep_len-1  # padding cutoff\n",
    "        rtg.append(discount_cumsum(traj['rewards'][si:], gamma=1.)[:s[-1].shape[1] + 1].reshape(1, -1, 1))\n",
    "        if rtg[-1].shape[1] <= s[-1].shape[1]:\n",
    "            rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "        # padding and state + reward normalization\n",
    "        tlen = s[-1].shape[1]\n",
    "        s[-1] = np.concatenate([np.zeros((1, max_len - tlen, state_dim)), s[-1]], axis=1)\n",
    "        s[-1] = (s[-1] - state_mean) / state_std\n",
    "        a[-1] = np.concatenate([np.ones((1, max_len - tlen, act_dim)) * -10., a[-1]], axis=1)\n",
    "        r[-1] = np.concatenate([np.zeros((1, max_len - tlen, 1)), r[-1]], axis=1)\n",
    "        d[-1] = np.concatenate([np.ones((1, max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "        rtg[-1] = np.concatenate([np.zeros((1, max_len - tlen, 1)), rtg[-1]], axis=1) / scale\n",
    "        timesteps[-1] = np.concatenate([np.zeros((1, max_len - tlen)), timesteps[-1]], axis=1)\n",
    "        mask.append(np.concatenate([np.zeros((1, max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "\n",
    "    s = torch.from_numpy(np.concatenate(s, axis=0)).to(dtype=torch.float32, device=device)\n",
    "    a = torch.from_numpy(np.concatenate(a, axis=0)).to(dtype=torch.float32, device=device)\n",
    "    r = torch.from_numpy(np.concatenate(r, axis=0)).to(dtype=torch.float32, device=device)\n",
    "    d = torch.from_numpy(np.concatenate(d, axis=0)).to(dtype=torch.long, device=device)\n",
    "    rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).to(dtype=torch.float32, device=device)\n",
    "    timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).to(dtype=torch.long, device=device)\n",
    "    mask = torch.from_numpy(np.concatenate(mask, axis=0)).to(device=device)\n",
    "\n",
    "    return s, a, r, d, rtg, timesteps, mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['observations', 'next_observations', 'actions', 'rewards', 'terminals'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hopper_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((470, 11), (470, 11), (470, 3), (470,), (470,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hopper_data[0][\"observations\"].shape, hopper_data[0][\"next_observations\"].shape, hopper_data[0][\"actions\"].shape, hopper_data[0][\"rewards\"].shape, hopper_data[0][\"terminals\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11,), (11,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hopper_data[0][\"observations\"][1].shape, hopper_data[0][\"next_observations\"][230].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Hopper-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "state_dim, act_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"hopper\"\n",
    "dataset = \"medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2186"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = f'../gym/data/{env_name}-{dataset}-v2.pkl'\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    trajectories = pickle.load(f)\n",
    "len(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2186, (2186,), (2186,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states, traj_lens, returns = [], [], []\n",
    "for path in trajectories:\n",
    "    states.append(path['observations'])\n",
    "    traj_lens.append(len(path['observations']))\n",
    "    returns.append(path['rewards'].sum())\n",
    "traj_lens, returns = np.array(traj_lens), np.array(returns)\n",
    "len(states), traj_lens.shape, returns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999906, 11)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = np.concatenate(states, axis=0)\n",
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.311279  , -0.08469521, -0.5382719 , -0.07201576,  0.04932366,\n",
       "         2.1066856 , -0.15017354,  0.00878345, -0.2848186 , -0.18540096,\n",
       "        -0.28461286], dtype=float32),\n",
       " array([0.17790751, 0.05444621, 0.21297139, 0.14530419, 0.6124444 ,\n",
       "        0.85174465, 1.4515252 , 0.6751696 , 1.536239  , 1.6160746 ,\n",
       "        5.6072536 ], dtype=float32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "state_mean, state_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999906"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_timesteps = sum(traj_lens)\n",
    "num_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2186,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Starting new experiment: hopper medium\n",
      "2186 trajectories, 999906 timesteps found\n",
      "Average return: 1422.06, std: 378.95\n",
      "Max return: 3222.36, min: 315.87\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print('=' * 50)\n",
    "print(f'Starting new experiment: {env_name} {dataset}')\n",
    "print(f'{len(traj_lens)} trajectories, {num_timesteps} timesteps found')\n",
    "print(f'Average return: {np.mean(returns):.2f}, std: {np.std(returns):.2f}')\n",
    "print(f'Max return: {np.max(returns):.2f}, min: {np.min(returns):.2f}')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_action = nn.Sequential(\n",
    "    *([nn.Linear(128, 11)]+([nn.Tanh()]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=128, out_features=11, bias=True)\n",
       "  (1): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(\n",
    "        exp_prefix,\n",
    "        variant,\n",
    "):\n",
    "    device = variant.get('device', 'cuda')\n",
    "    log_to_wandb = variant.get('log_to_wandb', False)\n",
    "\n",
    "    env_name, dataset = variant['env'], variant['dataset']\n",
    "    model_type = variant['model_type']\n",
    "    group_name = f'{exp_prefix}-{env_name}-{dataset}'\n",
    "    exp_prefix = f'{group_name}-{random.randint(int(1e5), int(1e6) - 1)}'\n",
    "\n",
    "    if env_name == 'hopper':\n",
    "        env = gym.make('Hopper-v3')\n",
    "        max_ep_len = 1000\n",
    "        env_targets = [3600, 1800]  # evaluation conditioning targets\n",
    "        scale = 1000.  # normalization for rewards/returns\n",
    "    elif env_name == 'halfcheetah':\n",
    "        env = gym.make('HalfCheetah-v3')\n",
    "        max_ep_len = 1000\n",
    "        env_targets = [12000, 6000]\n",
    "        scale = 1000.\n",
    "    elif env_name == 'walker2d':\n",
    "        env = gym.make('Walker2d-v3')\n",
    "        max_ep_len = 1000\n",
    "        env_targets = [5000, 2500]\n",
    "        scale = 1000.\n",
    "    elif env_name == 'reacher2d':\n",
    "        from decision_transformer.envs.reacher_2d import Reacher2dEnv\n",
    "        env = Reacher2dEnv()\n",
    "        max_ep_len = 100\n",
    "        env_targets = [76, 40]\n",
    "        scale = 10.\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if model_type == 'bc':\n",
    "        env_targets = env_targets[:1]  # since BC ignores target, no need for different evaluations\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    # load dataset\n",
    "    dataset_path = f'data/{env_name}-{dataset}-v2.pkl'\n",
    "    with open(dataset_path, 'rb') as f:\n",
    "        trajectories = pickle.load(f)\n",
    "\n",
    "    # save all path information into separate lists\n",
    "    mode = variant.get('mode', 'normal')\n",
    "    states, traj_lens, returns = [], [], []\n",
    "    for path in trajectories:\n",
    "        if mode == 'delayed':  # delayed: all rewards moved to end of trajectory\n",
    "            path['rewards'][-1] = path['rewards'].sum()\n",
    "            path['rewards'][:-1] = 0.\n",
    "        states.append(path['observations'])\n",
    "        traj_lens.append(len(path['observations']))\n",
    "        returns.append(path['rewards'].sum())\n",
    "    traj_lens, returns = np.array(traj_lens), np.array(returns)\n",
    "\n",
    "    # used for input normalization\n",
    "    states = np.concatenate(states, axis=0)\n",
    "    state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "\n",
    "    num_timesteps = sum(traj_lens)\n",
    "\n",
    "    print('=' * 50)\n",
    "    print(f'Starting new experiment: {env_name} {dataset}')\n",
    "    print(f'{len(traj_lens)} trajectories, {num_timesteps} timesteps found')\n",
    "    print(f'Average return: {np.mean(returns):.2f}, std: {np.std(returns):.2f}')\n",
    "    print(f'Max return: {np.max(returns):.2f}, min: {np.min(returns):.2f}')\n",
    "    print('=' * 50)\n",
    "\n",
    "    K = variant['K']\n",
    "    batch_size = variant['batch_size']\n",
    "    num_eval_episodes = variant['num_eval_episodes']\n",
    "    pct_traj = variant.get('pct_traj', 1.)\n",
    "\n",
    "    # only train on top pct_traj trajectories (for %BC experiment)\n",
    "    num_timesteps = max(int(pct_traj*num_timesteps), 1)\n",
    "    sorted_inds = np.argsort(returns)  # lowest to highest\n",
    "    num_trajectories = 1\n",
    "    timesteps = traj_lens[sorted_inds[-1]]\n",
    "    ind = len(trajectories) - 2\n",
    "    while ind >= 0 and timesteps + traj_lens[sorted_inds[ind]] <= num_timesteps:\n",
    "        timesteps += traj_lens[sorted_inds[ind]]\n",
    "        num_trajectories += 1\n",
    "        ind -= 1\n",
    "    sorted_inds = sorted_inds[-num_trajectories:]\n",
    "\n",
    "    # used to reweight sampling so we sample according to timesteps instead of trajectories\n",
    "    p_sample = traj_lens[sorted_inds] / sum(traj_lens[sorted_inds])\n",
    "\n",
    "    def get_batch(batch_size=256, max_len=K):\n",
    "        batch_inds = np.random.choice(\n",
    "            np.arange(num_trajectories),\n",
    "            size=batch_size,\n",
    "            replace=True,\n",
    "            p=p_sample,  # reweights so we sample according to timesteps\n",
    "        )\n",
    "\n",
    "        s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            traj = trajectories[int(sorted_inds[batch_inds[i]])]\n",
    "            si = random.randint(0, traj['rewards'].shape[0] - 1)\n",
    "\n",
    "            # get sequences from dataset\n",
    "            s.append(traj['observations'][si:si + max_len].reshape(1, -1, state_dim))\n",
    "            a.append(traj['actions'][si:si + max_len].reshape(1, -1, act_dim))\n",
    "            r.append(traj['rewards'][si:si + max_len].reshape(1, -1, 1))\n",
    "            if 'terminals' in traj:\n",
    "                d.append(traj['terminals'][si:si + max_len].reshape(1, -1))\n",
    "            else:\n",
    "                d.append(traj['dones'][si:si + max_len].reshape(1, -1))\n",
    "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "            timesteps[-1][timesteps[-1] >= max_ep_len] = max_ep_len-1  # padding cutoff\n",
    "            rtg.append(discount_cumsum(traj['rewards'][si:], gamma=1.)[:s[-1].shape[1] + 1].reshape(1, -1, 1))\n",
    "            if rtg[-1].shape[1] <= s[-1].shape[1]:\n",
    "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "            # padding and state + reward normalization\n",
    "            tlen = s[-1].shape[1]\n",
    "            s[-1] = np.concatenate([np.zeros((1, max_len - tlen, state_dim)), s[-1]], axis=1)\n",
    "            s[-1] = (s[-1] - state_mean) / state_std\n",
    "            a[-1] = np.concatenate([np.ones((1, max_len - tlen, act_dim)) * -10., a[-1]], axis=1)\n",
    "            r[-1] = np.concatenate([np.zeros((1, max_len - tlen, 1)), r[-1]], axis=1)\n",
    "            d[-1] = np.concatenate([np.ones((1, max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "            rtg[-1] = np.concatenate([np.zeros((1, max_len - tlen, 1)), rtg[-1]], axis=1) / scale\n",
    "            timesteps[-1] = np.concatenate([np.zeros((1, max_len - tlen)), timesteps[-1]], axis=1)\n",
    "            mask.append(np.concatenate([np.zeros((1, max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "\n",
    "        s = torch.from_numpy(np.concatenate(s, axis=0)).to(dtype=torch.float32, device=device)\n",
    "        a = torch.from_numpy(np.concatenate(a, axis=0)).to(dtype=torch.float32, device=device)\n",
    "        r = torch.from_numpy(np.concatenate(r, axis=0)).to(dtype=torch.float32, device=device)\n",
    "        d = torch.from_numpy(np.concatenate(d, axis=0)).to(dtype=torch.long, device=device)\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).to(dtype=torch.float32, device=device)\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).to(dtype=torch.long, device=device)\n",
    "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).to(device=device)\n",
    "\n",
    "        return s, a, r, d, rtg, timesteps, mask\n",
    "\n",
    "    def eval_episodes(target_rew):\n",
    "        def fn(model):\n",
    "            returns, lengths = [], []\n",
    "            for _ in range(num_eval_episodes):\n",
    "                with torch.no_grad():\n",
    "                    if model_type == 'dt':\n",
    "                        ret, length = evaluate_episode_rtg(\n",
    "                            env,\n",
    "                            state_dim,\n",
    "                            act_dim,\n",
    "                            model,\n",
    "                            max_ep_len=max_ep_len,\n",
    "                            scale=scale,\n",
    "                            target_return=target_rew/scale,\n",
    "                            mode=mode,\n",
    "                            state_mean=state_mean,\n",
    "                            state_std=state_std,\n",
    "                            device=device,\n",
    "                        )\n",
    "                    else:\n",
    "                        ret, length = evaluate_episode(\n",
    "                            env,\n",
    "                            state_dim,\n",
    "                            act_dim,\n",
    "                            model,\n",
    "                            max_ep_len=max_ep_len,\n",
    "                            target_return=target_rew/scale,\n",
    "                            mode=mode,\n",
    "                            state_mean=state_mean,\n",
    "                            state_std=state_std,\n",
    "                            device=device,\n",
    "                        )\n",
    "                returns.append(ret)\n",
    "                lengths.append(length)\n",
    "            return {\n",
    "                f'target_{target_rew}_return_mean': np.mean(returns),\n",
    "                f'target_{target_rew}_return_std': np.std(returns),\n",
    "                f'target_{target_rew}_length_mean': np.mean(lengths),\n",
    "                f'target_{target_rew}_length_std': np.std(lengths),\n",
    "            }\n",
    "        return fn\n",
    "\n",
    "    if model_type == 'dt':\n",
    "        model = DecisionTransformer(\n",
    "            state_dim=state_dim,\n",
    "            act_dim=act_dim,\n",
    "            max_length=K,\n",
    "            max_ep_len=max_ep_len,\n",
    "            hidden_size=variant['embed_dim'],\n",
    "            n_layer=variant['n_layer'],\n",
    "            n_head=variant['n_head'],\n",
    "            n_inner=4*variant['embed_dim'],\n",
    "            activation_function=variant['activation_function'],\n",
    "            n_positions=1024,\n",
    "            resid_pdrop=variant['dropout'],\n",
    "            attn_pdrop=variant['dropout'],\n",
    "        )\n",
    "    elif model_type == 'bc':\n",
    "        model = MLPBCModel(\n",
    "            state_dim=state_dim,\n",
    "            act_dim=act_dim,\n",
    "            max_length=K,\n",
    "            hidden_size=variant['embed_dim'],\n",
    "            n_layer=variant['n_layer'],\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    warmup_steps = variant['warmup_steps']\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=variant['learning_rate'],\n",
    "        weight_decay=variant['weight_decay'],\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lambda steps: min((steps+1)/warmup_steps, 1)\n",
    "    )\n",
    "\n",
    "    if model_type == 'dt':\n",
    "        trainer = SequenceTrainer(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            batch_size=batch_size,\n",
    "            get_batch=get_batch,\n",
    "            scheduler=scheduler,\n",
    "            loss_fn=lambda s_hat, a_hat, r_hat, s, a, r: torch.mean((a_hat - a)**2),\n",
    "            eval_fns=[eval_episodes(tar) for tar in env_targets],\n",
    "        )\n",
    "    elif model_type == 'bc':\n",
    "        trainer = ActTrainer(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            batch_size=batch_size,\n",
    "            get_batch=get_batch,\n",
    "            scheduler=scheduler,\n",
    "            loss_fn=lambda s_hat, a_hat, r_hat, s, a, r: torch.mean((a_hat - a)**2),\n",
    "            eval_fns=[eval_episodes(tar) for tar in env_targets],\n",
    "        )\n",
    "\n",
    "    if log_to_wandb:\n",
    "        wandb.init(\n",
    "            name=exp_prefix,\n",
    "            group=group_name,\n",
    "            project='decision-transformer',\n",
    "            config=variant\n",
    "        )\n",
    "        # wandb.watch(model)  # wandb has some bug\n",
    "\n",
    "    for iter in range(variant['max_iters']):\n",
    "        outputs = trainer.train_iteration(num_steps=variant['num_steps_per_iter'], iter_num=iter+1, print_logs=True)\n",
    "        if log_to_wandb:\n",
    "            wandb.log(outputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision-transformer-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
