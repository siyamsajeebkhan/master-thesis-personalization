{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from gym import utils\n",
    "import gym\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLens Dataset Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the MovieLens dataset\n",
    "ML_LATEST_SMALL_DATA_ROOT_PATH = \"../data/dt-datasets/movielens/ml-latest-small\"\n",
    "ML_LATEST_DATA_ROOT_PATH = \"../data/dt-datasets/movielens/ml-latest\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the ML-latest-small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df = pd.read_csv(os.path.join(ML_LATEST_SMALL_DATA_ROOT_PATH, \"links.csv\"))\n",
    "movies_df = pd.read_csv(os.path.join(ML_LATEST_SMALL_DATA_ROOT_PATH, \"movies.csv\"))\n",
    "ratings_df = pd.read_csv(os.path.join(ML_LATEST_SMALL_DATA_ROOT_PATH, \"ratings.csv\"))\n",
    "tags_df = pd.read_csv(os.path.join(ML_LATEST_SMALL_DATA_ROOT_PATH, \"tags.csv\"))\n",
    "print(f\"links shape: {links_df.shape}\\nmovies shape: {movies_df.shape}\\nratings shape: {ratings_df.shape}\\ntags shape: {tags_df.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the ML-latest-full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_25m_df = pd.read_csv(os.path.join(ML_LATEST_DATA_ROOT_PATH, \"links.csv\"))\n",
    "movies_25m_df = pd.read_csv(os.path.join(ML_LATEST_DATA_ROOT_PATH, \"movies.csv\"))\n",
    "ratings_25m_df = pd.read_csv(os.path.join(ML_LATEST_DATA_ROOT_PATH, \"ratings.csv\"))\n",
    "tags_25m_df = pd.read_csv(os.path.join(ML_LATEST_DATA_ROOT_PATH, \"tags.csv\"))\n",
    "print(f\"links shape: {links_25m_df.shape}\\nmovies shape: {movies_25m_df.shape}\\nratings shape: {ratings_25m_df.shape}\\ntags shape: {tags_25m_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data distribution of the rating count per user in the small dataset\n",
    "rating_count_per_user = ratings_df.groupby('userId')['movieId'].count().values\n",
    "print(f'min no of rating: {np.min(rating_count_per_user)}, max no of rating: {np.max(rating_count_per_user)}')\n",
    "print(f'average rating per user: {np.mean(rating_count_per_user)}, median rating per user: {np.median(rating_count_per_user)}')\n",
    "print(f'number of users who rated less than 140 movies: {(rating_count_per_user <= 140).sum()}')\n",
    "sns.histplot(data = rating_count_per_user)\n",
    "plt.show()\n",
    "# sns.boxplot(rating_count_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data distribution of the rating count per user in the small dataset\n",
    "rating_count_per_user = ratings_25m_df.groupby('userId')['movieId'].count().values\n",
    "print(f'min no of rating: {np.min(rating_count_per_user)}, max no of rating: {np.max(rating_count_per_user)}')\n",
    "print(f'average rating per user: {np.mean(rating_count_per_user)}, median rating per user: {np.median(rating_count_per_user)}')\n",
    "\n",
    "rating_count_less_than_140 = (rating_count_per_user <= 140).sum()\n",
    "print(f'number of users who rated less than 140 movies: {rating_count_less_than_140} which is {(rating_count_less_than_140 / ratings_25m_df.userId.unique().shape[0]) * 100}% of the users.')\n",
    "sns.histplot(data = rating_count_per_user, bins=100)\n",
    "plt.show()\n",
    "# sns.boxplot(rating_count_per_user)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movies_df['title'].unique().shape[0])\n",
    "\n",
    "# Find the duplicated movie titles\n",
    "duplicated_movies = movies_df[movies_df['title'].duplicated(keep=False)].sort_values(by=['title', 'movieId'])\n",
    "\n",
    "# Some movies have additional genres, that's why the duplicates\n",
    "# Merge the genres, keep the ID of the one that has the highest number\n",
    "# of genres, create a map of the old and new id\n",
    "# drop the row with the least genre after merging\n",
    "duplicated_movie_titles = duplicated_movies['title'].unique()\n",
    "movie_ids_to_remove = duplicated_movies['movieId'].values\n",
    "movie_indices_to_remove = duplicated_movies.index.values\n",
    "id_mapping = {}\n",
    "resolved_movies = []\n",
    "for title in duplicated_movie_titles:\n",
    "    movie_ids = duplicated_movies[duplicated_movies['title'] == title]['movieId'].values\n",
    "    genre_groups = duplicated_movies[duplicated_movies['title'] == title]['genres'].values\n",
    "\n",
    "    for i in range(1, len(movie_ids)):\n",
    "        id_mapping[movie_ids[i]] = movie_ids[0]\n",
    "    merged_genres = set()\n",
    "    for genre_grp in genre_groups:\n",
    "        genres = genre_grp.split('|')\n",
    "        merged_genres = merged_genres.union(set(genres))\n",
    "    merged_genres = list(merged_genres)\n",
    "    genres_mixed = \"\"\n",
    "    for (i, g) in enumerate(merged_genres):\n",
    "        if i != len(merged_genres)-1:\n",
    "            genres_mixed += g + \"|\"\n",
    "        else:\n",
    "            genres_mixed += g\n",
    "\n",
    "    resolved_movies.append({'movieId': movie_ids[0], 'title': title, 'genres': genres_mixed})\n",
    "\n",
    "pd.DataFrame(resolved_movies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now remove the duplicate movies from the movies_df\n",
    "print(f\"Shape before rmeoving: {movies_df.shape}\")\n",
    "movies_df = movies_df.drop(movie_indices_to_remove)\n",
    "print(f\"Shape after rmeoving: {movies_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now concatenate the movies_df with removed movies and the merged_genres movies\n",
    "movies_df = pd.concat([movies_df, pd.DataFrame(resolved_movies)], ignore_index=True)\n",
    "print(movies_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have to replace the removed movie ids in the ratings_df\n",
    "# and also in the tags_df\n",
    "ratings_df.head()\n",
    "ratings_df_copy = ratings_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the ids with the id_mapping\n",
    "ratings_df_copy['movieId'] = ratings_df_copy['movieId'].replace(id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the duplicates\n",
    "ratings_df_copy[ratings_df_copy[['userId', 'movieId']].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df_copy = ratings_df_copy.drop_duplicates(['userId', 'movieId', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only one movie with two different ratings from the same user\n",
    "ratings_df_copy[ratings_df_copy[['userId', 'movieId']].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will drop the one with the lowest rating\n",
    "ratings_df = ratings_df_copy.drop([11241])\n",
    "ratings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no more duplicates\n",
    "ratings_df[ratings_df[['userId', 'movieId']].duplicated(keep=False)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have to replace the removed movie ids in the tags_df\n",
    "tags_df_copy = tags_df.copy()\n",
    "tags_df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the ids first with the id_mapping\n",
    "tags_df_copy['movieId'] = tags_df_copy['movieId'].replace(id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the duplicates\n",
    "tags_df_copy[tags_df_copy[['userId', 'movieId']].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results suggest that there are some outliers in the dataset\n",
    "tags_df_copy.groupby(['userId'])['movieId'].count().mean(), tags_df_copy.groupby(['userId'])['movieId'].count().median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of tags per user per movie\n",
    "tags_df_copy.groupby(['userId', 'movieId']).count()['tag'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier: user 599 provided 173 tags for the movie 296\n",
    "print(f\"name of the movie: {movies_df[movies_df['movieId'] == 296]['title'].values[0]}\")\n",
    "tags_df[(tags_df['userId'] == 599) & (tags_df['movieId'] == 296)].sort_values(by='timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags_df['tag'] = tags_df['tag'].str.lower()\n",
    "tags_df['tag'] = tags_df['tag'].str.lower().astype(str)\n",
    "\n",
    "# Grouping by 'movieId' and 'userId' and aggregating tags\n",
    "tags_df_merged = tags_df.groupby(['movieId', 'userId'])['tag'].agg(lambda x: '|'.join(x)).reset_index()\n",
    "tags_df_merged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge the movies and ratings dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_and_ratings = pd.merge(movies_df, ratings_df, on='movieId')\n",
    "movies_and_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_and_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any duplicate\n",
    "print(movies_and_ratings.duplicated().sum())\n",
    "\n",
    "print(movies_and_ratings.isna().sum())\n",
    "movies_and_ratings[movies_and_ratings[['movieId', 'userId']].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_and_ratings.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any null values\n",
    "movies_and_ratings['userId'].isna().sum(),  movies_and_ratings['movieId'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all the userIds are present in the merged df as well\n",
    "sorted(ratings_df.userId.unique()) ==  sorted(movies_and_ratings.userId.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the movie ratings by userId and timestamp in an ascending manner\n",
    "movies_and_ratings = movies_and_ratings.sort_values(by=['userId', 'timestamp'])\n",
    "movies_and_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the different rating values\n",
    "sorted(movies_and_ratings['rating'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another reward scheme\n",
    "# Scaling the rewards between 0 & 1\n",
    "highest_rating = 5.0\n",
    "movies_and_ratings['reward'] = (1 - abs(movies_and_ratings['rating'] - highest_rating) / 4.5) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge the movies and ratings dataframes together with the tags dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ids_set = set(movies_and_ratings['movieId'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ids_set_from_tags_df = set(tags_df['movieId'].tolist())\n",
    "len(movie_ids_set_from_tags_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(movie_ids_set_from_tags_df.difference(movie_ids_set.intersection(movie_ids_set_from_tags_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_ratings_and_tags = pd.merge(movies_and_ratings, tags_df_merged, on=['movieId', 'userId'], how='left')\n",
    "movies_ratings_and_tags['tag'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many different genres do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.concatenate(movies_and_ratings['genres'].apply(lambda g: g.split(\"|\")).tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = movies_and_ratings['genres']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using BERT: <a>https://colab.research.google.com/drive/1ea3zDFrCQFQhkvinaQfdbvXlOhR7hw01?usp=sharing#scrollTo=pqYxJi28X63W</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "           output_hidden_states = True,)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2TokenizerFast, GPT2LMHeadModel, GPT2Tokenizer\n",
    "# model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "# word_embeddings = model.transformer.wte.weight # Word Token Embeddings\n",
    "# position_embeddings = model.transformer.wpe.weight  # Word Position Embeddings\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = tags_df.groupby(['userId', 'movieId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding the movie titles, genres and the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With BERT\n",
    "\n",
    "def bert_text_preparation(text, tokenizer):\n",
    "  \"\"\"\n",
    "  Preprocesses text input in a way that BERT can interpret.\n",
    "  \"\"\"\n",
    "  marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "  tokenized_text = tokenizer.tokenize(marked_text)\n",
    "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "  segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "  # convert inputs to tensors\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  segments_tensor = torch.tensor([segments_ids])\n",
    "\n",
    "  return tokenized_text, tokens_tensor, segments_tensor\n",
    "\n",
    "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
    "    \"\"\"\n",
    "    Obtains BERT embeddings for tokens, in context of the given sentence.\n",
    "    \"\"\"\n",
    "    # gradient calculation id disabled\n",
    "    with torch.no_grad():\n",
    "      # obtain hidden states\n",
    "      outputs = model(tokens_tensor, segments_tensor)\n",
    "      hidden_states = outputs[2]\n",
    "\n",
    "    # concatenate the tensors for all layers\n",
    "    # use \"stack\" to create new dimension in tensor\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # remove dimension 1, the \"batches\"\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # swap dimensions 0 and 1 so we can loop over tokens\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # intialized list to store embeddings\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # \"token_embeddings\" is a [Y x 12 x 768] tensor\n",
    "    # where Y is the number of tokens in the sentence\n",
    "\n",
    "    # loop over tokens in sentence\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # \"token\" is a [12 x 768] tensor\n",
    "\n",
    "        # sum the vectors from the last four layers\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "        # token_vecs_sum_tensor = torch.stack(token_vecs_sum)\n",
    "        # token_embedding = torch.sum(token_vecs_sum_tensor, dim=0)\n",
    "\n",
    "    # return token_embedding\n",
    "    return token_vecs_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_embeddings(tokenized_text, list_token_embeddings):\n",
    "    context_embeddings = []\n",
    "    context_tokens = []\n",
    "    # make ordered dictionary to keep track of the position of each word\n",
    "    tokens = OrderedDict()\n",
    "\n",
    "    # loop over tokens in sensitive sentence\n",
    "    for token in tokenized_text[1:-1]:\n",
    "        # keep track of position of word and whether it occurs multiple times\n",
    "        if token in tokens:\n",
    "            tokens[token] += 1\n",
    "        else:\n",
    "            tokens[token] = 1\n",
    "\n",
    "        # compute the position of the current token\n",
    "        token_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n",
    "        current_index = token_indices[tokens[token]-1]\n",
    "\n",
    "        # get the corresponding embedding\n",
    "        token_vec = list_token_embeddings[current_index]\n",
    "        # save values\n",
    "        context_tokens.append(token)\n",
    "        context_embeddings.append(token_vec)\n",
    "    context_embeddings_tensor = torch.stack(context_embeddings)\n",
    "    merged_embedding = torch.sum(context_embeddings_tensor, dim=0)\n",
    "\n",
    "    # print(f\"context_embeddings_tensor.shape: {context_embeddings_tensor.shape}\")\n",
    "    return np.array(merged_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_ratings_and_tags['tag'].fillna('NA', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For BERT\n",
    "# Embed a movie title\n",
    "def embed_movie_title(title):\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(title, tokenizer)\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "    title_embedding = merge_embeddings(tokenized_text, list_token_embeddings)\n",
    "    return title_embedding\n",
    "\n",
    "# Embed the movie genres and produce an evrage embedding vector\n",
    "def embed_movie_genres(genres):\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(genres.replace(\"|\", \" \"), tokenizer)\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "    genres_embedding = merge_embeddings(tokenized_text, list_token_embeddings)\n",
    "    return genres_embedding\n",
    "\n",
    "def embed_movie_tags(tags):\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(tags.replace(\"|\", \" \"), tokenizer)\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "    tags_embedding = merge_embeddings(tokenized_text, list_token_embeddings)\n",
    "    return tags_embedding\n",
    "\n",
    "# # Embed the userId\n",
    "# def embed_userid(user_id):\n",
    "#     embedding = nn.Embedding(num_users, embedding_dim)\n",
    "#     genres_embedding = merge_embeddings(tokenized_text, list_token_embeddings)\n",
    "#     return genres_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For GPT2\n",
    "# # Embed a movie title\n",
    "# def embed_movie_title(title):\n",
    "#     text_index = tokenizer.encode(title, add_prefix_space=True)\n",
    "#     vector = model.transformer.wte.weight[text_index,:]\n",
    "#     return np.average(vector.detach().numpy(), axis=0)\n",
    "\n",
    "# # Embed the movie genres and produce an evrage embedding vector\n",
    "# def embed_movie_genres(genres):\n",
    "#     text_index = tokenizer.encode(genres.split('|'), add_prefix_space=True)\n",
    "#     vector = model.transformer.wte.weight[text_index,:]\n",
    "#     return np.average(vector.detach().numpy(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Introduce reward\n",
    "# # DO NOT DELETE THIS CELL \n",
    "# movies_and_ratings['reward'] = 1\n",
    "# movies_and_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summation_based_merging():\n",
    "    pass\n",
    "def concatenation_based_merging(embedding_list):\n",
    "    embedding_arr_list = []\n",
    "    for embed in embedding_list:\n",
    "        with torch.no_grad():\n",
    "            embedding_arr_list.append(np.vstack(embed.values))\n",
    "    merged_embeddings = np.concatenate(embedding_arr_list, axis=1)\n",
    "    return merged_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now produce the embeddings\n",
    "# from tqdm import tqdm\n",
    "# groups = movies_and_ratings.groupby('userId')\n",
    "# actions = np.arange(0.5, 5.5, 0.5)\n",
    "\n",
    "# custom_act_to_orig_act = {idx: action for idx, action in enumerate(actions)}\n",
    "# orig_act_to_custom_act = {action: idx for idx, action in enumerate(actions)}\n",
    "\n",
    "# group_list = []\n",
    "# train_trajectories, test_trajectories = [], []\n",
    "\n",
    "# user_id_embedder = torch.nn.Embedding(610, 32)\n",
    "\n",
    "# for group_id, group_df in tqdm(groups):\n",
    "#    # group_list.append(group_df)\n",
    "#    rewards = group_df['reward'].tolist()\n",
    "#    user_id = group_df['userId'].unique()[0]\n",
    "   \n",
    "#    raw_observations = group_df.drop(['movieId', 'rating', 'timestamp', 'reward'], axis=1)\n",
    "\n",
    "#    titles_embeddings = raw_observations['title'].apply(lambda x: embed_movie_title(x))\n",
    "#    genres_embeddings = raw_observations['genres'].apply(lambda x: embed_movie_genres(x))\n",
    "#    user_id_embeddings = raw_observations['userId'].apply(lambda x: user_id_embedder(torch.LongTensor([x-1])))\n",
    "\n",
    "\n",
    "#    titles_embeds_arr = np.vstack(titles_embeddings.values)\n",
    "#    genres_embeds_arr = np.vstack(genres_embeddings.values)\n",
    "\n",
    "#    with torch.no_grad():\n",
    "#       user_id_embeds_arr = np.vstack(user_id_embeddings.values)\n",
    "   \n",
    "#    observations = np.concatenate((titles_embeds_arr, genres_embeds_arr, user_id_embeds_arr), axis=1)\n",
    "\n",
    "#    features = raw_observations.columns\n",
    "#    targets = group_df['rating'].tolist()\n",
    "#    # observations = observations.values.tolist()\n",
    "#    actions = [orig_act_to_custom_act[temp] for temp in targets]\n",
    "\n",
    "#    next_observations = observations\n",
    "   \n",
    "#    terminals = [False]*(len(observations)-1) + [True]\n",
    "#    trajectory = {\n",
    "#       'observations': observations,\n",
    "#       'next_observations': next_observations,\n",
    "#       'actions': np.array(actions),\n",
    "#       'rewards': np.array(rewards),\n",
    "#       'terminals': np.array(terminals),\n",
    "#       'features': np.array(features),\n",
    "#       'targets': np.array(targets),\n",
    "#       'user_id': user_id,\n",
    "#    }\n",
    "#    train_trajectories.append(trajectory)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_ratings_and_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now produce the embeddings\n",
    "from tqdm import tqdm\n",
    "groups = movies_ratings_and_tags.groupby('userId')\n",
    "actions = np.arange(0.5, 5.5, 0.5)\n",
    "\n",
    "custom_act_to_orig_act = {idx: action for idx, action in enumerate(actions)}\n",
    "orig_act_to_custom_act = {action: idx for idx, action in enumerate(actions)}\n",
    "\n",
    "group_list = []\n",
    "all_trajectories, test_trajectories = [], []\n",
    "\n",
    "user_id_embedder = torch.nn.Embedding(610, 32)\n",
    "\n",
    "for group_id, group_df in tqdm(groups):\n",
    "   # group_list.append(group_df)\n",
    "   rewards = group_df['reward'].tolist()\n",
    "   user_id = group_df['userId'].unique()[0]\n",
    "   \n",
    "   raw_observations = group_df.drop(['movieId', 'rating', 'timestamp', 'reward'], axis=1)\n",
    "\n",
    "   titles_embeddings = raw_observations['title'].apply(lambda x: embed_movie_title(x))\n",
    "   genres_embeddings = raw_observations['genres'].apply(lambda x: embed_movie_genres(x))\n",
    "   tags_embeddings = raw_observations['tag'].apply(lambda x: embed_movie_tags(x))\n",
    "\n",
    "   user_id_embeddings = raw_observations['userId'].apply(lambda x: user_id_embedder(torch.LongTensor([x-1])))\n",
    "\n",
    "   titles_and_genres_embeddings_merged = concatenation_based_merging([titles_embeddings, genres_embeddings, tags_embeddings])\n",
    "\n",
    "   titles_embeds_arr = np.vstack(titles_embeddings.values)\n",
    "   genres_embeds_arr = np.vstack(genres_embeddings.values)\n",
    "\n",
    "   \n",
    "   observations = concatenation_based_merging([titles_embeddings, genres_embeddings, tags_embeddings, user_id_embeddings])\n",
    "\n",
    "   # print(f\"Shape of the observations: {observations.shape}\")\n",
    "\n",
    "   features = raw_observations.columns\n",
    "   targets = group_df['rating'].tolist()\n",
    "   # observations = observations.values.tolist()\n",
    "   actions = [orig_act_to_custom_act[temp] for temp in targets]\n",
    "\n",
    "   next_observations = observations\n",
    "   \n",
    "   terminals = [False]*(len(observations)-1) + [True]\n",
    "   trajectory = {\n",
    "      'observations': observations,\n",
    "      'next_observations': next_observations,\n",
    "      'actions': np.array(actions),\n",
    "      'rewards': np.array(rewards),\n",
    "      'terminals': np.array(terminals),\n",
    "      'features': np.array(features),\n",
    "      'targets': np.array(targets),\n",
    "      'user_id': user_id,\n",
    "   }\n",
    "   all_trajectories.append(trajectory)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the all-trajectories to the disk\n",
    "import pickle\n",
    "with open('../data/dt-datasets/movielens/processed-data/all_trajectories_with_concatenated_movname_genres_tags_userid_reward_of_scale_5.pkl', 'wb') as f:\n",
    "    pickle.dump(all_trajectories, f)\n",
    "\n",
    "# Write the df to the disk\n",
    "movies_ratings_and_tags.to_csv(\"../data/movies_ratings_and_tags_mlens_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preloaded from disk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "movies_ratings_and_tags = pd.read_csv(\"../data/movies_ratings_and_tags_mlens_small.csv\")\n",
    "movies_ratings_and_tags.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "with open(\"../data/dt-datasets/movielens/processed-data/all_trajectories_with_concatenated_movname_genres_tags_userid_reward_of_scale_5.pkl\", 'rb') as f:\n",
    "    all_trajectories = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_nearest_half(number):\n",
    "    return round(number * 2) / 2\n",
    "\n",
    "# Example usage\n",
    "num = 3.7\n",
    "rounded_num = round_to_nearest_half(num)\n",
    "print(rounded_num) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_by_user = movies_ratings_and_tags[(movies_ratings_and_tags['movieId'] == 804) & (movies_ratings_and_tags['userId'] == 1)]['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_by_user.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# movie_embed_to_id = {}\n",
    "# for traj in tqdm(all_trajectories):\n",
    "#     user_id = traj['user_id']\n",
    "#     movie_ids = (movies_ratings_and_tags[movies_ratings_and_tags['userId'] == user_id]['movieId']).tolist()\n",
    "#     observations = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "#     for (mid, obs) in zip(movie_ids, observations):\n",
    "#         movie_embed_to_id[tuple(obs)] = mid\n",
    "\n",
    "# with open(\"../data/dt-datasets/movielens/processed-data/movie_embed_to_id_mapping.pkl\", 'wb') as f:\n",
    "#     pickle.dump(movie_embed_to_id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/dt-datasets/movielens/processed-data/movie_embed_to_id_mapping.pkl\", 'rb') as f:\n",
    "    movie_embed_to_id = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the trajectories now\n",
    "for traj in all_trajectories:\n",
    "    traj['actions'] = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "    traj['observations'] = traj['observations'][:, 768*3: 768*3+32]\n",
    "    traj['rewards'] = traj['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the size for the training set\n",
    "np.random.seed(42)\n",
    "trajectories = all_trajectories\n",
    "indices = {i for i in range(len(trajectories))}\n",
    "train_indices = list(np.random.choice(list(indices), size=round(0.7*len(indices)), replace=False))\n",
    "remaining_indices = indices.difference(train_indices)\n",
    "test_indices = remaining_indices\n",
    "\n",
    "print(f\"total train users: {len(train_indices)}\")\n",
    "print(f\"total test users: {len(test_indices)}\")\n",
    "\n",
    "train_trajectories = [trajectories[idx]for idx in train_indices]\n",
    "test_trajectories = [trajectories[idx]for idx in test_indices]\n",
    "\n",
    "print(\"Train set:\", len(train_trajectories))\n",
    "print(\"Test set:\", len(test_trajectories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 'movies as actions' trajectories\n",
    "# Save the train and test trajectories as pickle files to load them later\n",
    "import pickle\n",
    "with open(f'../data/dt-datasets/movielens/train-test-sets/mlens-train-trajectories-movies-as-actions.pkl', 'wb') as f:\n",
    "    pickle.dump(train_trajectories, f)\n",
    "with open(f'../data/dt-datasets/movielens/train-test-sets/mlens-test-trajectories-movies-as-actions', 'wb') as f:\n",
    "    pickle.dump(test_trajectories, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocab of all movies and save\n",
    "all_actions = list(movie_embed_to_id.keys())\n",
    "action_vocab = np.array(all_actions)\n",
    "\n",
    "with open (\"../data/dt-datasets/movielens/processed-data/action_vocab.pkl\", 'wb') as f:\n",
    "    pickle.dump(action_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save file name\n",
    "# total_users = movies_and_ratings['userId'].nunique()\n",
    "# total_ratings = movies_and_ratings.shape[0]\n",
    "# total_features = len(train_trajectories[0]['features'])\n",
    "# reward_system = 'range_0_to_1'\n",
    "# min_event_count = 20\n",
    "# version_no = 1\n",
    "\n",
    "# all_traj_pkl_file_name = f\"all_trajectories_{total_users}_users_{total_ratings}_events_{total_features}_features_min_events_{min_event_count}_{reward_system}_rewards_v{version_no}_with_tags.pkl\"\n",
    "# all_traj_pkl_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the trajectories\n",
    "# # Save the train and test trajectories as pickle files to load them later\n",
    "# import pickle\n",
    "# with open(f'../data/dt-datasets/movielens/train-test-sets/mlens-train-trajectories-v{version_no}_with_0_to_1_reward_with_tags.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_trajectories, f)\n",
    "# with open(f'../data/dt-datasets/movielens/train-test-sets/mlens-test-trajectories-v{version_no}_with_0_to_1_reward_with_tags.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_trajectories, f)\n",
    "# with open(f'../data/dt-datasets/movielens/all-trajs/{all_traj_pkl_file_name}', 'wb') as f:\n",
    "#     pickle.dump(trajectories, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sum_up_embeddings(trajectories):\n",
    "#     for traj in tqdm(trajectories):\n",
    "#         titles_embeddings = traj['observations'][:, 0:768]\n",
    "#         genres_embeddings = traj['observations'][:, 768:1536]\n",
    "#         # tags_embeddings = traj['observations'][:, 1536:2304]\n",
    "#         user_id_embeddings = traj['observations'][:, 2304:2336]\n",
    "\n",
    "#         # summed_embeddings = titles_embeddings + genres_embeddings + tags_embeddings\n",
    "#         summed_embeddings = titles_embeddings + genres_embeddings\n",
    "#         observations = np.concatenate((summed_embeddings, user_id_embeddings), axis=1)\n",
    "#         traj['observations'] = observations\n",
    "#     return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_reward_to_binary_reward(trajectories):\n",
    "    for traj in tqdm(trajectories):\n",
    "        binary_rewards = (traj['targets'] >= 3.5).astype(int)\n",
    "        traj['rewards'] = binary_rewards\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trajectories[0]['actions'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'../data/dt-datasets/movielens/train-test-sets/mlens-train-trajectories-v1_with_0_to_1_reward_with_tags.pkl', 'rb') as f:\n",
    "#     train_trajectories = pickle.load(f)\n",
    "\n",
    "# with open(f'../data/dt-datasets/movielens/train-test-sets/mlens-test-trajectories-v1_with_0_to_1_reward_with_tags.pkl', 'rb') as f:\n",
    "#     test_trajectories = pickle.load(f)\n",
    "\n",
    "train_trajectories = sum_up_embeddings(train_trajectories)\n",
    "test_trajectories = sum_up_embeddings(test_trajectories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the rewards to binary\n",
    "train_trajectories = convert_reward_to_binary_reward(train_trajectories)\n",
    "test_trajectories = convert_reward_to_binary_reward(test_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trajectories[0]['observations'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [3.0, 3.5, 2.5, 4.0, 5.0, 1.5]\n",
    "np.sum((np.array(a) >= 3.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 4\n",
    "pred = 3\n",
    "\n",
    "if target >= 3.5 and pred >= 3.5:\n",
    "    print(f\"reward: 1\")\n",
    "else:\n",
    "    print(\"reward 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trajectories\n",
    "# Save the train and test trajectories as pickle files to load them later\n",
    "import pickle\n",
    "with open(f'../data/dt-datasets/movielens/train-test-sets/mlens-train-trajectories-v{version_no}_with_binary_reward_wo_tags_and_summed.pkl', 'wb') as f:\n",
    "    pickle.dump(train_trajectories, f)\n",
    "with open(f'../data/dt-datasets/movielens/train-test-sets/mlens-test-trajectories-v{version_no}_with_binary_reward_wo_tags_and_summed.pkl', 'wb') as f:\n",
    "    pickle.dump(test_trajectories, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trajectories[0]['observations'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3*768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = (train_trajectories[0]['targets'] >= 3.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_trajectory(df):\n",
    "#     actions = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n",
    "\n",
    "#     custom_act_to_orig_act = {idx: action for idx, action in enumerate(actions)}\n",
    "#     orig_act_to_custom_act = {action: idx for idx, action in enumerate(actions)}\n",
    "#     observations = []\n",
    "#     next_observations = []\n",
    "#     actions = []\n",
    "#     rewards = []\n",
    "#     terminals = []\n",
    "#     train_traj, test_traj, eval_traj = {}, {}, {}\n",
    "#     for idx, row in df.iterrows():\n",
    "#         title_embed = embed_movie_title(row.title)\n",
    "#         genres_embed = embed_movie_genres(row.genres)\n",
    "#         obs = np.concatenate((title_embed, genres_embed))\n",
    "#         # print(f'obs shape: {obs.shape}')\n",
    "#         observations.append(obs)\n",
    "#         actions.append(orig_act_to_custom_act[row['rating']])\n",
    "#         # print(f\"converted action: {actions[-1]}\")\n",
    "#         rewards.append(0)\n",
    "#         terminals.append(False)\n",
    "#     next_observations = observations.copy()\n",
    "#     observations.pop(df.shape[0]-1)\n",
    "#     next_observations.pop(0)\n",
    "#     terminals.pop(0)\n",
    "#     terminals[len(terminals)-1] = True\n",
    "#     rewards.pop(0)\n",
    "\n",
    "#     observations = np.array(observations)\n",
    "#     next_observations = np.array(next_observations)\n",
    "#     actions = np.array(actions)\n",
    "#     rewards = np.array(rewards)\n",
    "#     terminals = np.array(terminals)\n",
    "\n",
    "#     # print(f\"observations.shape: {observations.shape}, terminals.shape: {terminals.shape}, rewards.shape: {rewards.shape}\")\n",
    "#     # return observations, next_observations, actions, rewards, terminals\n",
    "#     # return trajectory\n",
    "#     # print(f'observations shape: {len(observations)} and next_observations shape: {len(next_observations)}')\n",
    "#     # print(f'observations shape: {observations.shape} and next_observations shape: {next_observations.shape}')\n",
    "#     # We will use a 70-30 split for training and testing\n",
    "#     # First select the indices for training and testing\n",
    "#     indices = {i for i in range(len(observations))}\n",
    "#     # print(f'total indices: {len(indices)}')\n",
    "#     # Randomly select indices for training and testing\n",
    "#     eval_indices = list(np.random.choice(list(indices), size=round(0.2*len(indices)), replace=False))\n",
    "#     remaining_indices = list(indices.difference(eval_indices))\n",
    "#     test_indices = list(np.random.choice(list(remaining_indices), size=round(0.2*len(indices)), replace=False))\n",
    "#     train_indices = list(remaining_indices.difference(test_indices))\n",
    "\n",
    "#     train_traj['observations'] = observations[train_indices]\n",
    "#     train_traj['next_observations'] = next_observations[train_indices]\n",
    "#     train_traj['actions'] = actions[train_indices]\n",
    "#     train_traj['rewards'] = rewards[train_indices]\n",
    "#     train_traj['terminals'] = terminals[train_indices]\n",
    "\n",
    "#     eval_traj['observations'] = observations[eval_indices]\n",
    "#     eval_traj['actions'] = actions[eval_indices]\n",
    "\n",
    "#     test_traj['observations'] = observations[test_indices]\n",
    "#     test_traj['actions'] = actions[test_indices]\n",
    "#     return train_traj, eval_traj, test_traj\n",
    "#     # print(f\"train instances: {len(train_indices)} and test indices: {len(test_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_trajectory_smart(df):\n",
    "#     observations = []\n",
    "#     next_observations = []\n",
    "#     actions = []\n",
    "#     rewards = []\n",
    "#     terminals = []\n",
    "#     df['title'].apply(lambda x: embed_movie_title(x))\n",
    "\n",
    "\n",
    "#     for idx, row in df.iterrows():\n",
    "#         title_embed = embed_movie_title(row.title)\n",
    "#         genres_embed = embed_movie_genres(row.genres)\n",
    "#         obs = np.concatenate((title_embed, genres_embed))\n",
    "#         # print(f'obs shape: {obs.shape}')\n",
    "#         observations.append(obs)\n",
    "#         actions.append(row['rating'])\n",
    "#         rewards.append(0)\n",
    "#         terminals.append(False)\n",
    "#     next_observations = observations.copy()\n",
    "#     observations.pop(df.shape[0]-1)\n",
    "#     next_observations.pop(0)\n",
    "#     print(f'observations shape: {len(observations)} and next_observations shape: {len(next_observations)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "user_ids = movies_and_ratings.userId.unique()\n",
    "mlens_train_trajectories = []\n",
    "test_obs, test_actions = [], []\n",
    "eval_obs, eval_actions = [], []\n",
    "\n",
    "for user_id in tqdm(user_ids):\n",
    "    df_to_prepare = movies_and_ratings[movies_and_ratings.userId == user_id]\n",
    "    train_traj, eval_traj, test_traj = prepare_trajectory(df_to_prepare)\n",
    "    mlens_train_trajectories.append(train_traj)\n",
    "    eval_obs.append(eval_traj['observations'])\n",
    "    eval_actions.append(eval_traj['actions'])\n",
    "\n",
    "    test_obs.append(test_traj['observations'])\n",
    "    test_actions.append(test_traj['actions'])\n",
    "\n",
    "# print(f\"Total train trajectories: {sum([len(traj['observations']) for traj in mlens_train_trajectories])} and total test trajectories{sum([len(traj['observations']) for traj in mlens_test_trajectories])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlens_eval_trajectories, mlens_test_trajectories = {}\n",
    "\n",
    "mlens_eval_trajectories['observations'] = np.concatenate(eval_obs)\n",
    "mlens_eval_trajectories['actions'] = np.concatenate(eval_actions)\n",
    "\n",
    "mlens_test_trajectories['observations'] = np.concatenate(test_obs)\n",
    "mlens_test_trajectories['actions'] = np.concatenate(test_actions)\n",
    "\n",
    "mlens_eval_trajectories['observations'].shape, mlens_test_trajectories['observations'].shape\n",
    "# mlens_test_trajectories = pd.DataFrame(mlens_test_trajectories)\n",
    "# mlens_test_trajectories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the train trajectories\n",
    "with open('../gym/data/mlens/mlens-train-trajectories-v1.pkl', 'wb') as file:\n",
    "    pickle.dump(mlens_train_trajectories, file)\n",
    "# Pickle the eval trajectories\n",
    "with open('../gym/data/mlens/mlens-eval-trajectories-v1.pkl', 'wb') as file:\n",
    "    pickle.dump(mlens_eval_trajectories, file)\n",
    "# Pickle the test trajectories\n",
    "with open('../gym/data/mlens/mlens-test-trajectories-v1.pkl', 'wb') as file:\n",
    "    pickle.dump(mlens_test_trajectories, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try out hash encoding for the movies\n",
    "# import category_encoders as ce\n",
    "# encoder=ce.HashingEncoder(cols='Month',n_components=100)\n",
    "\n",
    "# encoder=ce.HashingEncoder(cols='title',n_components=17)\n",
    "\n",
    "# #Fit and Transform Data\n",
    "# encoder.fit_transform(movies_and_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the action space shape from 768 to a lower dimesnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple neural network for dimensionality reduction\n",
    "class DimensionReducer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DimensionReducer, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Specify input and output dimensions for reduction\n",
    "input_dim = 768  # BERT embedding size\n",
    "output_dim = 8  # Reduced dimensionality\n",
    "\n",
    "# Create the dimension reduction model\n",
    "reducer_model = DimensionReducer(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preloaded from disk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "movies_ratings_and_tags = pd.read_csv(\"../data/movies_ratings_and_tags_mlens_small.csv\")\n",
    "movies_ratings_and_tags.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "with open(\"../data/dt-datasets/movielens/processed-data/all_trajectories_with_concatenated_movname_genres_tags_userid_reward_of_scale_5.pkl\", 'rb') as f:\n",
    "    all_trajectories = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 610/610 [00:00<00:00, 913.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "movie_embed_to_id = {}\n",
    "for traj in tqdm(all_trajectories):\n",
    "    user_id = traj['user_id']\n",
    "    movie_ids = (movies_ratings_and_tags[movies_ratings_and_tags['userId'] == user_id]['movieId']).tolist()\n",
    "    observations = traj['observations'][:, 0:768] + traj['observations'][:, 768:2*768]\n",
    "    observations = torch.from_numpy(observations)\n",
    "    obs_flattened = observations.view(-1, input_dim)\n",
    "    obs_reduced = reducer_model(obs_flattened).detach().numpy()\n",
    "    for (mid, obs) in zip(movie_ids, obs_reduced):\n",
    "        movie_embed_to_id[tuple(obs)] = mid\n",
    "\n",
    "with open(f\"../data/dt-datasets/movielens/processed-data/movie_embed_with_shape_{output_dim}_to_id_mapping.pkl\", 'wb') as f:\n",
    "    pickle.dump(movie_embed_to_id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../data/dt-datasets/movielens/processed-data/movie_embed_with_shape_{output_dim}_to_id_mapping.pkl\", 'rb') as f:\n",
    "    movie_embed_to_id = pickle.load(f)\n",
    "\n",
    "# Create a vocab of all movies and save\n",
    "all_actions = list(movie_embed_to_id.keys())\n",
    "action_vocab = np.array(all_actions)\n",
    "\n",
    "with open (f\"../data/dt-datasets/movielens/processed-data/action_vocab_of_shape_{output_dim}.pkl\", 'wb') as f:\n",
    "    pickle.dump(action_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the current train and test trajectories with action space's shape 768\n",
    "import pickle\n",
    "with open(\"/home/ssk/Desktop/master-thesis/master-thesis-personalization/data/dt-datasets/movielens/train-test-sets/mlens-train-trajectories-movies-as-actions.pkl\", 'rb') as f:\n",
    "    train_trajectories = pickle.load(f)\n",
    "\n",
    "with open(\"/home/ssk/Desktop/master-thesis/master-thesis-personalization/data/dt-datasets/movielens/train-test-sets/mlens-train-trajectories-movies-as-actions.pkl\", 'rb') as f:\n",
    "    test_trajectories = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [00:00<00:00, 13857.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for traj in tqdm(train_trajectories):\n",
    "    # Flatten the embeddings for the linear layer\n",
    "    action_embeddings = torch.from_numpy(traj['actions'])\n",
    "    # print(action_embeddings.shape)\n",
    "    embeddings_flattened = action_embeddings.view(-1, input_dim)\n",
    "\n",
    "    # Pass the embeddings through the reducer model\n",
    "    reduced_embeddings = reducer_model(embeddings_flattened)\n",
    "\n",
    "    traj['actions'] = reduced_embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [00:00<00:00, 13009.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for traj in tqdm(test_trajectories):\n",
    "    # Flatten the embeddings for the linear layer\n",
    "    action_embeddings = torch.from_numpy(traj['actions'])\n",
    "    # print(action_embeddings.shape)\n",
    "    embeddings_flattened = action_embeddings.view(-1, input_dim)\n",
    "\n",
    "    # Pass the embeddings through the reducer model\n",
    "    reduced_embeddings = reducer_model(embeddings_flattened)\n",
    "\n",
    "    traj['actions'] = reduced_embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_trajectories[0]['actions'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 'movies as actions' trajectories\n",
    "# Save the train and test trajectories as pickle files to load them later\n",
    "import pickle\n",
    "with open(f'../data/dt-datasets/movielens/train-test-sets/mlens-train-trajectories-movies-as-actions-reduced-from-{input_dim}-to-{output_dim}.pkl', 'wb') as f:\n",
    "    pickle.dump(train_trajectories, f)\n",
    "with open(f'../data/dt-datasets/movielens/train-test-sets/mlens-test-trajectories-movies-as-actions-reduced-from-{input_dim}-to-{output_dim}.pkl', 'wb') as f:\n",
    "    pickle.dump(test_trajectories, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# # Example list of movies with genres\n",
    "# movies = [\n",
    "#     \"Movie1\", \"Movie2\", \"Movie3\", \"Movie4\",\n",
    "#     \"Movie5\", \"Movie6\", \"Movie7\", \"Movie8\",\n",
    "# ]\n",
    "\n",
    "# genres = [\n",
    "#     \"Animation|Children|Drama\",\n",
    "#     \"Action|Adventure|Fantasy\",\n",
    "#     \"Comedy|Romance\",\n",
    "#     \"Drama\",\n",
    "#     \"Comedy|Drama|Romance\",\n",
    "#     \"Action|Adventure|Fantasy\",\n",
    "#     \"Drama\",\n",
    "#     \"Animation|Children|Fantasy\",\n",
    "# ]\n",
    "\n",
    "# # Split genres into lists\n",
    "# genre_lists = [genre.split('|') for genre in genres]\n",
    "\n",
    "# # Use MultiLabelBinarizer to one-hot encode genres\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# genre_matrix = mlb.fit_transform(genre_lists)\n",
    "\n",
    "# # Apply K-Means clustering\n",
    "# num_clusters = 3  # You can choose the number of clusters based on your requirements\n",
    "# kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "# cluster_labels = kmeans.fit_predict(genre_matrix)\n",
    "\n",
    "# # Assign movies to clusters\n",
    "# movie_clusters = {}\n",
    "# for movie, cluster_label in zip(movies, cluster_labels):\n",
    "#     if cluster_label not in movie_clusters:\n",
    "#         movie_clusters[cluster_label] = []\n",
    "#     movie_clusters[cluster_label].append(movie)\n",
    "\n",
    "# # Print the clusters\n",
    "# for cluster, movies_in_cluster in movie_clusters.items():\n",
    "#     print(f\"Cluster {cluster + 1}: {movies_in_cluster}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Apply PCA to reduce dimensionality to 2D\n",
    "# pca = PCA(n_components=2)\n",
    "# reduced_features = pca.fit_transform(genre_matrix)\n",
    "\n",
    "# # Create a scatter plot of the clusters\n",
    "# plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=cluster_labels)\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.title('Cluster Visualization using PCA')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# # Apply t-SNE to reduce dimensionality to 2D\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# reduced_features = tsne.fit_transform(genre_matrix)\n",
    "\n",
    "# # Create a scatter plot of the clusters\n",
    "# plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=cluster_labels)\n",
    "# plt.xlabel('t-SNE Component 1')\n",
    "# plt.ylabel('t-SNE Component 2')\n",
    "# plt.title('Cluster Visualization using t-SNE')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# # Fit K-Means to obtain cluster centroids\n",
    "# kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "# kmeans.fit(genre_matrix)\n",
    "\n",
    "# # Plot cluster centroids\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for i, cluster_center in enumerate(kmeans.cluster_centers_):\n",
    "#     plt.plot(cluster_center, label=f'Cluster {i + 1}')\n",
    "# plt.xlabel('Genre Feature')\n",
    "# plt.ylabel('Feature Value')\n",
    "# plt.title('Cluster Centroids')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# # Sample data\n",
    "# data = {\n",
    "#     'UserID': [1, 1, 2, 2, 3, 3, 4, 5, 5, 6, 6],\n",
    "#     'MovieID': [101, 102, 103, 104, 105, 106, 101, 103, 106, 102, 104],\n",
    "#     'MovieName': ['Movie1', 'Movie2', 'Movie3', 'Movie4', 'Movie5', 'Movie6', 'Movie1', 'Movie3', 'Movie6', 'Movie2', 'Movie4'],\n",
    "#     'MovieGenre': ['Action|Adventure|Fantasy', 'Action|Drama', 'Action|Adventure|Fantasy', 'Drama', 'Comedy|Romance', 'Action|Adventure|Fantasy', 'Action|Adventure|Fantasy', 'Comedy|Romance', 'Action|Adventure|Fantasy', 'Action|Drama', 'Drama'],\n",
    "#     'Rating': [4.5, 3.0, 4.0, 3.5, 4.5, 3.0, 3.5, 4.0, 3.0, 2.5, 3.5]\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Split genres into lists\n",
    "# df['MovieGenre'] = df['MovieGenre'].apply(lambda x: x.split('|'))\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use MultiLabelBinarizer to one-hot encode genres\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# genre_matrix = mlb.fit_transform(df['MovieGenre'])\n",
    "# genre_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a user-item matrix with user ratings\n",
    "# user_item_matrix = pd.pivot_table(df, index='UserID', columns='MovieName', values='Rating')\n",
    "# # Handle missing values in the user-item matrix\n",
    "# user_item_matrix.fillna(0, inplace=True)\n",
    "\n",
    "# user_item_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply K-Means clustering to the user-item matrix\n",
    "# num_clusters = 2  # Adjust based on your preference\n",
    "# kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "# cluster_labels = kmeans.fit_predict(user_item_matrix)\n",
    "\n",
    "# # Assign labels to user clusters based on interpretation\n",
    "# user_clusters = {}\n",
    "# for user, cluster_label in zip(user_item_matrix.index, cluster_labels):\n",
    "#     if cluster_label not in user_clusters:\n",
    "#         user_clusters[cluster_label] = []\n",
    "#     user_clusters[cluster_label].append(user)\n",
    "\n",
    "# # Print user clusters\n",
    "# for cluster, users_in_cluster in user_clusters.items():\n",
    "#     print(f\"Cluster {cluster + 1}: {users_in_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reduce dimensionality with PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# reduced_features = pca.fit_transform(user_item_matrix)\n",
    "\n",
    "# # # Assign labels to user clusters based on interpretation\n",
    "# # user_clusters = {}\n",
    "# # for user, cluster_label in zip(user_item_matrix.index, cluster_labels):\n",
    "# #     if cluster_label not in user_clusters:\n",
    "# #         user_clusters[cluster_label] = []\n",
    "# #     user_clusters[cluster_label].append(user)\n",
    "\n",
    "# # # Create a scatter plot of the clusters\n",
    "# # plt.figure(figsize=(8, 6))\n",
    "# # for cluster, users_in_cluster in user_clusters.items():\n",
    "# #     x = reduced_features[users_in_cluster, 0]\n",
    "# #     y = reduced_features[users_in_cluster, 1]\n",
    "# #     plt.scatter(x, y, label=f'Cluster {cluster + 1}')\n",
    "# # plt.xlabel('PCA Component 1')\n",
    "# # plt.ylabel('PCA Component 2')\n",
    "# # plt.title('User Clusters Visualization')\n",
    "# # plt.legend()\n",
    "# # plt.show()\n",
    "\n",
    "# # Create a scatter plot of the clusters\n",
    "# plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=cluster_labels)\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.title('Cluster Visualization using PCA')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# # Sample data with multiple users and movies\n",
    "# data = {\n",
    "#     'UserID': [1, 1, 2, 2, 3, 3, 4, 5, 5, 6, 6],\n",
    "#     'MovieID': [101, 102, 103, 104, 105, 106, 101, 103, 106, 102, 104],\n",
    "#     'MovieName': ['Movie1', 'Movie2', 'Movie3', 'Movie4', 'Movie5', 'Movie6', 'Movie1', 'Movie3', 'Movie6', 'Movie2', 'Movie4'],\n",
    "#     'MovieGenre': ['Action|Adventure|Fantasy', 'Action|Drama', 'Action|Adventure|Fantasy', 'Drama', 'Comedy|Romance', 'Action|Adventure|Fantasy', 'Action|Adventure|Fantasy', 'Comedy|Romance', 'Action|Adventure|Fantasy', 'Action|Drama', 'Drama'],\n",
    "#     'Rating': [4.5, 3.0, 4.0, 3.5, 4.5, 3.0, 3.5, 4.0, 3.0, 2.5, 3.5]\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Split genres into lists\n",
    "# df['MovieGenre'] = df['MovieGenre'].apply(lambda x: x.split('|'))\n",
    "\n",
    "# # Use MultiLabelBinarizer to one-hot encode genres\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# genre_matrix = mlb.fit_transform(df['MovieGenre'])\n",
    "# genre_matrix.shape\n",
    "\n",
    "# # Create a user-item matrix with user ratings\n",
    "# user_item_matrix = pd.pivot_table(df, index='UserID', columns='MovieName', values='Rating')\n",
    "\n",
    "# # Handle missing values in the user-item matrix\n",
    "# user_item_matrix.fillna(0, inplace=True)\n",
    "# genre_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate user preferences for genres\n",
    "# user_preferences = {}\n",
    "# for user in user_item_matrix.index:\n",
    "#     user_ratings = user_item_matrix.loc[user].values\n",
    "#     print(user_ratings)\n",
    "#     genre_ratings = np.dot(user_ratings, genre_matrix.T)\n",
    "#     print(genre_ratings)\n",
    "#     total_genre_movies = np.sum(genre_matrix, axis=0)\n",
    "#     user_genre_preferences = genre_ratings / (total_genre_movies + 1e-9)  # Add a small epsilon to avoid division by zero\n",
    "#     user_preferences[user] = user_genre_preferences\n",
    "\n",
    "# # Define a threshold (75% of movies with ratings >= 4) for genre lover\n",
    "# threshold = 0.75\n",
    "\n",
    "# # Create columns for each genre lover\n",
    "# for genre_index, genre_name in enumerate(mlb.classes_):\n",
    "#     column_name = genre_name.lower() + '_lover'\n",
    "#     df[column_name] = [user_preferences[user][genre_index] >= threshold for user in df['UserID']]\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.decomposition import PCA\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Load BERT model and tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# all_genres = np.unique(np.concatenate(movies_and_ratings['genres'].apply(lambda g: g.split(\"|\")).tolist()))\n",
    "# print(all_genres)\n",
    "\n",
    "\n",
    "\n",
    "# # Define a list of words you want to cluster\n",
    "# # word_list = [\"apple\", \"banana\", \"orange\", \"car\", \"bus\", \"train\", \"elephant\", \"lion\", \"tiger\"]\n",
    "# word_list = all_genres\n",
    "\n",
    "# # Encode the words and get BERT embeddings\n",
    "# word_embeddings = []\n",
    "# for word in word_list:\n",
    "#     inputs = tokenizer(word, return_tensors='pt')\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     word_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "#     word_embeddings.append(word_embedding)\n",
    "\n",
    "# # Convert the list of embeddings into a NumPy array\n",
    "# word_embeddings = np.array(word_embeddings)\n",
    "\n",
    "# # Apply PCA for dimensionality reduction (optional)\n",
    "# pca = PCA(n_components=2)\n",
    "# word_embeddings_pca = pca.fit_transform(word_embeddings)\n",
    "\n",
    "# # Cluster the word embeddings using K-Means\n",
    "# num_clusters = 5  # Adjust this based on your needs\n",
    "# kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "# clusters = kmeans.fit_predict(word_embeddings_pca)\n",
    "\n",
    "# # Plot the clustered words\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k']\n",
    "# for i in range(len(word_list)):\n",
    "#     plt.scatter(word_embeddings_pca[i, 0], word_embeddings_pca[i, 1], c=colors[clusters[i]], label=word_list[i])\n",
    "\n",
    "# # Add labels to data points\n",
    "# for i in range(len(word_list)):\n",
    "#     plt.annotate(word_list[i], (word_embeddings_pca[i, 0], word_embeddings_pca[i, 1]))\n",
    "\n",
    "# plt.title('Word Clustering with BERT Embeddings')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
